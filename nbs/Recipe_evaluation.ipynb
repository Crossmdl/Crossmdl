{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import shutil\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from argparse import Namespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import linalg as LA\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "#wandb_logger = lambda dir, version: WandbLogger(\n",
    " #   name=\"wandb\", save_dir=dir, version=version\n",
    "#)\n",
    "def wandb_logger(dir,version,name):\n",
    "    return WandbLogger(\n",
    "    name=name, save_dir=dir, version=version\n",
    ")\n",
    "csvlogger = lambda dir, version: CSVLogger(dir, name=\"csvlogs\", version=version)\n",
    "tblogger = lambda dir, version: TensorBoardLogger(dir, name=\"tblogs\", version=version)\n",
    "\n",
    "def get_loggers(dir,version,name,lis=[\"csv\"]):\n",
    "    lgrs = []\n",
    "    if \"wandb\" in lis:\n",
    "        lgrs.append(wandb_logger(dir, version))\n",
    "    if \"csv\" in lis:\n",
    "        lgrs.append(csvlogger(dir, version))\n",
    "    if \"tb\" in lis:\n",
    "        lgrs.append(tblogger(dir, version))\n",
    "    return lgrs\n",
    "\n",
    "#global vars\n",
    "DATA_DIR = '/common/home/vk405/Projects/Crossmdl/Data/Recipe/'\n",
    "EMB_TRN = DATA_DIR+'embeddings_train1.pkl'\n",
    "EMB_VAL = DATA_DIR+'embeddings_val1.pkl'\n",
    "ING_TRN = DATA_DIR+'ingredients_embeddings_train.pkl'\n",
    "ING_VAL = DATA_DIR+'ingredients_embeddings_val.pkl'\n",
    "#os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class RecipeDset(Dataset):\n",
    "    def __init__(self,data_dir= '/common/home/vk405/Projects/Crossmdl/Data/Recipe/'\\\n",
    "        ,split='train',txt_emb_type='total'):\n",
    "        self.DATA_DIR = data_dir\n",
    "        self.init_data_locs()\n",
    "        self.txt_emb_type = txt_emb_type\n",
    "        self.split = split\n",
    "        if self.txt_emb_type == 'total':\n",
    "            if self.split == 'train':\n",
    "                with open(self.EMB_TRN, 'rb') as files:\n",
    "                    self.emb_vid,self.emb_txt,self.ids = pickle.load(files)\n",
    "            elif self.split == 'valid':\n",
    "                with open(self.EMB_VAL, 'rb') as files:\n",
    "                    self.emb_vid,self.emb_txt,self.ids = pickle.load(files)\n",
    "            elif self.split == 'test':\n",
    "                with open(self.EMB_TST, 'rb') as files:\n",
    "                    self.emb_vid,self.emb_txt,self.ids = pickle.load(files)\n",
    "\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        rind = idx+1\n",
    "        if rind == len(self.emb_txt):\n",
    "            rind = idx-1\n",
    "        return self.emb_vid[idx],self.emb_txt[idx],self.emb_vid[rind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emb_txt)\n",
    "\n",
    "    def init_data_locs(self):\n",
    "        #TOTAL EMBEDDINGS\n",
    "        self.EMB_TRN = self.DATA_DIR+'embeddings_train1.pkl'\n",
    "        self.EMB_VAL = self.DATA_DIR+'embeddings_val1.pkl'\n",
    "        self.EMB_TST = self.DATA_DIR+'embeddings_test1.pkl'\n",
    "        #INGRIDIENTS Embeddings\n",
    "        self.ING_TRN = self.DATA_DIR+'ingredients_embeddings_train.pkl'\n",
    "        self.ING_VAL = self.DATA_DIR+'ingredients_embeddings_val.pkl'\n",
    "        self.ING_TST = self.DATA_DIR + 'ingredients_embeddings_test.pkl'\n",
    "\n",
    "        #TITLE EMBEDDINGS\n",
    "        self.TIT_TRN = self.DATA_DIR+'title_embeddings_train.pkl'\n",
    "        self.TIT_VAL = self.DATA_DIR+'title_embeddings_val.pkl'\n",
    "        self.TIT_TST = self.DATA_DIR + 'title_embeddings_test.pkl'\n",
    "\n",
    "        #Instructions\n",
    "        self.INS_TRN = self.DATA_DIR+'instructions_embeddings_train.pkl'\n",
    "        self.INS_VAL = self.DATA_DIR+'instructions_embeddings_val.pkl'\n",
    "        self.INS_TST = self.DATA_DIR+'instructions_embeddings_test.pkl'\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "class EmbModel(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.lyrs = []\n",
    "        dim = self.params['input_dim']\n",
    "        for i in range(self.params['lyrs']):\n",
    "            if \"emb_dim\" not in self.params:\n",
    "                lyr = nn.Linear(dim,dim)\n",
    "            else:\n",
    "                prev_dim = dim\n",
    "                out_dim = self.params['emb_dim'][i]\n",
    "                if i>0:\n",
    "                    prev_dim = self.params['emb_dim'][i-1]\n",
    "                lyr = nn.Linear(prev_dim,out_dim)\n",
    "            if self.params['act'] == 'relu':\n",
    "                non_lin = nn.ReLU()\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            self.lyrs.append(lyr)\n",
    "            self.lyrs.append(non_lin)\n",
    "        self.feedforward = nn.Sequential(*self.lyrs)\n",
    "    def forward(self,x):\n",
    "        return self.feedforward(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RecipeModel(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        self.txt_emb = EmbModel(self.hparams.txt_model)\n",
    "        self.img_emb = EmbModel(self.hparams.img_model)\n",
    "        self.shared = nn.Linear(self.hparams.txt_model['fin_dim'],\\\n",
    "            self.hparams.shared_emb_dim)\n",
    "    def forward(self,x):\n",
    "        # Ignores anchor embedding\n",
    "        img,txt = x\n",
    "        img_emb = self.img_emb(img)\n",
    "        txt_emb = self.txt_emb(txt)\n",
    "        #anch_img_emb = self.img_emb(anch_img)\n",
    "\n",
    "        img_fin_emb = self.shared(img_emb)\n",
    "        txt_fin_emb = self.shared(txt_emb)\n",
    "        #anch_img_fin_emb = self.shared(anch_img_emb)\n",
    "        return img_fin_emb,txt_fin_emb\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        img,txt,anch_img = batch\n",
    "        anch_img_emb = self.img_emb(anch_img)\n",
    "        anch_img_fin_emb = self.shared(anch_img_emb)\n",
    "        img_fin_emb,txt_fin_emb = self((img,txt))\n",
    "        loss,log_losses = self.get_loss(img_fin_emb,txt_fin_emb,anch_img_fin_emb)\n",
    "        self.log(\"train_loss\",loss,on_step=True)\n",
    "        self.log(\"cos_sim_n\",log_losses[0],on_step=True)\n",
    "        self.log(\"cos_sim_p\",log_losses[-1],on_step=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        img,txt,anch_img = batch\n",
    "        anch_img_emb = self.img_emb(anch_img)\n",
    "        anch_img_fin_emb = self.shared(anch_img_emb)\n",
    "        img_fin_emb,txt_fin_emb = self((img,txt))\n",
    "        loss,log_losses = self.get_loss(img_fin_emb,txt_fin_emb,anch_img_fin_emb)\n",
    "        #collects batchwise stats for rank over half of batchsize\n",
    "        sz = int(img_fin_emb.shape[0]//2)\n",
    "        median,recall = self.rank('image',img_fin_emb.detach().cpu().numpy(),txt_fin_emb.detach().cpu().numpy(),sz)\n",
    "\n",
    "        self.log(\"val_loss\",loss,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_cos_sim_n\",log_losses[0],on_step=False, on_epoch=True)\n",
    "        self.log(\"val_cos_sim_p\",log_losses[-1],on_step=False, on_epoch=True)\n",
    "        # main metrics\n",
    "        self.log(\"val_medianrank\",median,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall_1\",recall[1],on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall_5\",recall[5],on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall_10\",recall[10],on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    # inference code\n",
    "\n",
    "    def get_loss(self,img,txt,anch,reduce='mean'):\n",
    "        #𝐿𝑐𝑜𝑠(𝒂,𝒑,𝒏)=max[𝑑(𝒂,𝒏)−𝑑(𝒂,𝒑)+𝜖,0]\n",
    "        eps = self.hparams.eps if 'eps' in self.hparams else 1e-8\n",
    "        \n",
    "        im_norm,txt_norm,anch_norm = LA.norm(img,dim=-1).reshape(img.shape[0],1),\\\n",
    "        LA.norm(txt,dim=-1).reshape(txt.shape[0],1),LA.norm(anch,dim=-1).reshape(anch.shape[0],1)\n",
    "        normd_img = img/im_norm\n",
    "        normd_txt = txt/txt_norm\n",
    "        normd_anch = anch/anch_norm\n",
    "\n",
    "        cos_sim_p = torch.sum(normd_img*normd_txt,dim=-1)\n",
    "        cos_sim_n = torch.sum(normd_anch*normd_txt,dim=-1)\n",
    "\n",
    "        unclipped_loss = cos_sim_n-cos_sim_p+eps\n",
    "        clipped_loss = torch.relu(unclipped_loss)\n",
    "        if reduce == 'mean':\n",
    "            return torch.mean(clipped_loss),(torch.mean(cos_sim_n),torch.mean(cos_sim_p))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr if 'lr' in self.hparams else 1e-3\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        return optimizer\n",
    "\n",
    "    def rank(self,type_embedding , img_embeds, rec_embeds, samples):\n",
    "        random.seed(42)\n",
    "        im_vecs = img_embeds \n",
    "        instr_vecs = rec_embeds \n",
    "\n",
    "\n",
    "        # Sort based on names to always pick same samples for medr\n",
    "    #     idxs = np.argsort(names)\n",
    "    #     names = names[idxs]\n",
    "\n",
    "        # Ranker\n",
    "        N = samples\n",
    "        idxs = range(N)\n",
    "        \n",
    "        glob_rank = []\n",
    "        glob_recall = {1:0.0,5:0.0,10:0.0}\n",
    "        for i in range(10):\n",
    "\n",
    "            ids = random.sample(range(0,len(img_embeds)), N)\n",
    "            im_sub = im_vecs[ids,:]\n",
    "            instr_sub = instr_vecs[ids,:]\n",
    "    #         ids_sub = names[ids]\n",
    "\n",
    "            # if params.embedding == 'image':\n",
    "            if type_embedding == 'image':\n",
    "                sims = np.dot(im_sub,instr_sub.T) # for im2recipe\n",
    "            else:\n",
    "                sims = np.dot(instr_sub,im_sub.T) # for recipe2im\n",
    "\n",
    "            med_rank = []\n",
    "            recall = {1:0.0,5:0.0,10:0.0}\n",
    "\n",
    "            for ii in idxs:\n",
    "\n",
    "    #             name = ids_sub[ii]\n",
    "                # get a column of similarities\n",
    "                sim = sims[ii,:]\n",
    "\n",
    "                # sort indices in descending order\n",
    "                sorting = np.argsort(sim)[::-1].tolist()\n",
    "\n",
    "                # find where the index of the pair sample ended up in the sorting\n",
    "                pos = sorting.index(ii)\n",
    "\n",
    "                if (pos+1) == 1:\n",
    "                    recall[1]+=1\n",
    "                if (pos+1) <=5:\n",
    "                    recall[5]+=1\n",
    "                if (pos+1)<=10:\n",
    "                    recall[10]+=1\n",
    "\n",
    "                # store the position\n",
    "                med_rank.append(pos+1)\n",
    "\n",
    "            for i in recall.keys():\n",
    "                recall[i]=recall[i]/N\n",
    "\n",
    "            med = np.median(med_rank)\n",
    "            # print \"median\", med\n",
    "\n",
    "            for i in recall.keys():\n",
    "                glob_recall[i]+=recall[i]\n",
    "            glob_rank.append(med)\n",
    "\n",
    "        for i in glob_recall.keys():\n",
    "            glob_recall[i] = glob_recall[i]/10\n",
    "\n",
    "        return np.average(glob_rank), glob_recall\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg corresponding to best model\n",
    "\n",
    "#eps = eps = 0.1\n",
    "cfg = Namespace(\n",
    "    seed = 0,\n",
    "    version = 'retrievallyr_1',\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/Recipe/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/Recipe/\",\n",
    "    mode = 'train',\n",
    "    txt_model = {'input_dim':1024,'lyrs':1,'fin_dim':1024,'act':'relu'},\n",
    "    img_model = {'input_dim':1024,'lyrs':1,'fin_dim':1024,'act':'relu'},\n",
    "    shared_emb_dim = 1024,\n",
    "    txt_emb_type = 'total',\n",
    "    lr = 1e-4,\n",
    "    eps = 0.5,\n",
    "    loggers = [\"csv\"],\n",
    "    cbs = [\"checkpoint\",\"early_stop\"],\n",
    "    trainer = {'log_every_n_steps': 50,\n",
    "    'max_epochs': 10},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"val_recall_1\"},\n",
    "    early_stop = {\"monitor\":\"val_recall_1\",\"patience\":2,\"mode\":'max'},\n",
    "    batch_size=512,\n",
    "    val_batch_size = 2000\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/common/home/vk405/Projects/Crossmdl/nbs/Recipe/ckpts/retrievallyr_1/epoch=2-val_recall_1=0.54.ckpt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = '/common/home/vk405/Projects/Crossmdl/nbs/Recipe/'\n",
    "ckpt_dir = log_dir+'ckpts/'+cfg.version+'/'\n",
    "ckpt = ckpt_dir+os.listdir(ckpt_dir)[0]\n",
    "ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "recipedata_test = RecipeDset(data_dir=cfg.data_dir,split='test',\\\n",
    "            txt_emb_type = cfg.txt_emb_type)\n",
    "\n",
    "test_loader = DataLoader(recipedata_test,batch_size=1000,shuffle=False)\n",
    "net = RecipeModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net.load_from_checkpoint(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb_list = []\n",
    "txt_emb_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        img,txt,_ = batch\n",
    "        img_emb,txt_emb = model((img,txt))\n",
    "        img_emb_list.append(img_emb)\n",
    "        txt_emb_list.append(txt_emb)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_img_emb = np.concatenate(img_emb_list)\n",
    "tst_txt_emb = np.concatenate(txt_emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1K Samples:\n",
      "1.0 {1: 0.54, 5: 0.8200999999999998, 10: 0.8886000000000001}\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "median,recall = model.rank('image',tst_img_emb,tst_txt_emb,1000)\n",
    "\n",
    "print(\"1K Samples:\")\n",
    "print(median,recall)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10K Samples:\n",
      "5.85 {1: 0.22646000000000002, 5: 0.49429, 10: 0.61405}\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "median,recall = model.rank('image',tst_img_emb,tst_txt_emb,10000)\n",
    "\n",
    "print(\"10K Samples:\")\n",
    "print(median,recall)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv logs\n",
    "\n",
    "log_dir = '/common/home/vk405/Projects/Crossmdl/nbs/Recipe/'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(log_dir+'csvlogs/retrievaleps_0.5_defaultbest/metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "\n",
    "1.Plot traninig plots for all the hyperparam experiments - train-loss, cos-sim,sin-sim.\n",
    "\n",
    "2.Show comparision stats on test set for [total,title,ingredients,instruction models].\n",
    "\n",
    "3.Show retrieved images\n",
    "\n",
    "4. Plot how smooth interpolation between images retrieve interpolated text too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Crossmdl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
