{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pad your sequences\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import joblib\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numpy import linalg as LA\n",
    "from argparse import Namespace\n",
    "from numpy import genfromtxt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint,LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import clip\n",
    "\n",
    "\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "wandb_logger = lambda dir, version: WandbLogger(\n",
    "    name=\"wandb\", save_dir=dir, version=version\n",
    ")\n",
    "csvlogger = lambda dir, version: CSVLogger(dir, name=\"csvlogs\", version=version)\n",
    "tblogger = lambda dir, version: TensorBoardLogger(dir, name=\"tblogs\", version=version)\n",
    "\n",
    "def get_loggers(dir,version,lis=[\"csv\"]):\n",
    "    lgrs = []\n",
    "    if \"wandb\" in lis:\n",
    "        lgrs.append(wandb_logger(dir, version))\n",
    "    if \"csv\" in lis:\n",
    "        lgrs.append(csvlogger(dir, version))\n",
    "    if \"tb\" in lis:\n",
    "        lgrs.append(tblogger(dir, version))\n",
    "    return lgrs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vid_ids(split='training',\\\n",
    "    annotns_file='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/annotations/youcookii_annotations_trainval.json'):\n",
    "    # Returns vid_ids corresponding to the split: 'training'/'validation'\n",
    "    \n",
    "    vid_lis = []\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)['database']\n",
    "        for key in annotns:\n",
    "            if annotns[key]['subset'] == split:\n",
    "                vid_lis.append(key)\n",
    "    return vid_lis\n",
    "\n",
    "\n",
    "def get_split_files(split='training',\\\n",
    "    annotns_file='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/annotations/youcookii_annotations_trainval.json',\\\n",
    "        data_dir = '/common/users/vk405/Youcook/'):\n",
    "    total_ids = get_vid_ids(split,annotns_file)\n",
    "    downloaded_ids = set([dir for dir in os.listdir(data_dir) if 'joblib' not in dir])\n",
    "    vid_locs = []\n",
    "    sents = {}\n",
    "    segs = {}\n",
    "    incomplete = []\n",
    "    for id in total_ids:\n",
    "        if id in downloaded_ids:\n",
    "            vid_loc = data_dir+id + '/'\n",
    "            if len(os.listdir(vid_loc))>=495:\n",
    "                vid_locs.append(vid_loc)\n",
    "                seg = joblib.load(data_dir+f'{id}global_segs.joblib')\n",
    "                sent = joblib.load(data_dir+f'{id}global_sents.joblib')\n",
    "                try:\n",
    "                    sents[id] = sent[id]\n",
    "                    segs[id] = seg[id]\n",
    "                except:\n",
    "                    print(f\"{id} is no corresponding global sent/seg\")\n",
    "            else:\n",
    "                #print(f\"{id} has only imgs {len(os.listdir(vid_loc))}\")\n",
    "                incomplete.append(id)\n",
    "    return vid_locs,segs,sents,incomplete \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "\n",
    "FEAT_DIR = pathlib.Path('/common/users/vk405/CLIP_FEAT')\n",
    "RAWFRAME_DIR = pathlib.Path('/common/users/vk405/Youcook/')\n",
    "tokens =  {'O':0,'B':1,'I':2}\n",
    "\n",
    "\n",
    "# modified with #nclasses\n",
    "class Dset(Dataset):\n",
    "    def __init__(self,data_dir,feat_dir,split='training'):\n",
    "        self.data_dir = data_dir\n",
    "        self.feat_dir = feat_dir\n",
    "        self.split = split\n",
    "        self.vid_ids,self.sents = self.get_ids()\n",
    "        self.labels = self.getlabels()\n",
    "        self.sanitycheck()\n",
    "        #self.data = self.getdata()\n",
    "        \n",
    "    def segs_class(self,segs):\n",
    "        out = np.zeros(500)\n",
    "        for segid,seg in enumerate(segs):\n",
    "            st,end = seg\n",
    "            out[st:end+1] = segid+1\n",
    "        return out    \n",
    "\n",
    "\n",
    "    def sanitycheck(self):\n",
    "        mis = []\n",
    "        #import pdb;pdb.set_trace()\n",
    "        for key in self.labels.keys():\n",
    "            txt_loc = self.feat_dir/self.split/f'txt_{key}.joblib'\n",
    "            txt = joblib.load(txt_loc)\n",
    "            if len(self.labels[key]) == len(self.sents[key]) == len(txt):\n",
    "                pass\n",
    "            else:\n",
    "                print(key)\n",
    "                mis.append(key)\n",
    "        print(f\"segs are not matching:{mis}\")\n",
    "        for key in mis:\n",
    "            self.vid_ids.remove(key)\n",
    "        self.sents = None\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vid_ids)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.load(self.vid_ids[idx])\n",
    "\n",
    "    def load(self,vid_id):\n",
    "        vid_frames_loc = self.feat_dir/self.split/f'vid_{vid_id}.joblib'\n",
    "        txt_loc = self.feat_dir/self.split/f'txt_{vid_id}.joblib'\n",
    "        vid = joblib.load(vid_frames_loc)\n",
    "        try:\n",
    "            txt = joblib.load(txt_loc)\n",
    "        except:\n",
    "            import pdb;pdb.set_trace()\n",
    "        segs = self.labels[vid_id]\n",
    "        labels = self.segs_class(segs)\n",
    "        return vid,txt,labels\n",
    "\n",
    "    def getlabels(self):\n",
    "        label_dict = {}\n",
    "        for vidid in self.vid_ids:\n",
    "            vidloc = self.data_dir/vidid\n",
    "            segs = self.extract_seg(vidloc)\n",
    "            label_dict[vidid] = segs\n",
    "        return label_dict\n",
    "    \n",
    "    def extract_seg(self,vid_loc):\n",
    "        imgs = sorted(os.listdir(vid_loc),key=lambda x: int(x.split('_')[0]))\n",
    "        segs = defaultdict(list)\n",
    "        for img in imgs:\n",
    "            ind,rem = int(img.split('_')[0]),img.split('_')[-1]\n",
    "            \n",
    "            if 'n.' not in rem:\n",
    "                #print(ind,rem)\n",
    "                seg_id = int(rem.split('.')[0])\n",
    "                segs[seg_id].append(ind)\n",
    "                #print(seg_id,ind)\n",
    "        final_segs = []\n",
    "        #import pdb;pdb.set_trace()\n",
    "        segids = sorted(segs.keys())\n",
    "        for segid in segids:\n",
    "            final_segs.append((min(segs[segid]),max(segs[segid])))\n",
    "        return final_segs\n",
    "        \n",
    "    def get_ids(self):\n",
    "        annotns_file='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/annotations/youcookii_annotations_trainval.json'\n",
    "        data_dir = '/common/users/vk405/Youcook/'\n",
    "        vid_locs,_,sents,_ = get_split_files(self.split,annotns_file,data_dir)\n",
    "        ids = [ele.split('/')[-2] for ele in vid_locs]\n",
    "        files = set(os.listdir(self.feat_dir/self.split))\n",
    "        finids = []\n",
    "        missing = []\n",
    "        for id in ids:\n",
    "            if f'vid_{id}.joblib' in files:\n",
    "                finids.append(id)\n",
    "            else:missing.append(id)\n",
    "        print(f\"missing:{missing}\")\n",
    "        return finids,sents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS-ATTN LAYER\n",
    "#model utils\n",
    "\n",
    "#!pip install transformers\n",
    "\n",
    "def init_parameters_xavier_uniform(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "def scaled_dot(query, key, mask_key=None):  \n",
    "    score = torch.matmul(query, key.transpose(-2, -1))\n",
    "    score /= math.sqrt(query.size(-1))\n",
    "    if mask_key is not None:\n",
    "        score = score.masked_fill(mask_key, -1e18)  # Represents negative infinity\n",
    "    return score      \n",
    "            \n",
    "def attend(query, key, value, mask_key=None, dropout=None):\n",
    "    # TODO: Implement\n",
    "    # Use scaled_dot, be sure to mask key\n",
    "    #smax = nn.Softmax(-1)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    score = scaled_dot(query,key,mask_key)  \n",
    "    attention = F.softmax(score,dim=-1)\n",
    "    if dropout is not None:#do = nn.Dropout(dropout)\n",
    "        attention = dropout(attention)\n",
    "    answer = torch.matmul(attention,value) \n",
    "    # Convexly combine value embeddings using attention, this should be just a matrix-matrix multiplication.\n",
    "    return answer, attention\n",
    "\n",
    "\n",
    "\n",
    "def split_heads(batch, num_heads):  \n",
    "    (batch_size, length, dim) = batch.size()  # These are the expected batch dimensions.\n",
    "    assert dim % num_heads == 0  # Assert that dimension is divisible by the number of heads.\n",
    "    dim_head = dim // num_heads\n",
    "\n",
    "    # No new memory allocation\n",
    "    splitted = batch.view(batch_size, -1, num_heads, dim_head).transpose(1, 2)  \n",
    "    return splitted  # (batch_size, num_heads, length, dim_head), note that now the last two dimensions are compatible with our attention functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_heads(batch):  \n",
    "    (batch_size, num_heads, length, dim_head) = batch.size()  # These are the expected batch dimensions.\n",
    "\n",
    "    # New memory allocation (reshape), can't avoid.\n",
    "    merged = batch.transpose(1, 2).reshape(batch_size, -1, num_heads * dim_head)\n",
    "    return merged  # (batch_size, length, dim)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "\n",
    "        self.linear_query = nn.Linear(dim, dim)\n",
    "        self.linear_key = nn.Linear(dim, dim)\n",
    "        self.linear_value = nn.Linear(dim, dim)\n",
    "        self.linear_final = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, query, key, value, mask_key=None, layer_cache=None,\n",
    "              memory_attention=False):\n",
    "        \"\"\"\n",
    "        INPUT\n",
    "          query: (batch_size, length_query, dim)\n",
    "          key: (batch_size, length_key, dim)\n",
    "          value: (batch_size, length_key, dim_value)\n",
    "          mask_key: (*, 1, length_key) if queries share the same mask, else\n",
    "                    (*, length_query, length_key)\n",
    "          layer_cache: if not None, stepwise decoding (cache of key/value)\n",
    "          memory_attention: doing memory attention in stepwise decoding?\n",
    "        OUTPUT\n",
    "          answer: (batch_size, length_query, dim_value)\n",
    "          attention: (batch_size, num_heads, length_query, length_key) else\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.linear_query(query)\n",
    "        query = split_heads(query, self.num_heads)  # (batch_size, num_heads, -1, dim_head)\n",
    "\n",
    "        def process_key_value(key, value):  # Only called when necessary.\n",
    "            key = self.linear_key(key)\n",
    "            key = split_heads(key, self.num_heads)\n",
    "            value = self.linear_value(value)\n",
    "            value = split_heads(value, self.num_heads)\n",
    "            return key, value\n",
    "\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if layer_cache is None:\n",
    "            key, value = process_key_value(key, value)\n",
    "        else:\n",
    "            assert query.size(2) == 1  # Stepwise decoding\n",
    "            \n",
    "            if memory_attention:\n",
    "                if layer_cache['memory_key'] is None:  # One-time calculation\n",
    "                    key, value = process_key_value(key, value)\n",
    "                    # (batch_size, num_heads, length_memory, dim)\n",
    "                    layer_cache['memory_key'] = key\n",
    "                    layer_cache['memory_value'] = value\n",
    "\n",
    "                key = layer_cache['memory_key']\n",
    "                value = layer_cache['memory_value']\n",
    "\n",
    "            else:  # Self-attention during decoding\n",
    "                key, value = process_key_value(key, value)\n",
    "                assert key.size(2) == 1 and value.size(2) == 1\n",
    "                \n",
    "                # Append to previous.\n",
    "                if layer_cache['self_key'] is not None:\n",
    "                    key = torch.cat((layer_cache['self_key'], key), dim=2)\n",
    "                    value = torch.cat((layer_cache['self_value'], value), dim=2)\n",
    "                    \n",
    "                 # (batch_size, num_heads, length_decoded, dim)\n",
    "                layer_cache['self_key'] = key  # Recache.\n",
    "                layer_cache['self_value'] = value\n",
    "        # Because we've splitted embeddings into heads, we must also split the mask. \n",
    "        # And because each query uses the same mask for all heads (we don't use different masking for different heads), \n",
    "        # we can specify length 1 for the head dimension.\n",
    "        if mask_key is not None:  \n",
    "            mask_key = mask_key.unsqueeze(1)  # (batch_size, 1, -1, length_key)\n",
    "\n",
    "        answer, attention = attend(query, key, value, mask_key, self.dropout)\n",
    "\n",
    "        answer = merge_heads(answer)  # (batch_size, length_key, dim)\n",
    "        answer = self.linear_final(answer)\n",
    "\n",
    "        return answer, attention\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_hidden, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, dim_hidden)\n",
    "        self.w2 = nn.Linear(dim_hidden, dim)\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.drop1 = nn.Dropout(drop_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(drop_rate)\n",
    "    def forward(self, x):\n",
    "        inter = self.drop1(self.relu(self.w1(self.layer_norm(x))))\n",
    "        output = self.drop2(self.w2(inter))\n",
    "        return output + x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SinusoidalPositioner(nn.Module):\n",
    "    def __init__(self, dim, drop_rate=0.1, length_max=5000):\n",
    "        super().__init__()\n",
    "        frequency = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.) / dim))  # Using different frequency for each dim\n",
    "        positions = torch.arange(0, length_max).unsqueeze(1)\n",
    "        wave = torch.zeros(length_max, dim)\n",
    "        wave[:, 0::2] = torch.sin(frequency * positions)\n",
    "        wave[:, 1::2] = torch.cos(frequency * positions)\n",
    "        self.register_buffer('wave', wave.unsqueeze(0))  # (1, length_max, dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.dim = dim\n",
    "        self.length_max = length_max\n",
    "    def forward(self, x, step=-1):\n",
    "        assert x.size(-2) <= self.length_max\n",
    "\n",
    "        if step < 0:  # Take the corresponding leftmost embeddings.\n",
    "            position_encoding = self.wave[:, :x.size(-2), :]\n",
    "        else:  # Take the embedding at the step.\n",
    "            position_encoding = self.wave[:, step, :]\n",
    "\n",
    "        x = x * math.sqrt(self.dim)\n",
    "        return self.dropout(x + position_encoding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, dim, num_heads, dim_hidden, drop_rate):\n",
    "    super().__init__()\n",
    "    self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "    self.self_attention = MultiHeadAttention(dim, num_heads, drop_rate)\n",
    "    self.drop = nn.Dropout(drop_rate)\n",
    "    self.feedforward = PositionwiseFeedForward(dim, dim_hidden, drop_rate)\n",
    "\n",
    "  def forward(self, source, mask_source=None):\n",
    "    # TODO: Implement\n",
    "    #print(source.shape)\n",
    "    normed = self.layer_norm(source)  \n",
    "    # Apply layer norm on source\n",
    "\n",
    "    attended, attention = self.self_attention(normed,normed,normed,mask_source)\n",
    "    #None, None  # Apply self-attention on normed (be sure to use mask_source).\n",
    "    attended = self.drop(attended) + source  \n",
    "    # Re-write attended by applying dropout and adding a residual connection to source.\n",
    "    return self.feedforward(attended), attention\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self,dim,num_heads,dim_hidden,drop_rate):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.context_attention = MultiHeadAttention(dim, num_heads, drop_rate)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "        self.feedforward = PositionwiseFeedForward(dim, dim_hidden, drop_rate)\n",
    "        \n",
    "    def forward(self,target,memory,layer_cache=None):\n",
    "        \n",
    "        cross_attn_target = self.layer_norm(target)\n",
    "        attended, attention = self.context_attention(cross_attn_target,memory,memory,layer_cache=layer_cache,memory_attention=True)\n",
    "        \n",
    "        attended = target + self.drop(attended)\n",
    "        \n",
    "        return self.feedforward(attended),attention\n",
    "\n",
    "\n",
    "\n",
    "layer_cache = {'memory_key': None, 'memory_value': None, 'self_key': None, 'self_value': None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmAttn(nn.Module):\n",
    "    def __init__(self,lstm_lyrs,lstm_hdim,bidirectional,input_dim,nheads,attn_hdim,attn_dropout,tgt_dim):\n",
    "        self.lstm_lyrs,self.lstm_hdim,self.bidirectional,self.input_dim,self.nheads,self.attn_hdim,self.attn_dropout,self.tgt_dim = \\\n",
    "            lstm_lyrs,lstm_hdim,bidirectional,input_dim,nheads,attn_hdim,attn_dropout,tgt_dim\n",
    "        super().__init__()\n",
    "        if bidirectional:\n",
    "            #as lstm concates both directions\n",
    "            lstm_hdim = lstm_hdim//2\n",
    "        self.lstm = nn.LSTM(input_dim,lstm_hdim,\\\n",
    "        lstm_lyrs,bidirectional=bidirectional,batch_first=True)\n",
    "        self.crossattn = CrossAttentionLayer(self.lstm_hdim,nheads,attn_hdim,attn_dropout)\n",
    "        self.txt_enc = nn.Linear(input_dim,self.lstm_hdim)\n",
    "        self.attn_weights = None\n",
    "        self.act = nn.ReLU()\n",
    "        self.tgt_enc = nn.Linear(self.lstm_hdim,tgt_dim)\n",
    "\n",
    "    def forward(self,vid,txt):\n",
    "        #import pdb;pdb.set_trace()\n",
    "        hiddens, (final_h, final_c) = self.lstm(vid)\n",
    "        txt_inp = self.act(self.txt_enc(txt))\n",
    "        out_emb,self.attn_weights = self.crossattn(hiddens,txt_inp)\n",
    "        tgt_emb = self.tgt_enc(out_emb)\n",
    "        return tgt_emb\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    \"\"\"Conditional random field.\n",
    "    This module implements a conditional random field [LMP01]_. The forward computation\n",
    "    of this class computes the log likelihood of the given sequence of tags and\n",
    "    emission score tensor. This class also has `~CRF.decode` method which finds\n",
    "    the best tag sequence given an emission score tensor using `Viterbi algorithm`_.\n",
    "    Args:\n",
    "        num_tags: Number of tags.\n",
    "        batch_first: Whether the first dimension corresponds to the size of a minibatch.\n",
    "    Attributes:\n",
    "        start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size\n",
    "            ``(num_tags,)``.\n",
    "        end_transitions (`~torch.nn.Parameter`): End transition score tensor of size\n",
    "            ``(num_tags,)``.\n",
    "        transitions (`~torch.nn.Parameter`): Transition score tensor of size\n",
    "            ``(num_tags, num_tags)``.\n",
    "    .. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).\n",
    "       \"Conditional random fields: Probabilistic models for segmenting and\n",
    "       labeling sequence data\". *Proc. 18th International Conf. on Machine\n",
    "       Learning*. Morgan Kaufmann. pp. 282–289.\n",
    "    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_tags: int, batch_first: bool = False) -> None:\n",
    "        if num_tags <= 0:\n",
    "            raise ValueError(f'invalid number of tags: {num_tags}')\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.batch_first = batch_first\n",
    "        self.start_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.end_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Initialize the transition parameters.\n",
    "        The parameters will be initialized randomly from a uniform distribution\n",
    "        between -0.1 and 0.1.\n",
    "        \"\"\"\n",
    "        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "        \n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(num_tags={self.num_tags})'\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            emissions: torch.Tensor,\n",
    "            tags: torch.LongTensor,\n",
    "            mask: Optional[torch.ByteTensor] = None,\n",
    "            reduction: str = 'sum',\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\n",
    "        Args:\n",
    "            emissions (`~torch.Tensor`): Emission score tensor of size\n",
    "                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "            tags (`~torch.LongTensor`): Sequence of tags tensor of size\n",
    "                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length)`` otherwise.\n",
    "            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n",
    "                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n",
    "            reduction: Specifies  the reduction to apply to the output:\n",
    "                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.\n",
    "                ``sum``: the output will be summed over batches. ``mean``: the output will be\n",
    "                averaged over batches. ``token_mean``: the output will be averaged over tokens.\n",
    "        Returns:\n",
    "            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if\n",
    "            reduction is ``none``, ``()`` otherwise.\n",
    "        \"\"\"\n",
    "        self._validate(emissions, tags=tags, mask=mask)\n",
    "        if reduction not in ('none', 'sum', 'mean', 'token_mean'):\n",
    "            raise ValueError(f'invalid reduction: {reduction}')\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(tags, dtype=torch.uint8)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            tags = tags.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        numerator = self._compute_score(emissions, tags, mask)\n",
    "        # shape: (batch_size,)\n",
    "        denominator = self._compute_normalizer(emissions, mask)\n",
    "        # shape: (batch_size,)\n",
    "        llh = numerator - denominator\n",
    "\n",
    "        if reduction == 'none':\n",
    "            return llh\n",
    "        if reduction == 'sum':\n",
    "            return llh.sum()\n",
    "        if reduction == 'mean':\n",
    "            return llh.mean()\n",
    "        assert reduction == 'token_mean'\n",
    "        return llh.sum() / mask.type_as(emissions).sum()\n",
    "\n",
    "    def decode(self, emissions: torch.Tensor,\n",
    "               mask: Optional[torch.ByteTensor] = None) -> List[List[int]]:\n",
    "        \"\"\"Find the most likely tag sequence using Viterbi algorithm.\n",
    "        Args:\n",
    "            emissions (`~torch.Tensor`): Emission score tensor of size\n",
    "                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n",
    "                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n",
    "        Returns:\n",
    "            List of list containing the best tag sequence for each batch.\n",
    "        \"\"\"\n",
    "        self._validate(emissions, mask=mask)\n",
    "        if mask is None:\n",
    "            mask = emissions.new_ones(emissions.shape[:2], dtype=torch.uint8)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        return self._viterbi_decode(emissions, mask)\n",
    "\n",
    "    def _validate(\n",
    "            self,\n",
    "            emissions: torch.Tensor,\n",
    "            tags: Optional[torch.LongTensor] = None,\n",
    "            mask: Optional[torch.ByteTensor] = None) -> None:\n",
    "        if emissions.dim() != 3:\n",
    "            raise ValueError(f'emissions must have dimension of 3, got {emissions.dim()}')\n",
    "        if emissions.size(2) != self.num_tags:\n",
    "            raise ValueError(\n",
    "                f'expected last dimension of emissions is {self.num_tags}, '\n",
    "                f'got {emissions.size(2)}')\n",
    "\n",
    "        if tags is not None:\n",
    "            if emissions.shape[:2] != tags.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and tags must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(tags.shape)}')\n",
    "\n",
    "        if mask is not None:\n",
    "            if emissions.shape[:2] != mask.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and mask must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(mask.shape)}')\n",
    "            no_empty_seq = not self.batch_first and mask[0].all()\n",
    "            no_empty_seq_bf = self.batch_first and mask[:, 0].all()\n",
    "            if not no_empty_seq and not no_empty_seq_bf:\n",
    "                raise ValueError('mask of the first timestep must all be on')\n",
    "\n",
    "    def _compute_score(\n",
    "            self, emissions: torch.Tensor, tags: torch.LongTensor,\n",
    "            mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # tags: (seq_length, batch_size)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and tags.dim() == 2\n",
    "        assert emissions.shape[:2] == tags.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask.shape == tags.shape\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length, batch_size = tags.shape\n",
    "        mask = mask.type_as(emissions)\n",
    "\n",
    "        # Start transition score and first emission\n",
    "        # shape: (batch_size,)\n",
    "        score = self.start_transitions[tags[0]]\n",
    "        score += emissions[0, torch.arange(batch_size), tags[0]]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Transition score to next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += self.transitions[tags[i - 1], tags[i]] * mask[i]\n",
    "\n",
    "            # Emission score for next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        # shape: (batch_size,)\n",
    "        last_tags = tags[seq_ends, torch.arange(batch_size)]\n",
    "        # shape: (batch_size,)\n",
    "        score += self.end_transitions[last_tags]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _compute_normalizer(\n",
    "            self, emissions: torch.Tensor, mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and mask.dim() == 2\n",
    "        assert emissions.shape[:2] == mask.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length = emissions.size(0)\n",
    "\n",
    "        # Start transition score and first emission; score has size of\n",
    "        # (batch_size, num_tags) where for each batch, the j-th column stores\n",
    "        # the score that the first timestep has tag j\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emissions = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the sum of scores of all\n",
    "            # possible tag sequences so far that end with transitioning from tag i to tag j\n",
    "            # and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emissions\n",
    "\n",
    "            # Sum over all possible current tags, but we're in score space, so a sum\n",
    "            # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of\n",
    "            # all possible tag sequences so far, that end in tag i\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score = torch.logsumexp(next_score, dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Sum (log-sum-exp) over all possible tags\n",
    "        # shape: (batch_size,)\n",
    "        return torch.logsumexp(score, dim=1)\n",
    "\n",
    "    def _viterbi_decode(self, emissions: torch.FloatTensor,\n",
    "                        mask: torch.ByteTensor) -> List[List[int]]:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and mask.dim() == 2\n",
    "        assert emissions.shape[:2] == mask.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length, batch_size = mask.shape\n",
    "\n",
    "        # Start transition and first emission\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "        history = []\n",
    "\n",
    "        # score is a tensor of size (batch_size, num_tags) where for every batch,\n",
    "        # value at column j stores the score of the best tag sequence so far that ends\n",
    "        # with tag j\n",
    "        # history saves where the best tags candidate transitioned from; this is used\n",
    "        # when we trace back the best tag sequence\n",
    "\n",
    "        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n",
    "        # for every possible next tag\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast viterbi score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emission = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the score of the best\n",
    "            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emission\n",
    "\n",
    "            # Find the maximum score over all possible current tag\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score, indices = next_score.max(dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # and save the index that produces the next score\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "            history.append(indices)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Now, compute the best path for each sample\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        best_tags_list = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            # Find the tag which maximizes the score at the last timestep; this is our best tag\n",
    "            # for the last timestep\n",
    "            _, best_last_tag = score[idx].max(dim=0)\n",
    "            best_tags = [best_last_tag.item()]\n",
    "\n",
    "            # We trace back where the best last tag comes from, append that to our best tag\n",
    "            # sequence, and trace it back again, and so on\n",
    "            for hist in reversed(history[:seq_ends[idx]]):\n",
    "                best_last_tag = hist[idx][best_tags[-1]]\n",
    "                best_tags.append(best_last_tag.item())\n",
    "\n",
    "            # Reverse the order because we start from the last timestep\n",
    "            best_tags.reverse()\n",
    "            best_tags_list.append(best_tags)\n",
    "\n",
    "        return best_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipCRF(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.emission_model = LstmAttn(**self.hparams.emission_model)\n",
    "        self.crf = CRF(self.hparams.emission_model['tgt_dim'])\n",
    "\n",
    "    def forward(self,vid,txt):\n",
    "        tgt_emb = self.emission_model(vid.float(),txt.float())\n",
    "        batch_size,seq_length,num_tags = tgt_emb.shape\n",
    "        emissions = torch.reshape(tgt_emb,(seq_length, batch_size, num_tags))\n",
    "        return self.crf.decode(emissions)\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        vid,txt,labels = batch\n",
    "        batch_size,seq_length = labels.shape\n",
    "        labels = torch.reshape(torch.tensor(labels,dtype=torch.long),(seq_length,batch_size))\n",
    "        \n",
    "        tgt_emb = self.emission_model(vid.float(),txt.float())\n",
    "        batch_size,seq_length,num_tags = tgt_emb.shape\n",
    "        emissions = torch.reshape(tgt_emb,(seq_length, batch_size, num_tags))\n",
    "        loglike = self.crf(emissions,labels)\n",
    "        loss = -1*loglike\n",
    "        self.log(\"train_loglikelihood\",loglike,on_step=True)\n",
    "        self.log(\"train_loss\",loss,on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        vid,txt,labels = batch\n",
    "        batch_size,seq_length = labels.shape\n",
    "        labels = torch.reshape(torch.tensor(labels,dtype=torch.long),(seq_length,batch_size))\n",
    "        \n",
    "        tgt_emb = self.emission_model(vid.float(),txt.float())\n",
    "        batch_size,seq_length,num_tags = tgt_emb.shape\n",
    "        emissions = torch.reshape(tgt_emb,(seq_length, batch_size, num_tags))\n",
    "        loglike = self.crf(emissions,labels)\n",
    "        loss = -1*loglike\n",
    "        self.log(\"val_loss\",loss,on_step=False,on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        num_warmup_steps = self.hparams.num_warmup_steps\n",
    "        lr_dim = self.hparams.lr_dim\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr=self.hparams.lr,betas=(0.9, 0.98), eps=1e-9)\n",
    "        #scheduler = ScheduledOptim(optimizer,1,lr_dim,num_warmup_steps)\n",
    "        return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "def get_testdset(cfg):\n",
    "    #returns test and validation set joined\n",
    "    valset = Dset(cfg.RAWFRAME_DIR,cfg.FEAT_DIR)\n",
    "    tset = Dset(cfg.RAWFRAME_DIR,cfg.FEAT_DIR,'validation')\n",
    "    totset = ConcatDataset([valset,tset])\n",
    "    return totset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Namespace(\n",
    "version = 'clip_crf_trn_ncls',\n",
    "id = 0,\n",
    "FEAT_DIR = FEAT_DIR,\n",
    "RAWFRAME_DIR = RAWFRAME_DIR,\n",
    "artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/\",\n",
    "data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/YouCookII/\",\n",
    "trn_split = 0.8,\n",
    "mode = 'train',\n",
    "split = 'training',\n",
    "loggers = [\"csv\"],\n",
    "seed = 0,\n",
    "emission_model = {'bidirectional':True,'input_dim':512,'nheads':1,'lstm_lyrs':1,\\\n",
    "'lstm_hdim':128,'attn_hdim':64,\\\n",
    "'attn_dropout':0.0,\n",
    "'tgt_dim': 17\n",
    "},\n",
    "cbs = [\"checkpoint\"],\n",
    "trainer = {'log_every_n_steps': 1, \n",
    "'max_epochs': 60},\n",
    "checkpoint = {\"every_n_epochs\": 1,\n",
    "\"monitor\": \"val_loss\"},\n",
    "early_stop = {\"monitor\":\"val_loss\",\"mode\":\"min\",\"patience\":5},\n",
    "lr = 1e-4,\n",
    "lr_dim = 200,\n",
    "num_warmup_steps = 4000,\n",
    "batch_sz = 1\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = ClipCRF(cfg)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "def get_model(cfg):\n",
    "    #takes the first stored weight\n",
    "    hparams = cfg    \n",
    "    PATH = Path(cfg.artifacts_loc)/'ckpts'/cfg.version\n",
    "    ckpt = os.listdir(PATH)[1]\n",
    "    net = ClipCRF(hparams)\n",
    "    print(f\"loading ckpt:{ckpt}\")\n",
    "    new_model = net.load_from_checkpoint(checkpoint_path=str(PATH/ckpt))\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model,dl):\n",
    "    frame_level_acc = []\n",
    "    labellis = []\n",
    "    predlis = []\n",
    "    model.eval()\n",
    "    for batch in tqdm(dl):\n",
    "        vid,text,labels = batch\n",
    "        pred_labels = model(vid,text)\n",
    "        ground_labels = labels.detach().cpu().numpy()[0]\n",
    "        predlis.append(pred_labels)\n",
    "        labellis.append(ground_labels)\n",
    "        acc = np.mean(ground_labels == pred_labels)\n",
    "        frame_level_acc.append(acc)\n",
    "    joblib.dump(labellis,'labellis.joblib')\n",
    "    joblib.dump(predlis,'predlis.joblib')\n",
    "    return np.mean(frame_level_acc),labellis,predlis\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing:['ukfCQQpZ0k4', 'NK2xHVWojgY', 'mixdagZ-fwI']\n",
      "cwsDQ7M5OTI\n",
      "uf65nfh6X2U\n",
      "segs are not matching:['cwsDQ7M5OTI', 'uf65nfh6X2U']\n",
      "missing:[]\n",
      "95WMX64RIBc\n",
      "segs are not matching:['95WMX64RIBc']\n"
     ]
    }
   ],
   "source": [
    "#get-dataloader\n",
    "d_test = get_testdset(cfg)\n",
    "testl = DataLoader(d_test,batch_size=1,shuffle=False,num_workers = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ckpt:epoch=216-train_loss=91.17.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = get_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#studying attention weights\n",
    "def study_attn():\n",
    "    # returns the percentile score of ground truth segments\n",
    "    i = np.random.choice(100)\n",
    "    testl = DataLoader(d_test,batch_size=1,shuffle=False,num_workers = 5)\n",
    "    for ind,batch in enumerate(testl):\n",
    "        if ind == i:\n",
    "            vid,text,labels = batch\n",
    "            with torch.no_grad():\n",
    "                PRED = model(vid,text)\n",
    "            base_segs = extract_segs(labels.numpy())\n",
    "            st,end = base_segs[0][0]\n",
    "            first_seg_attn = torch.squeeze(model.emission_model.attn_weights[:,:,:,1]).numpy()\n",
    "            s1 = np.mean(first_seg_attn[st:end+1])\n",
    "            sz = end-st\n",
    "            avgs = []\n",
    "            for i in range(1000):\n",
    "                st_id = np.random.randint(500-sz)\n",
    "                end_id = st_id+sz\n",
    "                m = np.mean(first_seg_attn[st_id:end_id])\n",
    "                avgs.append(m)\n",
    "            from scipy import stats\n",
    "    return stats.percentileofscore(avgs,s1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = model.crf.transitions.detach().cpu().numpy()\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing transition matrix\n",
    "\n",
    "import numpy as np; \n",
    "import seaborn as sns; sns.set_theme()\n",
    "#uniform_data = np.random.rand(10, 12)\n",
    "ax = sns.heatmap(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-be86d3c016ba>:293: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:328.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    }
   ],
   "source": [
    "# Visualizing attention weights\n",
    "\n",
    "batch = next(iter(testl))\n",
    "vid,text,labels = batch\n",
    "with torch.no_grad():\n",
    "    PRED = model(vid,text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 500, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.emission_model.attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(186, 210), (236, 262), (273, 285), (288, 299), (358, 359)]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_segs = extract_segs(labels.numpy())\n",
    "base_segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12230025, 0.11855522, 0.119487  , 0.11929157, 0.11732614,\n",
       "       0.11780079, 0.11494639, 0.11377048, 0.10968129, 0.10929181,\n",
       "       0.10540858, 0.10180341, 0.10655935, 0.10909887, 0.11357592,\n",
       "       0.11971118, 0.11860047, 0.1195482 , 0.11875094, 0.11856016,\n",
       "       0.11857875, 0.11666175, 0.11607134, 0.1165452 , 0.11865624,\n",
       "       0.11851682, 0.12026682, 0.11923701, 0.11878303, 0.11687866,\n",
       "       0.11823639, 0.11776974, 0.11958972, 0.11945596, 0.11965445,\n",
       "       0.11937862, 0.11987484, 0.12277874, 0.12197191, 0.12158917,\n",
       "       0.11518773, 0.11267312, 0.11397988, 0.11515063, 0.11629314,\n",
       "       0.11619922, 0.11395086, 0.11423334, 0.11626251, 0.11654215,\n",
       "       0.11597037, 0.11653413, 0.11827859, 0.11779778, 0.11757927,\n",
       "       0.1170874 , 0.11619654, 0.11651737, 0.1152657 , 0.11407791,\n",
       "       0.11395665, 0.11380307, 0.11554276, 0.11762325, 0.11890599,\n",
       "       0.11779173, 0.11665255, 0.11543538, 0.11453485, 0.11363327,\n",
       "       0.11294598, 0.11286867, 0.11126503, 0.11011329, 0.11159475,\n",
       "       0.11222016, 0.11143585, 0.11211347, 0.10894512, 0.11192242,\n",
       "       0.1118299 , 0.11317838, 0.11335162, 0.11408923, 0.11371411,\n",
       "       0.11425477, 0.11688085, 0.12022993, 0.12093711, 0.12465382,\n",
       "       0.1262185 , 0.12619945, 0.1260285 , 0.1261001 , 0.12522449,\n",
       "       0.12211701, 0.12281524, 0.12338556, 0.12175348, 0.12192525,\n",
       "       0.12040973, 0.11878137, 0.11769563, 0.11750889, 0.11778012,\n",
       "       0.11542535, 0.11703336, 0.11750391, 0.11681534, 0.11720691,\n",
       "       0.11686075, 0.11774062, 0.11745163, 0.11841374, 0.11787869,\n",
       "       0.11652125, 0.11658631, 0.11719514, 0.11715139, 0.11853608,\n",
       "       0.11927334, 0.11736575, 0.11560263, 0.11535162, 0.11628135,\n",
       "       0.11663511, 0.11670289, 0.11786553, 0.11600732, 0.11661223,\n",
       "       0.11816338, 0.11942561, 0.12211312, 0.12578735, 0.12850197,\n",
       "       0.12838975, 0.12678991, 0.1262071 , 0.12654302, 0.12427146,\n",
       "       0.1223723 , 0.12286002, 0.1240619 , 0.124559  , 0.12376918,\n",
       "       0.12600975, 0.12548389, 0.12693433, 0.1296081 , 0.12693238,\n",
       "       0.12547767, 0.12694627, 0.12598866, 0.12590462, 0.12430324,\n",
       "       0.12355559, 0.11882289, 0.11975817, 0.11918057, 0.12243146,\n",
       "       0.12539917, 0.12510969, 0.12784973, 0.12971273, 0.1320171 ,\n",
       "       0.13078336, 0.1306304 , 0.1310324 , 0.13121691, 0.12988873,\n",
       "       0.1280445 , 0.12546597, 0.12328757, 0.12063599, 0.12063769,\n",
       "       0.12156704, 0.12334783, 0.12673633, 0.1283245 , 0.12884384,\n",
       "       0.12922174, 0.130569  , 0.13139558, 0.13166979, 0.13174312,\n",
       "       0.13028334, 0.12979445, 0.12883098, 0.12951623, 0.13329032,\n",
       "       0.13487402, 0.13571994, 0.13546728, 0.13572948, 0.13543065,\n",
       "       0.13305354, 0.13064387, 0.1296823 , 0.12746525, 0.12450577,\n",
       "       0.11953285, 0.11567061, 0.11186377, 0.1094768 , 0.10984382,\n",
       "       0.10947584, 0.10668395, 0.10520069, 0.10520033, 0.10408624,\n",
       "       0.09573874, 0.08913848, 0.08989934, 0.08531913, 0.08279045,\n",
       "       0.09034856, 0.09501458, 0.09435602, 0.09523987, 0.09398295,\n",
       "       0.09106545, 0.09485305, 0.09939595, 0.09813315, 0.09750598,\n",
       "       0.10005909, 0.10042684, 0.08996472, 0.09362615, 0.09214288,\n",
       "       0.08799932, 0.08441077, 0.08310869, 0.08347254, 0.08050255,\n",
       "       0.07929848, 0.07873415, 0.07679373, 0.07648898, 0.07696689,\n",
       "       0.07757821, 0.07693071, 0.073136  , 0.07366613, 0.07204664,\n",
       "       0.07163198, 0.0684547 , 0.06780703, 0.06727134, 0.06848179,\n",
       "       0.06845271, 0.06803942, 0.0673219 , 0.06648261, 0.06355046,\n",
       "       0.06179357, 0.06048454, 0.06073293, 0.059033  , 0.05752327,\n",
       "       0.05611558, 0.05287691, 0.05518677, 0.05673443, 0.06056737,\n",
       "       0.05576175, 0.05394462, 0.05098696, 0.05268946, 0.04820132,\n",
       "       0.04478895, 0.03971862, 0.0363681 , 0.0337506 , 0.03128416,\n",
       "       0.02657352, 0.02573149, 0.025872  , 0.02589747, 0.02521049,\n",
       "       0.02545535, 0.02271954, 0.02243995, 0.02137788, 0.02064909,\n",
       "       0.02070645, 0.02031906, 0.01944429, 0.01884539, 0.01805432,\n",
       "       0.01795127, 0.01738025, 0.0172903 , 0.01631894, 0.01619147,\n",
       "       0.0163738 , 0.01583732, 0.01561369, 0.01557018, 0.01571643,\n",
       "       0.01585187, 0.0148634 , 0.01462952, 0.01429928, 0.01417394,\n",
       "       0.01456731, 0.01482342, 0.0158293 , 0.01644927, 0.01714457,\n",
       "       0.0167931 , 0.01654449, 0.01654565, 0.01706648, 0.01731071,\n",
       "       0.01822935, 0.01920876, 0.020209  , 0.02041213, 0.02150966,\n",
       "       0.02189607, 0.022893  , 0.02389726, 0.02451205, 0.02473085,\n",
       "       0.02497204, 0.02245287, 0.02029784, 0.02110456, 0.02081957,\n",
       "       0.02059879, 0.01975164, 0.01884777, 0.01963799, 0.01988772,\n",
       "       0.02072032, 0.02279611, 0.02627937, 0.02735996, 0.02947541,\n",
       "       0.03115742, 0.03238311, 0.03275107, 0.03036245, 0.03004309,\n",
       "       0.02798465, 0.02624982, 0.02775639, 0.02935436, 0.03339195,\n",
       "       0.02814172, 0.02887293, 0.0288682 , 0.02965222, 0.03015336,\n",
       "       0.03136245, 0.03104475, 0.031456  , 0.03292397, 0.0328406 ,\n",
       "       0.03326731, 0.03379233, 0.03288172, 0.03210975, 0.02908005,\n",
       "       0.02837838, 0.02814356, 0.02871275, 0.02924117, 0.02832087,\n",
       "       0.02724062, 0.02718299, 0.02812765, 0.0289669 , 0.02855846,\n",
       "       0.03191858, 0.03444231, 0.03896068, 0.04273607, 0.04687689,\n",
       "       0.05211182, 0.05745022, 0.06224009, 0.07394962, 0.08282122,\n",
       "       0.09025545, 0.09716692, 0.10379507, 0.10450918, 0.10538602,\n",
       "       0.10869984, 0.1117542 , 0.11153289, 0.11305259, 0.11810393,\n",
       "       0.11921889, 0.11897008, 0.12013935, 0.12019142, 0.12015532,\n",
       "       0.11878431, 0.12111235, 0.12613672, 0.12726314, 0.12901491,\n",
       "       0.13300963, 0.13381644, 0.13557807, 0.13583228, 0.13600658,\n",
       "       0.1368945 , 0.13681763, 0.13771671, 0.13789998, 0.13818163,\n",
       "       0.13874693, 0.13509294, 0.13472055, 0.13701697, 0.13568659,\n",
       "       0.13586815, 0.1367735 , 0.13612227, 0.13259305, 0.13141862,\n",
       "       0.12965477, 0.13264859, 0.13237981, 0.12931983, 0.12578836,\n",
       "       0.12302325, 0.12006309, 0.11720298, 0.11215422, 0.11033902,\n",
       "       0.11184873, 0.10870277, 0.11191324, 0.11300073, 0.1156271 ,\n",
       "       0.11308911, 0.1187055 , 0.12044964, 0.12304072, 0.11993658,\n",
       "       0.12264322, 0.11978002, 0.12394844, 0.12578668, 0.12416549,\n",
       "       0.12390534, 0.12474345, 0.12266197, 0.12019721, 0.11533607,\n",
       "       0.1171748 , 0.10758485, 0.10479305, 0.10236982, 0.10411466,\n",
       "       0.10584327, 0.10683243, 0.10707418, 0.11007897, 0.11898852,\n",
       "       0.11815704, 0.12117868, 0.12281646, 0.12454823, 0.12560195,\n",
       "       0.12638508, 0.12702003, 0.1295484 , 0.13004124, 0.12896068,\n",
       "       0.12823659, 0.12892358, 0.12890363, 0.12932432, 0.1306985 ,\n",
       "       0.13124216, 0.13145348, 0.13089871, 0.13672963, 0.13301851,\n",
       "       0.12983257, 0.13072023, 0.12965137, 0.13195586, 0.1320881 ,\n",
       "       0.13150914, 0.13172042, 0.13221127, 0.13161314, 0.13117087,\n",
       "       0.13043064, 0.13163038, 0.128082  , 0.12640625, 0.12203862],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see if we actually attend to these\n",
    "\n",
    "first_seg_attn = torch.squeeze(model.emission_model.attn_weights[:,:,:,1]).numpy()\n",
    "first_seg_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.mean(first_seg_attn[186:210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09199969, 0.09404079)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(first_seg_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 210-186\n",
    "avgs = []\n",
    "for i in range(1000):\n",
    "    st_id = np.random.randint(500-sz)\n",
    "    end_id = st_id+sz\n",
    "    m = np.mean(first_seg_attn[st_id:end_id])\n",
    "    avgs.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.percentileofscore(avgs,s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.66"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's do this multiple times and average\n",
    "\n",
    "sz = 10\n",
    "attn_w = []\n",
    "for i in range(sz):\n",
    "    w = study_attn()\n",
    "    attn_w.append(w)\n",
    "np.mean(attn_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 105/1603 [05:20<1:16:10,  3.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1a21a3620a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabellis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredlis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-67a506eacf7c>\u001b[0m in \u001b[0;36minference\u001b[0;34m(model, dl)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mvid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mground_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpredlis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-994ca7acbb34>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, vid, txt)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0memissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-be86d3c016ba>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, emissions, mask)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viterbi_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     def _validate(\n",
      "\u001b[0;32m<ipython-input-5-be86d3c016ba>\u001b[0m in \u001b[0;36m_viterbi_decode\u001b[0;34m(self, emissions, mask)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# Find the maximum score over all possible current tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;31m# shape: (batch_size, num_tags)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mnext_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;31m# Set score to the next score if this timestep is valid (mask == 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_acc,labellis,predlis = inference(model,testl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "labellis = joblib.load('labellis.joblib')\n",
    "predlis = joblib.load('predlis.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segs(labels):\n",
    "    #list of lists\n",
    "    labelsegs = []\n",
    "    predsegs = []\n",
    "    for label in labels:\n",
    "        sz = len(set(label))\n",
    "        label = np.array(label)\n",
    "        label_s = []\n",
    "        for i in range(1,sz):\n",
    "            #import pdb;pdb.set_trace()\n",
    "            match_ids, = np.where(label==i)\n",
    "            lst_end = -1\n",
    "            if label_s != []:\n",
    "                for prev_seg in label_s[::-1]:\n",
    "                    if prev_seg:\n",
    "                        lst_end = prev_seg[-1]\n",
    "        \n",
    "            nxt_st, = np.where(match_ids>lst_end)\n",
    "            if nxt_st.size>0:\n",
    "                st_ = match_ids[nxt_st[0]]\n",
    "            else:\n",
    "                label_s.append(())\n",
    "                continue\n",
    "\n",
    "            ind = np.where(match_ids==st_)[0][0]\n",
    "            end_ = st_\n",
    "            for i in range(ind,len(match_ids)):\n",
    "                if i != 0:\n",
    "                    if match_ids[i-1]+1 == match_ids[i]:\n",
    "                        end_ = match_ids[i]\n",
    "                    else:\n",
    "                        break\n",
    "            label_s.append((st_,end_))\n",
    "        labelsegs.append(label_s)\n",
    "    return labelsegs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds: list of lists corresponding to each video. [[(23,67),(89,102)]]\n",
    "import numpy as np\n",
    "def stats(preds,labels,sz=500):\n",
    "    accs = []\n",
    "    recalls = []\n",
    "    ious = []\n",
    "    for pred,label in zip(preds,labels):\n",
    "        # note: during inference we ensure that intervals are non-overlapping\n",
    "        #pred = sorted(pred,key=lambda x: x[0])\n",
    "        #label = sorted(label,key=lambda x: x[0])\n",
    "        pred_sz = len(pred)\n",
    "        lbl_sz = len(label)\n",
    "        pred_lis = np.zeros(sz)\n",
    "        label_lis = np.zeros(sz)\n",
    "        for ind in range(1,lbl_sz+1):\n",
    "            l_st,l_end = label[ind-1]\n",
    "            label_lis[l_st:l_end+1] = ind\n",
    "            if ind <= pred_sz and pred[ind-1]:\n",
    "                p_st,p_end = pred[ind-1]\n",
    "                pred_lis[p_st:l_end+1] = ind\n",
    "            else:\n",
    "                # wrong predictions\n",
    "                pred_lis[l_st:l_end+1] = -1\n",
    "\n",
    "\n",
    "        acc = np.mean(pred_lis == label_lis)\n",
    "        accs.append(acc)\n",
    "        #calc recall\n",
    "        #calc iou\n",
    "        inter_cnt = 0\n",
    "        union_cnt = 0\n",
    "        recall_match = 0.0\n",
    "        recall_cnt = 0\n",
    "        for p_seg,l_seg in zip(pred,label):\n",
    "            ls = set(range(l_seg[0],l_seg[-1]+1))\n",
    "            recall_cnt += 1\n",
    "            if not p_seg: \n",
    "                union_cnt += len(ls)\n",
    "                continue\n",
    "            ps = set(range(p_seg[0],p_seg[-1]+1))\n",
    "            inter_cnt += len(ps.intersection(ls))\n",
    "            union_cnt += len(ps.union(ls))\n",
    "            if l_seg[0]<=p_seg[0]<=l_seg[-1]:\n",
    "                recall_match += 1\n",
    "            #if ps.issubset(ls):\n",
    "            #recall_score.append(len(ls.intersection(ps))/len(ls))\n",
    "            # correctly assign or fall into ground truth interval\n",
    "            #recall_score += 1\n",
    "        ms = len(label)-len(pred)\n",
    "        if ms>0:\n",
    "            for i in range(ms):\n",
    "                recall_cnt += 1\n",
    "                #recall_score.append(0.0)\n",
    "        if union_cnt:\n",
    "            ious.append(inter_cnt/union_cnt)\n",
    "            recalls.append(recall_match/recall_cnt)\n",
    "        else:\n",
    "            print(pred)\n",
    "            print(label)\n",
    "    return np.mean(accs),np.mean(recalls),np.mean(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [ele[0] for ele in predlis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = labellis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_s = extract_segs(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_s = extract_segs(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_random_model(p_s,l_s):\n",
    "    rs = []\n",
    "    rious = []\n",
    "    for i in range(100):\n",
    "        r_s = []\n",
    "        g_s = []\n",
    "        for i,segs in enumerate(p_s):\n",
    "            rp_s = []\n",
    "            rg_s = []\n",
    "            #randomly select a segment\n",
    "            s = sorted(np.random.choice(500,2,replace=False))\n",
    "            rg_s.append(l_s[i][0])\n",
    "            rp_s.append((s[0],s[-1]))\n",
    "\n",
    "            r_s.append(rp_s)\n",
    "            g_s.append(rg_s)\n",
    "        _,rrecall,riou = stats(r_s,g_s)\n",
    "        rs.append(rrecall)\n",
    "        rious.append(riou)\n",
    "    return np.mean(rs),np.mean(rious)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1165206402695872, 0.06118370194891794)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stats for a random model\n",
    "eval_random_model(p_s,l_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[(84, 161), (163, 173), (182, 205), (299, 305), (310, 336)]\n",
      "[]\n",
      "[(60, 71), (88, 98), (126, 132), (138, 158), (172, 172), (215, 220), (280, 282), (357, 386), (413, 436), (480, 484)]\n",
      "[]\n",
      "[(214, 232), (233, 252), (253, 269), (270, 276), (277, 313), (314, 352)]\n",
      "[]\n",
      "[(44, 65), (85, 113), (131, 197), (198, 230), (239, 244), (253, 262), (281, 283), (293, 304), (310, 315), (345, 357), (358, 367), (386, 401), (421, 427)]\n",
      "[]\n",
      "[(52, 84), (93, 144), (146, 187), (197, 203), (211, 226), (229, 247), (274, 281), (283, 293), (325, 330), (349, 357), (373, 400), (424, 429)]\n"
     ]
    }
   ],
   "source": [
    "acc,recall,iou = stats(p_s,l_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3401451246628912"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5320176563920543"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7973007582139848"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Crossmdl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
