{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import shutil\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from argparse import Namespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import linalg as LA\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "#wandb_logger = lambda dir, version: WandbLogger(\n",
    " #   name=\"wandb\", save_dir=dir, version=version\n",
    "#)\n",
    "def wandb_logger(dir,version,name):\n",
    "    return WandbLogger(\n",
    "    name=name, save_dir=dir, version=version\n",
    ")\n",
    "csvlogger = lambda dir, version: CSVLogger(dir, name=\"csvlogs\", version=version)\n",
    "tblogger = lambda dir, version: TensorBoardLogger(dir, name=\"tblogs\", version=version)\n",
    "\n",
    "def get_loggers(dir,version,name,lis=[\"csv\"]):\n",
    "    lgrs = []\n",
    "    if \"wandb\" in lis:\n",
    "        lgrs.append(wandb_logger(dir, version))\n",
    "    if \"csv\" in lis:\n",
    "        lgrs.append(csvlogger(dir, version))\n",
    "    if \"tb\" in lis:\n",
    "        lgrs.append(tblogger(dir, version))\n",
    "    return lgrs\n",
    "\n",
    "#global vars\n",
    "DATA_DIR = '/common/home/vk405/Projects/Crossmdl/Data/Recipe/'\n",
    "EMB_TRN = DATA_DIR+'embeddings_train1.pkl'\n",
    "EMB_VAL = DATA_DIR+'embeddings_val1.pkl'\n",
    "ING_TRN = DATA_DIR+'ingredients_embeddings_train.pkl'\n",
    "ING_VAL = DATA_DIR+'ingredients_embeddings_val.pkl'\n",
    "#os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open(ING_TRN, 'rb') as files:\n",
    "    ing_trn = pickle.load(files)\n",
    "with open(ING_VAL, 'rb') as files:\n",
    "    ing_val = pickle.load(files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ing_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281598, 1024)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ing_trn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDset(Dataset):\n",
    "    def __init__(self,data_dir= '/common/home/vk405/Projects/Crossmdl/Data/Recipe/'\\\n",
    "        ,split='train',txt_emb_type='total'):\n",
    "        self.DATA_DIR = data_dir\n",
    "        self.init_data_locs()\n",
    "        self.txt_emb_type = txt_emb_type\n",
    "        self.split = split\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            with open(self.EMB_TRN, 'rb') as files:\n",
    "                self.emb_vid,_,_ = pickle.load(files)\n",
    "            if self.txt_emb_type == 'total':\n",
    "                with open(self.EMB_TRN, 'rb') as files:\n",
    "                    self.emb_vid,self.emb_txt,self.ids = pickle.load(files)\n",
    "            elif self.txt_emb_type == 'instructions':\n",
    "                with open(self.INS_TRN, 'rb') as files:\n",
    "                    self.emb_txt,self.ids = pickle.load(files)\n",
    "            elif self.txt_emb_type == 'ingredients':\n",
    "                with open(self.ING_TRN, 'rb') as files:\n",
    "                    self.emb_txt,self.ids = pickle.load(files)\n",
    "            else:\n",
    "                #title\n",
    "                with open(self.TIT_TRN, 'rb') as files:\n",
    "                    self.emb_txt,self.ids = pickle.load(files)\n",
    "\n",
    "\n",
    "            \n",
    "        elif self.split == 'valid':\n",
    "            with open(self.EMB_VAL, 'rb') as files:\n",
    "                    self.emb_vid,_,_ = pickle.load(files)\n",
    "            if self.txt_emb_type == 'total':\n",
    "                with open(self.EMB_VAL, 'rb') as files:\n",
    "                    self.emb_vid,self.emb_txt,self.ids = pickle.load(files)\n",
    "            elif self.txt_emb_type == 'instructions':\n",
    "                with open(self.INS_VAL, 'rb') as files:\n",
    "                    self.emb_txt,self.ids = pickle.load(files)\n",
    "            elif self.txt_emb_type == 'ingredients':\n",
    "                with open(self.ING_VAL, 'rb') as files:\n",
    "                    self.emb_txt,self.ids = pickle.load(files)\n",
    "            else:\n",
    "                #title\n",
    "                with open(self.TIT_VAL, 'rb') as files:\n",
    "                    self.emb_txt,self.ids = pickle.load(files)\n",
    "\n",
    "        elif self.split == 'test':\n",
    "            with open(self.EMB_TST, 'rb') as files:\n",
    "                self.emb_vid,_,_ = pickle.load(files)\n",
    "\n",
    "            if self.txt_emb_type == 'total':\n",
    "                with open(self.EMB_TST, 'rb') as files:\n",
    "                    self.emb_vid,self.emb_txt,self.ids = pickle.load(files)\n",
    "            elif self.txt_emb_type == 'instructions':\n",
    "                with open(self.INS_TST, 'rb') as files:\n",
    "                    self.emb_txt,self.ids = pickle.load(files)\n",
    "            elif self.txt_emb_type == 'ingredients':\n",
    "                with open(self.ING_TST, 'rb') as files:\n",
    "                    self.emb_txt,self.ids = pickle.load(files)\n",
    "            else:\n",
    "                #title\n",
    "                with open(self.TIT_TST, 'rb') as files:\n",
    "                    self.emb_txt,self.ids = pickle.load(files)\n",
    "\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        rind = idx+1\n",
    "        if rind == len(self.emb_txt):\n",
    "            rind = idx-1\n",
    "        return self.emb_vid[idx],self.emb_txt[idx],self.emb_vid[rind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emb_txt)\n",
    "\n",
    "    def init_data_locs(self):\n",
    "        #TOTAL EMBEDDINGS\n",
    "        self.EMB_TRN = self.DATA_DIR+'embeddings_train1.pkl'\n",
    "        self.EMB_VAL = self.DATA_DIR+'embeddings_val1.pkl'\n",
    "        self.EMB_TST = self.DATA_DIR+'embeddings_test1.pkl'\n",
    "        #INGRIDIENTS Embeddings\n",
    "        self.ING_TRN = self.DATA_DIR+'ingredients_embeddings_train.pkl'\n",
    "        self.ING_VAL = self.DATA_DIR+'ingredients_embeddings_val.pkl'\n",
    "        self.ING_TST = self.DATA_DIR + 'ingredients_embeddings_test.pkl'\n",
    "\n",
    "        #TITLE EMBEDDINGS\n",
    "        self.TIT_TRN = self.DATA_DIR+'title_embeddings_train.pkl'\n",
    "        self.TIT_VAL = self.DATA_DIR+'title_embeddings_val.pkl'\n",
    "        self.TIT_TST = self.DATA_DIR + 'title_embeddings_test.pkl'\n",
    "\n",
    "        #Instructions\n",
    "        self.INS_TRN = self.DATA_DIR+'instructions_embeddings_train.pkl'\n",
    "        self.INS_VAL = self.DATA_DIR+'instructions_embeddings_val.pkl'\n",
    "        self.INS_TST = self.DATA_DIR+'instructions_embeddings_test.pkl'\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trn_data = RecipeDset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbModel(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.lyrs = []\n",
    "        dim = self.params['input_dim']\n",
    "        for i in range(self.params['lyrs']):\n",
    "            if \"emb_dim\" not in self.params:\n",
    "                lyr = nn.Linear(dim,dim)\n",
    "            else:\n",
    "                prev_dim = dim\n",
    "                out_dim = self.params['emb_dim'][i]\n",
    "                if i>0:\n",
    "                    prev_dim = self.params['emb_dim'][i-1]\n",
    "                lyr = nn.Linear(prev_dim,out_dim)\n",
    "            if self.params['act'] == 'relu':\n",
    "                non_lin = nn.ReLU()\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            self.lyrs.append(lyr)\n",
    "            self.lyrs.append(non_lin)\n",
    "        self.feedforward = nn.Sequential(*self.lyrs)\n",
    "    def forward(self,x):\n",
    "        return self.feedforward(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RecipeModel(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.txt_emb = EmbModel(hparams.txt_model)\n",
    "        self.img_emb = EmbModel(hparams.img_model)\n",
    "        self.shared = nn.Linear(hparams.txt_model['fin_dim'],\\\n",
    "            hparams.shared_emb_dim)\n",
    "    def forward(self,x):\n",
    "        # Ignores anchor embedding\n",
    "        img,txt = x\n",
    "        img_emb = self.img_emb(img)\n",
    "        txt_emb = self.txt_emb(txt)\n",
    "        #anch_img_emb = self.img_emb(anch_img)\n",
    "\n",
    "        img_fin_emb = self.shared(img_emb)\n",
    "        txt_fin_emb = self.shared(txt_emb)\n",
    "        #anch_img_fin_emb = self.shared(anch_img_emb)\n",
    "        return img_fin_emb,txt_fin_emb\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        img,txt,anch_img = batch\n",
    "        anch_img_emb = self.img_emb(anch_img)\n",
    "        anch_img_fin_emb = self.shared(anch_img_emb)\n",
    "        img_fin_emb,txt_fin_emb = self((img,txt))\n",
    "\n",
    "        loss,log_losses = self.get_loss(img_fin_emb,txt_fin_emb,anch_img_fin_emb)\n",
    "        self.log(\"train_loss\",loss,on_step=True)\n",
    "        self.log(\"cos_sim_n\",log_losses[0],on_step=True)\n",
    "        self.log(\"cos_sim_p\",log_losses[-1],on_step=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        img,txt,anch_img = batch\n",
    "        anch_img_emb = self.img_emb(anch_img)\n",
    "        anch_img_fin_emb = self.shared(anch_img_emb)\n",
    "        img_fin_emb,txt_fin_emb = self((img,txt))\n",
    "        loss,log_losses = self.get_loss(img_fin_emb,txt_fin_emb,anch_img_fin_emb)\n",
    "        #collects batchwise stats for rank over half of batchsize\n",
    "        sz = int(img_fin_emb.shape[0]//2)\n",
    "        median,recall = self.rank('image',img_fin_emb.detach().cpu().numpy(),txt_fin_emb.detach().cpu().numpy(),sz)\n",
    "\n",
    "        self.log(\"val_loss\",loss,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_cos_sim_n\",log_losses[0],on_step=False, on_epoch=True)\n",
    "        self.log(\"val_cos_sim_p\",log_losses[-1],on_step=False, on_epoch=True)\n",
    "        # main metrics\n",
    "        self.log(\"val_medianrank\",median,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall_1\",recall[1],on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall_5\",recall[5],on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall_10\",recall[10],on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    # inference code\n",
    "\n",
    "    def get_loss(self,img,txt,anch,reduce='mean'):\n",
    "        #ùêøùëêùëúùë†(ùíÇ,ùíë,ùíè)=max[ùëë(ùíÇ,ùíè)‚àíùëë(ùíÇ,ùíë)+ùúñ,0]\n",
    "        eps = self.hparams.eps if 'eps' in self.hparams else 1e-8\n",
    "        \n",
    "        im_norm,txt_norm,anch_norm = LA.norm(img,dim=-1).reshape(img.shape[0],1),\\\n",
    "        LA.norm(txt,dim=-1).reshape(txt.shape[0],1),LA.norm(anch,dim=-1).reshape(anch.shape[0],1)\n",
    "        normd_img = img/im_norm\n",
    "        normd_txt = txt/txt_norm\n",
    "        normd_anch = anch/anch_norm\n",
    "\n",
    "        cos_sim_p = torch.sum(normd_img*normd_txt,dim=-1)\n",
    "        cos_sim_n = torch.sum(normd_anch*normd_txt,dim=-1)\n",
    "        if 'use_mse' in self.hparams and self.hparams['use_mse']:\n",
    "            #print(f\"USING MSE LOSS\")\n",
    "            loss  = F.mse_loss(img,txt)\n",
    "        else:\n",
    "            unclipped_loss = cos_sim_n-cos_sim_p+eps\n",
    "            loss = torch.relu(unclipped_loss)\n",
    "        if reduce == 'mean':\n",
    "            return torch.mean(loss),(torch.mean(cos_sim_n),torch.mean(cos_sim_p))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr if 'lr' in self.hparams else 1e-3\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        return optimizer\n",
    "\n",
    "    def squared_sim(self,a,b):\n",
    "        sims = []\n",
    "        for i in range(len(a)):\n",
    "            s = ((b-a[i])**2).sum(axis=-1)\n",
    "            sims.append(s)\n",
    "        return np.stack(sims)\n",
    "\n",
    "    def rank(self,type_embedding , img_embeds, rec_embeds, samples):\n",
    "        random.seed(42)\n",
    "        im_vecs = img_embeds \n",
    "        instr_vecs = rec_embeds \n",
    "\n",
    "\n",
    "        # Sort based on names to always pick same samples for medr\n",
    "    #     idxs = np.argsort(names)\n",
    "    #     names = names[idxs]\n",
    "\n",
    "        # Ranker\n",
    "        N = samples\n",
    "        idxs = range(N)\n",
    "        \n",
    "        glob_rank = []\n",
    "        glob_recall = {1:0.0,5:0.0,10:0.0}\n",
    "        for i in range(10):\n",
    "\n",
    "            ids = random.sample(range(0,len(img_embeds)), N)\n",
    "            im_sub = im_vecs[ids,:]\n",
    "            instr_sub = instr_vecs[ids,:]\n",
    "    #         ids_sub = names[ids]\n",
    "\n",
    "            # if params.embedding == 'image':\n",
    "            if type_embedding == 'image':\n",
    "                if 'use_mse' in self.hparams and self.hparams['use_mse']:\n",
    "                    sims = self.squared_sim(im_sub,instr_sub)\n",
    "                else:\n",
    "                    sims = np.dot(im_sub,instr_sub.T) # for im2recipe\n",
    "            else:\n",
    "                if 'use_mse' in self.hparams and self.hparams['use_mse']:\n",
    "                    sims = self.squared_sim(instr_sub,im_sub)\n",
    "                else:\n",
    "                    sims = np.dot(instr_sub,im_sub.T) # for recipe2im\n",
    "\n",
    "            med_rank = []\n",
    "            recall = {1:0.0,5:0.0,10:0.0}\n",
    "\n",
    "            for ii in idxs:\n",
    "\n",
    "    #             name = ids_sub[ii]\n",
    "                # get a column of similarities\n",
    "                sim = sims[ii,:]\n",
    "\n",
    "                # sort indices in descending order\n",
    "                sorting = np.argsort(sim)[::-1].tolist()\n",
    "\n",
    "                # find where the index of the pair sample ended up in the sorting\n",
    "                pos = sorting.index(ii)\n",
    "\n",
    "                if (pos+1) == 1:\n",
    "                    recall[1]+=1\n",
    "                if (pos+1) <=5:\n",
    "                    recall[5]+=1\n",
    "                if (pos+1)<=10:\n",
    "                    recall[10]+=1\n",
    "\n",
    "                # store the position\n",
    "                med_rank.append(pos+1)\n",
    "\n",
    "            for i in recall.keys():\n",
    "                recall[i]=recall[i]/N\n",
    "\n",
    "            med = np.median(med_rank)\n",
    "            # print \"median\", med\n",
    "\n",
    "            for i in recall.keys():\n",
    "                glob_recall[i]+=recall[i]\n",
    "            glob_rank.append(med)\n",
    "\n",
    "        for i in glob_recall.keys():\n",
    "            glob_recall[i] = glob_recall[i]/10\n",
    "\n",
    "        return np.average(glob_rank), glob_recall\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cfg):\n",
    "    pl.seed_everything(cfg.seed)\n",
    "    dir = cfg.artifacts_loc\n",
    "    version = str(cfg.version)\n",
    "    logger_list = get_loggers(dir, version,cfg.version,cfg.loggers)\n",
    "    cbs = []\n",
    "    if \"early_stop\" in cfg.cbs:\n",
    "        #? does'nt really work atm\n",
    "        params = cfg.early_stop\n",
    "        earlystopcb = EarlyStopping(**params, min_delta=0.00, verbose=False)\n",
    "        cbs.append(earlystopcb)\n",
    "    if \"checkpoint\" in cfg.cbs:\n",
    "        store_path = dir + \"ckpts/\" + str(cfg.version) + \"/\"\n",
    "        isExist = os.path.exists(store_path)\n",
    "        # first remove\n",
    "        if isExist and os.path.isdir(store_path):\n",
    "            shutil.rmtree(store_path)\n",
    "        # then create fresh\n",
    "        if not isExist:\n",
    "            os.makedirs(store_path)\n",
    "        fname = \"{epoch}-{val_recall_1:.2f}\"\n",
    "        params = cfg.checkpoint\n",
    "        checkptcb = ModelCheckpoint(**params, dirpath=store_path, filename=fname)\n",
    "        cbs.append(checkptcb)\n",
    "\n",
    "    if 'wandb' in cfg.loggers:\n",
    "        wandb.init(project=\"RecipeRetrieval\", config=cfg)\n",
    "        \n",
    "    if cfg.mode == 'train':\n",
    "        recipedata_trn = RecipeDset(data_dir=cfg.data_dir,split='train',\\\n",
    "            txt_emb_type = cfg.txt_emb_type)\n",
    "        recipedata_vld = RecipeDset(data_dir=cfg.data_dir,split='valid',\\\n",
    "            txt_emb_type = cfg.txt_emb_type)\n",
    "\n",
    "        train_loader = DataLoader(recipedata_trn,batch_size=cfg.batch_size,shuffle=True,\\\n",
    "            num_workers=4,pin_memory=True)    \n",
    "        \n",
    "        valid_loader = DataLoader(recipedata_vld,batch_size=cfg.val_batch_size,shuffle=False)\n",
    "        net = RecipeModel(cfg)\n",
    "        if 'use_cpu' in cfg and cfg.use_cpu:\n",
    "            trainer = pl.Trainer(\n",
    "            logger=logger_list,callbacks=cbs,deterministic=True, **cfg.trainer\n",
    "        )\n",
    "        else:\n",
    "            trainer = pl.Trainer(\n",
    "            logger=logger_list,callbacks=cbs, gpus=1,deterministic=True, **cfg.trainer\n",
    "                   )\n",
    "        trainer.fit(net, train_loader,valid_loader)\n",
    "        return trainer\n",
    "        #trainer.tune(net,train_loader)\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = Namespace(\n",
    "    seed = 0,\n",
    "    version = 'retrieval',\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/Recipe/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/Recipe/\",\n",
    "    mode = 'train',\n",
    "    txt_model = {'input_dim':1024,'lyrs':2,'fin_dim':1024,'act':'relu'},\n",
    "    img_model = {'input_dim':1024,'lyrs':2,'fin_dim':1024,'act':'relu'},\n",
    "    shared_emb_dim = 1024,\n",
    "    txt_emb_type = 'total',\n",
    "    lr = 1e-4,\n",
    "    eps = 0.3,\n",
    "    loggers = [\"csv\"],\n",
    "    cbs = [\"checkpoint\",\"early_stop\"],\n",
    "    trainer = {'log_every_n_steps': 50,\n",
    "    'max_epochs': 10},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"val_recall_1\"},\n",
    "    early_stop = {\"monitor\":\"val_recall_1\",\"patience\":2,\"mode\":'max'},\n",
    "    batch_size=512,\n",
    "    val_batch_size = 2000\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1 : With default hyperparameters I find the best eps. This let's me find a base eps to further tune hyperparameters. I use early stopping on `validatian recall@1`. Below code is commented for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With eps:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    }
   ],
   "source": [
    "# # 1. hp tuning -> best eps\n",
    "\n",
    "# import copy\n",
    "# #search over optimal eta\n",
    "# #find best image to recipe retrieval measured on the basis of R@1 (with sample sz =1000)\n",
    "# eps = [0.1,0.3,0.5,0.7,0.9,1.1,1.3,1.5]\n",
    "\n",
    "\n",
    "# for ep in eps:\n",
    "#     print(f\"With eps:{ep}\")\n",
    "#     new_cfg = copy.deepcopy(cfg)\n",
    "#     new_cfg.version += f\"eps_{ep}\"\n",
    "#     new_cfg.eps = ep\n",
    "#     _ = run(new_cfg)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we trained and stored best models for various eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps:0.1\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.0026270125526934,epoch:3.0,recall_1:0.4291946589946747\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.0027215878944844,epoch:1.0,recall_1:0.4407036900520324\n",
      "eps:0.3\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.010527117177844,epoch:4.0,recall_1:0.5049716830253601\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.0106477253139019,epoch:2.0,recall_1:0.5087153315544128\n",
      "eps:0.5\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.0329511947929859,epoch:3.0,recall_1:0.4995366632938385\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.0343403033912181,epoch:1.0,recall_1:0.5137135982513428\n",
      "eps:defaultbest\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.0328888893127441,epoch:5.0,recall_1:0.4922743439674377\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.0343403033912181,epoch:1.0,recall_1:0.5137135982513428\n",
      "eps:0.7\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.0782195851206779,epoch:3.0,recall_1:0.4124027788639068\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.0834617316722869,epoch:1.0,recall_1:0.4294826686382293\n",
      "eps:0.9\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.1590191125869751,epoch:2.0,recall_1:0.2053986936807632\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.1829883754253387,epoch:0.0,recall_1:0.2197808772325515\n",
      "eps:1.1\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.279838889837265,epoch:2.0,recall_1:0.0625169649720192\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.3180159330368042,epoch:0.0,recall_1:0.0885869413614273\n",
      "eps:1.3\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.4469697773456573,epoch:3.0,recall_1:0.0305418577045202\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.4618954062461853,epoch:1.0,recall_1:0.0355367250740528\n",
      "eps:1.5\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.6316236257553101,epoch:3.0,recall_1:0.0118830883875489\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.6561853885650635,epoch:1.0,recall_1:0.0145245101302862\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "met = '/common/home/vk405/Projects/Crossmdl/nbs/Recipe/csvlogs/'\n",
    "dirs = os.listdir(met)\n",
    "filt_dirs = sorted(list(filter(lambda x:'retrievaleps' in x,dirs)),key=lambda x:float(x.split('_')[1]))\n",
    "for dir in filt_dirs:\n",
    "    loc = met+dir+'/metrics.csv'\n",
    "    print(f\"eps:{dir.split('_')[-1]}\")\n",
    "    df = pd.read_csv(loc)\n",
    "    fil_df = df[['val_loss','val_recall_1','epoch']].dropna()\n",
    "    min_loss_id = fil_df['val_loss'].argmin()\n",
    "    max_recall_id = fil_df['val_recall_1'].argmax()\n",
    "    print('taking epoch with min-val-loss')\n",
    "    print(f\"min_val_loss:{fil_df.iloc[min_loss_id]['val_loss']},epoch:{fil_df.iloc[min_loss_id]['epoch']},recall_1:{fil_df.iloc[min_loss_id]['val_recall_1']}\")\n",
    "    print('taking epoch with max-recall-1')\n",
    "\n",
    "    print(f\"min_val_loss:{fil_df.iloc[max_recall_id]['val_loss']},epoch:{fil_df.iloc[max_recall_id]['epoch']},recall_1:{fil_df.iloc[max_recall_id]['val_recall_1']}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I FOUND `eps = 0.5` to be best. Now I tune for best learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:122: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\n",
      "Finding best initial lr:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [00:00<00:00, 122.55it/s]Restoring states from the checkpoint path at /common/home/vk405/Projects/Crossmdl/nbs/lr_find_temp_model_7a888bda-859c-480e-a590-32e8131c9de7.ckpt\n",
      "Finding best initial lr: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 86.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# find best learning rate for eps=0.5\n",
    "\n",
    "recipedata_trn = RecipeDset(data_dir=cfg.data_dir,split='train',\\\n",
    "            txt_emb_type = cfg.txt_emb_type)\n",
    "\n",
    "train_loader = DataLoader(recipedata_trn,batch_size=cfg.batch_size,shuffle=True,\\\n",
    "            num_workers=4,pin_memory=True)  \n",
    "\n",
    "net = RecipeModel(cfg)  \n",
    "\n",
    "trainer = pl.Trainer(\n",
    "auto_lr_find=True,callbacks=[], gpus=1,deterministic=True)\n",
    "\n",
    "lr_finder = trainer.tuner.lr_find(net,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010964781961431851"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lr = lr_finder.suggestion()\n",
    "new_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference = use starting LR = 1e-3 but i still stick with 1e-4 in my experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    }
   ],
   "source": [
    "# best recall = 0.51,epoch:1.0,eps = 0.5\n",
    "# Now train this model for more epochs and track the validation results(no early stopping.)\n",
    "import copy\n",
    "ep = 0.5\n",
    "lr  = 1e-3\n",
    "new_cfg = copy.deepcopy(cfg)\n",
    "cbs = [\"checkpoint\"]\n",
    "checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"val_recall_1\",\"save_top_k\":2}\n",
    "\n",
    "new_cfg.version += f\"eps_{ep}_defaultbest\"\n",
    "new_cfg.cbs = cbs\n",
    "new_cfg.checkpoint = checkpoint\n",
    "new_cfg.eps = ep\n",
    "_ = run(new_cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fixing eps=0.5 and tuning over #layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6316236257553101"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference:hasn't improved on recall@1. Tuning over #layers\n",
    "txt_model = {'input_dim':1024,'lyrs':2,'fin_dim':1024,'act':'relu'}\n",
    "img_model = {'input_dim':1024,'lyrs':2,'fin_dim':1024,'act':'relu'}\n",
    "lyrs = [1,3,4]\n",
    "\n",
    "for lyr in lyrs:\n",
    "    print(f\"lyr:{lyr}\")\n",
    "    t = copy.deepcopy(txt_model)\n",
    "    i = copy.deepcopy(img_model)\n",
    "    t['lyrs'] = lyr\n",
    "    i['lyrl'] = i\n",
    "    ep = 0.5\n",
    "    new_cfg = copy.deepcopy(cfg)\n",
    "    new_cfg.eps = ep\n",
    "    new_cfg.version += f\"lyr_{lyr}\"\n",
    "    new_cfg.txt_model = t\n",
    "    new_cfg.img_model = i\n",
    "    run(new_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "met = '/common/home/vk405/Projects/Crossmdl/nbs/Recipe/csvlogs/'\n",
    "dirs = os.listdir(met)\n",
    "filt_dirs = sorted(list(filter(lambda x:'retrievallyr' in x,dirs)),key=lambda x:float(x.split('_')[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:1\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.0329307466745376,epoch:2.0,recall_1:0.5373440384864807\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.036457534879446,epoch:0.0,recall_1:0.5445466637611389\n",
      "layers:3\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.0337071418762207,epoch:5.0,recall_1:0.4441097676753998\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.0338795185089111,epoch:4.0,recall_1:0.446059376001358\n",
      "layers:4\n",
      "taking epoch with min-val-loss\n",
      "min_val_loss:0.038360733538866,epoch:9.0,recall_1:0.2205686867237091\n",
      "taking epoch with max-recall-1\n",
      "min_val_loss:0.038360733538866,epoch:9.0,recall_1:0.2205686867237091\n"
     ]
    }
   ],
   "source": [
    "for dir in filt_dirs:\n",
    "    loc = met+dir+'/metrics.csv'\n",
    "    print(f\"layers:{dir.split('_')[-1]}\")\n",
    "    df = pd.read_csv(loc)\n",
    "    fil_df = df[['val_loss','val_recall_1','epoch']].dropna()\n",
    "    min_loss_id = fil_df['val_loss'].argmin()\n",
    "    max_recall_id = fil_df['val_recall_1'].argmax()\n",
    "    print('taking epoch with min-val-loss')\n",
    "    print(f\"min_val_loss:{fil_df.iloc[min_loss_id]['val_loss']},epoch:{fil_df.iloc[min_loss_id]['epoch']},recall_1:{fil_df.iloc[min_loss_id]['val_recall_1']}\")\n",
    "    print('taking epoch with max-recall-1')\n",
    "\n",
    "    print(f\"min_val_loss:{fil_df.iloc[max_recall_id]['val_loss']},epoch:{fil_df.iloc[max_recall_id]['epoch']},recall_1:{fil_df.iloc[max_recall_id]['val_recall_1']}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_dim:256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loggers/csv_logs.py:57: UserWarning: Experiment logs directory /common/home/vk405/Projects/Crossmdl/nbs/Recipe/csvlogs/retrievalhiddim_256 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name    | Type     | Params\n",
      "-------------------------------------\n",
      "0 | txt_emb | EmbModel | 262 K \n",
      "1 | img_emb | EmbModel | 262 K \n",
      "2 | shared  | Linear   | 65.8 K\n",
      "-------------------------------------\n",
      "590 K     Trainable params\n",
      "0         Non-trainable params\n",
      "590 K     Total params\n",
      "2.362     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 581/581 [00:54<00:00, 10.58it/s, loss=0.0106, v_num=_256] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hidden_dim:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loggers/csv_logs.py:57: UserWarning: Experiment logs directory /common/home/vk405/Projects/Crossmdl/nbs/Recipe/csvlogs/retrievalhiddim_512 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name    | Type     | Params\n",
      "-------------------------------------\n",
      "0 | txt_emb | EmbModel | 524 K \n",
      "1 | img_emb | EmbModel | 524 K \n",
      "2 | shared  | Linear   | 262 K \n",
      "-------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.249     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 581/581 [00:57<00:00, 10.03it/s, loss=0.0123, v_num=_512]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hidden_dim:2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loggers/csv_logs.py:57: UserWarning: Experiment logs directory /common/home/vk405/Projects/Crossmdl/nbs/Recipe/csvlogs/retrievalhiddim_2048 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name    | Type     | Params\n",
      "-------------------------------------\n",
      "0 | txt_emb | EmbModel | 2.1 M \n",
      "1 | img_emb | EmbModel | 2.1 M \n",
      "2 | shared  | Linear   | 4.2 M \n",
      "-------------------------------------\n",
      "8.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.4 M     Total params\n",
      "33.579    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 581/581 [01:05<00:00,  8.87it/s, loss=0.0119, v_num=2048]\n"
     ]
    }
   ],
   "source": [
    "# inference: layers:1,min_val_loss:0.036457534879446,epoch:0.0,recall_1:0.5445466637611389\n",
    "# searching over embedding layers\n",
    "\n",
    "import copy\n",
    "\n",
    "txt_model = {'input_dim':1024,'lyrs':2,'fin_dim':1024,'act':'relu'}\n",
    "img_model = {'input_dim':1024,'lyrs':2,'fin_dim':1024,'act':'relu'}\n",
    "# 1024 is already there.\n",
    "hidden_dim = [256,512,2048]\n",
    "\n",
    "for hdim in hidden_dim:\n",
    "\n",
    "    print(f\"hidden_dim:{hdim}\")\n",
    "    t = copy.deepcopy(txt_model)\n",
    "    i = copy.deepcopy(img_model)\n",
    "\n",
    "    t['fin_dim'] = hdim\n",
    "    t['lyrs'] = 1\n",
    "    t['emb_dim'] = [hdim]\n",
    "\n",
    "    i['fin_dim'] = hdim\n",
    "    i['lyrs'] = 1\n",
    "    i['emb_dim'] = [hdim]\n",
    "\n",
    "    \n",
    "    \n",
    "    ep = 0.5\n",
    "    new_cfg = copy.deepcopy(cfg)\n",
    "    new_cfg.shared_emb_dim = hdim\n",
    "    new_cfg.img_model = i\n",
    "    new_cfg.txt_model = t\n",
    "    new_cfg.eps = ep\n",
    "    new_cfg.version += f\"hiddim_{hdim}\"\n",
    "\n",
    "    run(new_cfg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Recall @1 is with layer=1 and hid_dim = 1024 with 0.54. We can use this model to check on the results in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "# Now training on typical cca loss (squared error),just add a  flag `use_mse`\n",
    "\n",
    "# Inference: Not good at all.\n",
    "\n",
    "cfg = Namespace(\n",
    "    seed = 0,\n",
    "    version = 'retrievalMSElyr_1',\n",
    "    use_mse = True,\n",
    "    use_cpu = True,\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/Recipe/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/Recipe/\",\n",
    "    mode = 'train',\n",
    "    txt_model = {'input_dim':1024,'lyrs':1,'fin_dim':1024,'act':'relu'},\n",
    "    img_model = {'input_dim':1024,'lyrs':1,'fin_dim':1024,'act':'relu'},\n",
    "    shared_emb_dim = 1024,\n",
    "    txt_emb_type = 'total',\n",
    "    lr = 1e-4,\n",
    "    eps = 0.5,\n",
    "    loggers = [\"csv\"],\n",
    "    cbs = [\"checkpoint\",\"early_stop\"],\n",
    "    trainer = {'log_every_n_steps': 50,\n",
    "    'max_epochs': 10},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"val_recall_1\"},\n",
    "    early_stop = {\"monitor\":\"val_recall_1\",\"patience\":2,\"mode\":'max'},\n",
    "    batch_size=512,\n",
    "    val_batch_size = 2000\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name    | Type     | Params\n",
      "-------------------------------------\n",
      "0 | txt_emb | EmbModel | 1.0 M \n",
      "1 | img_emb | EmbModel | 1.0 M \n",
      "2 | shared  | Linear   | 1.0 M \n",
      "-------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.595    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 581/581 [02:10<00:00,  4.44it/s, loss=0.0302, v_num=yr_1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer at 0x7fc672699040>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now just for comparision run on different embeddings - instructions,ingredients,title\n",
    "\n",
    "# with txt_emb_type = 'instructions'\n",
    "cfg = Namespace(\n",
    "    seed = 0,\n",
    "    version = 'retrievalINSlyr_1',\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/Recipe/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/Recipe/\",\n",
    "    mode = 'train',\n",
    "    txt_model = {'input_dim':1024,'lyrs':1,'fin_dim':1024,'act':'relu'},\n",
    "    img_model = {'input_dim':1024,'lyrs':1,'fin_dim':1024,'act':'relu'},\n",
    "    shared_emb_dim = 1024,\n",
    "    txt_emb_type = 'instructions',\n",
    "    lr = 1e-4,\n",
    "    eps = 0.5,\n",
    "    loggers = [\"csv\"],\n",
    "    cbs = [\"checkpoint\",\"early_stop\"],\n",
    "    trainer = {'log_every_n_steps': 50,\n",
    "    'max_epochs': 10},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"val_recall_1\"},\n",
    "    early_stop = {\"monitor\":\"val_recall_1\",\"patience\":2,\"mode\":'max'},\n",
    "    batch_size=512,\n",
    "    val_batch_size = 2000\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name    | Type     | Params\n",
      "-------------------------------------\n",
      "0 | txt_emb | EmbModel | 1.0 M \n",
      "1 | img_emb | EmbModel | 1.0 M \n",
      "2 | shared  | Linear   | 1.0 M \n",
      "-------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.595    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 581/581 [02:44<00:00,  3.54it/s, loss=0.0299, v_num=yr_1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer at 0x7fc668b03ac0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with txt_emb_type = 'ingredients'\n",
    "cfg = Namespace(\n",
    "    seed = 0,\n",
    "    version = 'retrievalINGlyr_1',\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/Recipe/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/Recipe/\",\n",
    "    mode = 'train',\n",
    "    txt_model = {'input_dim':1024,'lyrs':1,'fin_dim':1024,'act':'relu'},\n",
    "    img_model = {'input_dim':1024,'lyrs':1,'fin_dim':1024,'act':'relu'},\n",
    "    shared_emb_dim = 1024,\n",
    "    txt_emb_type = 'ingredients',\n",
    "    lr = 1e-4,\n",
    "    eps = 0.5,\n",
    "    loggers = [\"csv\"],\n",
    "    cbs = [\"checkpoint\",\"early_stop\"],\n",
    "    trainer = {'log_every_n_steps': 50,\n",
    "    'max_epochs': 10},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"val_recall_1\"},\n",
    "    early_stop = {\"monitor\":\"val_recall_1\",\"patience\":2,\"mode\":'max'},\n",
    "    batch_size=512,\n",
    "    val_batch_size = 2000\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name    | Type     | Params\n",
      "-------------------------------------\n",
      "0 | txt_emb | EmbModel | 1.0 M \n",
      "1 | img_emb | EmbModel | 1.0 M \n",
      "2 | shared  | Linear   | 1.0 M \n",
      "-------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.595    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 581/581 [02:47<00:00,  3.47it/s, loss=0.0595, v_num=yr_1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer at 0x7fc6726e3400>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with txt_emb_type = 'ingredients'\n",
    "cfg = Namespace(\n",
    "    seed = 0,\n",
    "    version = 'retrievalTITlyr_1',\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/Recipe/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/Recipe/\",\n",
    "    mode = 'train',\n",
    "    txt_model = {'input_dim':1024,'lyrs':1,'fin_dim':1024,'act':'relu'},\n",
    "    img_model = {'input_dim':1024,'lyrs':1,'fin_dim':1024,'act':'relu'},\n",
    "    shared_emb_dim = 1024,\n",
    "    txt_emb_type = 'title',\n",
    "    lr = 1e-4,\n",
    "    eps = 0.5,\n",
    "    loggers = [\"csv\"],\n",
    "    cbs = [\"checkpoint\",\"early_stop\"],\n",
    "    trainer = {'log_every_n_steps': 50,\n",
    "    'max_epochs': 10},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"val_recall_1\"},\n",
    "    early_stop = {\"monitor\":\"val_recall_1\",\"patience\":2,\"mode\":'max'},\n",
    "    batch_size=512,\n",
    "    val_batch_size = 2000\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
