{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import shutil\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from argparse import Namespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import linalg as LA\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "wandb_logger = lambda dir, version: WandbLogger(\n",
    "    name=\"wandb\", save_dir=dir, version=version\n",
    ")\n",
    "csvlogger = lambda dir, version: CSVLogger(dir, name=\"csvlogs\", version=version)\n",
    "tblogger = lambda dir, version: TensorBoardLogger(dir, name=\"tblogs\", version=version)\n",
    "\n",
    "def get_loggers(dir,version,lis=[\"csv\"]):\n",
    "    lgrs = []\n",
    "    if \"wandb\" in lis:\n",
    "        lgrs.append(wandb_logger(dir, version))\n",
    "    if \"csv\" in lis:\n",
    "        lgrs.append(csvlogger(dir, version))\n",
    "    if \"tb\" in lis:\n",
    "        lgrs.append(tblogger(dir, version))\n",
    "    return lgrs\n",
    "\n",
    "#global vars\n",
    "DATA_DIR = '/common/home/vk405/Projects/Crossmdl/Data/Recipe/'\n",
    "EMB_TRN = DATA_DIR+'embeddings_train1.pkl'\n",
    "EMB_VAL = DATA_DIR+'embeddings_val1.pkl'\n",
    "ING_TRN = DATA_DIR+'ingredients_embeddings_train.pkl'\n",
    "ING_VAL = DATA_DIR+'ingredients_embeddings_val.pkl'\n",
    "#os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(EMB_TRN, 'rb') as files:\n",
    "#     emb_trn = pickle.load(files)\n",
    "# with open(EMB_VAL, 'rb') as files:\n",
    "#     emb_val = pickle.load(files)\n",
    "\n",
    "# with open(ING_TRN, 'rb') as files:\n",
    "#     ing_trn = pickle.load(files)\n",
    "# with open(ING_VAL, 'rb') as files:\n",
    "#     ing_val = pickle.load(files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDset(Dataset):\n",
    "    def __init__(self,data_dir= '/common/home/vk405/Projects/Crossmdl/Data/Recipe/'\\\n",
    "        ,split='train',txt_emb_type='total'):\n",
    "        self.DATA_DIR = data_dir\n",
    "        self.init_data_locs()\n",
    "        self.txt_emb_type = txt_emb_type\n",
    "        self.split = split\n",
    "        if self.txt_emb_type == 'total':\n",
    "            if self.split == 'train':\n",
    "                with open(self.EMB_TRN, 'rb') as files:\n",
    "                    self.emb_vid,self.emb_txt,self.ids = pickle.load(files)\n",
    "            elif self.split == 'valid':\n",
    "                with open(self.EMB_VAL, 'rb') as files:\n",
    "                    self.emb_vid,self.emb_txt,self.ids = pickle.load(files)\n",
    "            elif self.split == 'test':\n",
    "                with open(self.EMB_TST, 'rb') as files:\n",
    "                    self.emb_vid,self.emb_txt,self.ids = pickle.load(files)\n",
    "\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        rind = idx+1\n",
    "        if rind == len(self.emb_txt):\n",
    "            rind = idx-1\n",
    "        return self.emb_vid[idx],self.emb_txt[idx],self.emb_vid[rind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emb_txt)\n",
    "\n",
    "    def init_data_locs(self):\n",
    "        #TOTAL EMBEDDINGS\n",
    "        self.EMB_TRN = self.DATA_DIR+'embeddings_train1.pkl'\n",
    "        self.EMB_VAL = self.DATA_DIR+'embeddings_val1.pkl'\n",
    "        self.EMB_TST = self.DATA_DIR+'embeddings_test1.pkl'\n",
    "        #INGRIDIENTS Embeddings\n",
    "        self.ING_TRN = self.DATA_DIR+'ingredients_embeddings_train.pkl'\n",
    "        self.ING_VAL = self.DATA_DIR+'ingredients_embeddings_val.pkl'\n",
    "        self.ING_TST = self.DATA_DIR + 'ingredients_embeddings_test.pkl'\n",
    "\n",
    "        #TITLE EMBEDDINGS\n",
    "        self.TIT_TRN = self.DATA_DIR+'title_embeddings_train.pkl'\n",
    "        self.TIT_VAL = self.DATA_DIR+'title_embeddings_val.pkl'\n",
    "        self.TIT_TST = self.DATA_DIR + 'title_embeddings_test.pkl'\n",
    "\n",
    "        #Instructions\n",
    "        self.INS_TRN = self.DATA_DIR+'instructions_embeddings_train.pkl'\n",
    "        self.INS_VAL = self.DATA_DIR+'instructions_embeddings_val.pkl'\n",
    "        self.INS_TST = self.DATA_DIR+'instructions_embeddings_test.pkl'\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trn_data = RecipeDset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbModel(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.lyrs = []\n",
    "        dim = self.params['input_dim']\n",
    "        for i in range(self.params['lyrs']):\n",
    "            lyr = nn.Linear(dim,dim)\n",
    "            if self.params['act'] == 'relu':\n",
    "                non_lin = nn.ReLU()\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            self.lyrs.append(lyr)\n",
    "            self.lyrs.append(non_lin)\n",
    "        self.feedforward = nn.Sequential(*self.lyrs)\n",
    "    def forward(self,x):\n",
    "        return self.feedforward(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RecipeModel(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.txt_emb = EmbModel(hparams.txt_model)\n",
    "        self.img_emb = EmbModel(hparams.img_model)\n",
    "        self.shared = nn.Linear(hparams.txt_model['fin_dim'],\\\n",
    "            hparams.shared_emb_dim)\n",
    "    def forward(self,x):\n",
    "        # Ignores anchor embedding\n",
    "        img,txt = x\n",
    "        img_emb = self.img_emb(img)\n",
    "        txt_emb = self.txt_emb(txt)\n",
    "        #anch_img_emb = self.img_emb(anch_img)\n",
    "\n",
    "        img_fin_emb = self.shared(img_emb)\n",
    "        txt_fin_emb = self.shared(txt_emb)\n",
    "        #anch_img_fin_emb = self.shared(anch_img_emb)\n",
    "        return img_fin_emb,txt_fin_emb\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        img,txt,anch_img = batch\n",
    "        anch_img_emb = self.img_emb(anch_img)\n",
    "        anch_img_fin_emb = self.shared(anch_img_emb)\n",
    "        img_fin_emb,txt_fin_emb = self((img,txt))\n",
    "        loss,log_losses = self.get_loss(img_fin_emb,txt_fin_emb,anch_img_fin_emb)\n",
    "        self.log(\"train_loss\",loss,on_step=True)\n",
    "        self.log(\"cos_sim_n\",log_losses[0],on_step=True)\n",
    "        self.log(\"cos_sim_p\",log_losses[-1],on_step=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        img,txt,anch_img = batch\n",
    "        anch_img_emb = self.img_emb(anch_img)\n",
    "        anch_img_fin_emb = self.shared(anch_img_emb)\n",
    "        img_fin_emb,txt_fin_emb = self((img,txt))\n",
    "        loss,log_losses = self.get_loss(img_fin_emb,txt_fin_emb,anch_img_fin_emb)\n",
    "        self.log(\"val_loss\",loss,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_cos_sim_n\",log_losses[0],on_step=False, on_epoch=True)\n",
    "        self.log(\"val_cos_sim_p\",log_losses[-1],on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def get_loss(self,img,txt,anch,reduce='mean'):\n",
    "        #ùêøùëêùëúùë†(ùíÇ,ùíë,ùíè)=max[ùëë(ùíÇ,ùíè)‚àíùëë(ùíÇ,ùíë)+ùúñ,0]\n",
    "        eps = self.hparams.eps if 'eps' in self.hparams else 1e-8\n",
    "        \n",
    "        im_norm,txt_norm,anch_norm = LA.norm(img,dim=-1).reshape(img.shape[0],1),\\\n",
    "        LA.norm(txt,dim=-1).reshape(txt.shape[0],1),LA.norm(anch,dim=-1).reshape(anch.shape[0],1)\n",
    "        normd_img = img/im_norm\n",
    "        normd_txt = txt/txt_norm\n",
    "        normd_anch = anch/anch_norm\n",
    "\n",
    "        cos_sim_p = torch.sum(normd_img*normd_txt,dim=-1)\n",
    "        cos_sim_n = torch.sum(normd_anch*normd_txt,dim=-1)\n",
    "\n",
    "        unclipped_loss = cos_sim_n-cos_sim_p+eps\n",
    "        clipped_loss = torch.relu(unclipped_loss)\n",
    "        if reduce == 'mean':\n",
    "            return torch.mean(clipped_loss),(torch.mean(cos_sim_n),torch.mean(cos_sim_p))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr if 'lr' in self.hparams else 1e-3\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cfg = Namespace(\n",
    "#     version = 'temp',\n",
    "#     artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/Recipe/\",\n",
    "#     data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/Recipe/\",\n",
    "#     mode = 'train',\n",
    "#     txt_model = {'input_dim':1024,'lyrs':2,'fin_dim':1024,'act':'relu'},\n",
    "#     img_model = {'input_dim':1024,'lyrs':2,'fin_dim':1024,'act':'relu'},\n",
    "#     shared_emb_dim = 1024,\n",
    "#     txt_emb_type = 'total',\n",
    "#     learning_rate = 1e-4,\n",
    "#     loggers = [\"csv\",\"wandb\"],\n",
    "#     seed = 0,\n",
    "#     cbs = [\"checkpoint\"],\n",
    "#     trainer = {'log_every_n_steps': 1,\n",
    "#     'max_epochs': 10},\n",
    "#     checkpoint = {\"every_n_epochs\": 1,\n",
    "#     \"monitor\": \"train_loss\"},\n",
    "\n",
    "\n",
    "#     use_precomp_emb = True,\n",
    "#     edim = 100,\n",
    "#     attnhdim = 50,\n",
    "#     nheads = 10,\n",
    "#     wrdim = 768,\n",
    "#     vidim = 512,\n",
    "#     hdim = 30,\n",
    "#     dropoutp=0.0,\n",
    "#     seqlen=26,\n",
    "#     framecnt=499,\n",
    "#     batch_size=512\n",
    "\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check -> OVERFIT ON SMALL DATA\n",
    "\n",
    "# trainer = pl.Trainer(overfit_batches=10)\n",
    "# trn_loader = DataLoader(trn_data,50,shuffle=True)\n",
    "# net = RecipeModel(cfg)\n",
    "\n",
    "# trainer.fit(net,trn_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(iter(trn_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cfg):\n",
    "    pl.seed_everything(cfg.seed)\n",
    "    dir = cfg.artifacts_loc\n",
    "    version = str(cfg.version)\n",
    "    logger_list = get_loggers(dir, version,cfg.loggers)\n",
    "    cbs = []\n",
    "    if \"early_stop\" in cfg.cbs:\n",
    "        #? does'nt really work atm\n",
    "        params = cfg.early_stop\n",
    "        earlystopcb = EarlyStopping(**params, min_delta=0.00, verbose=False)\n",
    "        cbs.append(earlystopcb)\n",
    "    if \"checkpoint\" in cfg.cbs:\n",
    "        store_path = dir + \"ckpts/\" + str(cfg.version) + \"/\"\n",
    "        isExist = os.path.exists(store_path)\n",
    "        # first remove\n",
    "        if isExist and os.path.isdir(store_path):\n",
    "            shutil.rmtree(store_path)\n",
    "        # then create fresh\n",
    "        if not isExist:\n",
    "            os.makedirs(store_path)\n",
    "        fname = \"{epoch}-{val_loss:.2f}\"\n",
    "        params = cfg.checkpoint\n",
    "        checkptcb = ModelCheckpoint(**params, dirpath=store_path, filename=fname)\n",
    "        cbs.append(checkptcb)\n",
    "\n",
    "    if 'wandb' in cfg.loggers:\n",
    "        wandb.init(project=\"RecipeRetrieval\", config=cfg)\n",
    "        \n",
    "    if cfg.mode == 'train':\n",
    "        recipedata_trn = RecipeDset(data_dir=cfg.data_dir,split='train',\\\n",
    "            txt_emb_type = cfg.txt_emb_type)\n",
    "        recipedata_vld = RecipeDset(data_dir=cfg.data_dir,split='valid',\\\n",
    "            txt_emb_type = cfg.txt_emb_type)\n",
    "\n",
    "        train_loader = DataLoader(recipedata_trn,batch_size=cfg.batch_size,shuffle=True,\\\n",
    "            num_workers=4,pin_memory=True)    \n",
    "\n",
    "        valid_loader = DataLoader(recipedata_vld,batch_size=cfg.batch_size,shuffle=False)\n",
    "        net = RecipeModel(cfg)\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger_list,callbacks=cbs, gpus=1,deterministic=True, **cfg.trainer\n",
    "        )\n",
    "        trainer.fit(net, train_loader,valid_loader)\n",
    "        return trainer\n",
    "        #trainer.tune(net,train_loader)\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eps = eps = 0.1\n",
    "cfg = Namespace(\n",
    "    seed = 0,\n",
    "    version = 'temp',\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/Recipe/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/Recipe/\",\n",
    "    mode = 'train',\n",
    "    txt_model = {'input_dim':1024,'lyrs':2,'fin_dim':1024,'act':'relu'},\n",
    "    img_model = {'input_dim':1024,'lyrs':2,'fin_dim':1024,'act':'relu'},\n",
    "    shared_emb_dim = 1024,\n",
    "    txt_emb_type = 'total',\n",
    "    lr = 1e-4,\n",
    "    eps = 0.1,\n",
    "    loggers = [\"csv\",\"wandb\"],\n",
    "    cbs = [\"checkpoint\",\"early_stop\"],\n",
    "    trainer = {'log_every_n_steps': 50,\n",
    "    'max_epochs': 10},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"val_loss\"},\n",
    "    early_stop = {\"monitor\":\"val_loss\",\"patience\":2,\"mode\":'min'},\n",
    "    batch_size=512\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvin136\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/vin136/RecipeRetrieval/runs/1v7130ui\" target=\"_blank\">dry-silence-10</a></strong> to <a href=\"https://wandb.ai/vin136/RecipeRetrieval\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loggers/csv_logs.py:57: UserWarning: Experiment logs directory /common/home/vk405/Projects/Crossmdl/nbs/Recipe/csvlogs/temp exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name    | Type     | Params\n",
      "-------------------------------------\n",
      "0 | txt_emb | EmbModel | 2.1 M \n",
      "1 | img_emb | EmbModel | 2.1 M \n",
      "2 | shared  | Linear   | 1.0 M \n",
      "-------------------------------------\n",
      "5.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 M     Total params\n",
      "20.992    Total estimated model params size (MB)\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /common/home/vk405/Projects/Crossmdl/nbs/Recipe/ckpts/temp exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 669/669 [00:09<00:00, 70.53it/s, loss=0.000164, v_num=temp]\n"
     ]
    }
   ],
   "source": [
    "trained_model = run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "met = '/common/home/vk405/Projects/Crossmdl/nbs/Recipe/csvlogs/temp/metrics.csv'\n",
    "met = pd.read_csv(met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.001964\n",
       "1     0.001240\n",
       "2     0.001516\n",
       "3     0.002234\n",
       "4     0.001287\n",
       "        ...   \n",
       "78    0.000196\n",
       "79    0.000360\n",
       "80    0.000340\n",
       "81    0.000098\n",
       "82    0.000325\n",
       "Name: train_loss, Length: 77, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met['train_loss'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = RecipeModel(cfg)\n",
    "# dl = DataLoader(trn_data,2,shuffle=False)\n",
    "# batch = next(iter(dl))\n",
    "# out = m((batch[0],batch[1]))\n",
    "# t_img,t_txt = out[0].detach(),out[1].detach()\n",
    "# img,txt,anch = t_img,t_txt,batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Crossmdl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
