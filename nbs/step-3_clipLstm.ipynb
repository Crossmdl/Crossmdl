{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pad your sequences\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import joblib\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numpy import linalg as LA\n",
    "from argparse import Namespace\n",
    "from numpy import genfromtxt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import clip\n",
    "\n",
    "\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "wandb_logger = lambda dir, version: WandbLogger(\n",
    "    name=\"wandb\", save_dir=dir, version=version\n",
    ")\n",
    "csvlogger = lambda dir, version: CSVLogger(dir, name=\"csvlogs\", version=version)\n",
    "tblogger = lambda dir, version: TensorBoardLogger(dir, name=\"tblogs\", version=version)\n",
    "\n",
    "def get_loggers(dir,version,lis=[\"csv\"]):\n",
    "    lgrs = []\n",
    "    if \"wandb\" in lis:\n",
    "        lgrs.append(wandb_logger(dir, version))\n",
    "    if \"csv\" in lis:\n",
    "        lgrs.append(csvlogger(dir, version))\n",
    "    if \"tb\" in lis:\n",
    "        lgrs.append(tblogger(dir, version))\n",
    "    return lgrs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vid_ids(split='training',\\\n",
    "    annotns_file='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/annotations/youcookii_annotations_trainval.json'):\n",
    "    # Returns vid_ids corresponding to the split: 'training'/'validation'\n",
    "    \n",
    "    vid_lis = []\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)['database']\n",
    "        for key in annotns:\n",
    "            if annotns[key]['subset'] == split:\n",
    "                vid_lis.append(key)\n",
    "    return vid_lis\n",
    "\n",
    "\n",
    "def get_split_files(split='training',\\\n",
    "    annotns_file='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/annotations/youcookii_annotations_trainval.json',\\\n",
    "        data_dir = '/common/users/vk405/Youcook/'):\n",
    "    total_ids = get_vid_ids(split,annotns_file)\n",
    "    downloaded_ids = set([dir for dir in os.listdir(data_dir) if 'joblib' not in dir])\n",
    "    vid_locs = []\n",
    "    sents = {}\n",
    "    segs = {}\n",
    "    incomplete = []\n",
    "    for id in total_ids:\n",
    "        if id in downloaded_ids:\n",
    "            vid_loc = data_dir+id + '/'\n",
    "            if len(os.listdir(vid_loc))>=495:\n",
    "                vid_locs.append(vid_loc)\n",
    "                seg = joblib.load(data_dir+f'{id}global_segs.joblib')\n",
    "                sent = joblib.load(data_dir+f'{id}global_sents.joblib')\n",
    "                try:\n",
    "                    sents[id] = sent[id]\n",
    "                    segs[id] = seg[id]\n",
    "                except:\n",
    "                    print(f\"{id} is no corresponding global sent/seg\")\n",
    "            else:\n",
    "                #print(f\"{id} has only imgs {len(os.listdir(vid_loc))}\")\n",
    "                incomplete.append(id)\n",
    "    return vid_locs,segs,sents,incomplete \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "FEAT_DIR = pathlib.Path('/common/users/vk405/CLIP_FEAT')\n",
    "RAWFRAME_DIR = pathlib.Path('/common/users/vk405/Youcook/')\n",
    "\n",
    "class Dset(Dataset):\n",
    "    def __init__(self,data_dir,feat_dir,split):\n",
    "        self.data_dir = data_dir\n",
    "        self.feat_dir = feat_dir\n",
    "        self.split = split\n",
    "        self.vid_ids,self.sents = self.get_ids()\n",
    "        self.labels = self.getlabels()\n",
    "        self.sanitycheck()\n",
    "        self.data = self.getdata()\n",
    "        \n",
    "\n",
    "\n",
    "    def sanitycheck(self):\n",
    "        mis = []\n",
    "        #import pdb;pdb.set_trace()\n",
    "        for key in self.labels.keys():\n",
    "            txt_loc = self.feat_dir/self.split/f'txt_{key}.joblib'\n",
    "            txt = joblib.load(txt_loc)\n",
    "            if len(self.labels[key]) == len(self.sents[key]) == len(txt):\n",
    "                pass\n",
    "            else:\n",
    "                print(key)\n",
    "                mis.append(key)\n",
    "        print(f\"segs are not matching:{mis}\")\n",
    "        for key in mis:\n",
    "            self.vid_ids.remove(key)\n",
    "        self.sents = None\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.load(self.data[idx])\n",
    "\n",
    "    def getdata(self):\n",
    "        data = []\n",
    "        for id in self.vid_ids:\n",
    "            segs = self.labels[id]\n",
    "            #import pdb;pdb.set_trace()\n",
    "            for i in range(len(segs)):\n",
    "                data.append((id,i))\n",
    "        return data\n",
    "\n",
    "    def load(self,data):\n",
    "        vid_id,ind = data\n",
    "        vid_frames_loc = self.feat_dir/self.split/f'vid_{vid_id}.joblib'\n",
    "        txt_loc = self.feat_dir/self.split/f'txt_{vid_id}.joblib'\n",
    "        st,end = self.labels[vid_id][ind]\n",
    "        vid = joblib.load(vid_frames_loc)\n",
    "        try:\n",
    "            txt = joblib.load(txt_loc)[ind]\n",
    "        except:\n",
    "            import pdb;pdb.set_trace()\n",
    "        #normalize data\n",
    "        #import pdb;pdb.set_trace()\n",
    "        vid = vid/(LA.norm(vid,axis=-1)).reshape(500,1)\n",
    "        txt = (txt/LA.norm(txt))\n",
    "        #out = np.squeeze(vid@txt.reshape(512,1))\n",
    "        #regression outputs\n",
    "        return vid,txt,st/499,end/499\n",
    "         \n",
    "\n",
    "    def getlabels(self):\n",
    "        label_dict = {}\n",
    "        for vidid in self.vid_ids:\n",
    "            vidloc = self.data_dir/vidid\n",
    "            segs = self.extract_seg(vidloc)\n",
    "            label_dict[vidid] = segs\n",
    "        return label_dict\n",
    "    \n",
    "    def extract_seg(self,vid_loc):\n",
    "        imgs = sorted(os.listdir(vid_loc),key=lambda x: int(x.split('_')[0]))\n",
    "        segs = defaultdict(list)\n",
    "        for img in imgs:\n",
    "            ind,rem = int(img.split('_')[0]),img.split('_')[-1]\n",
    "            \n",
    "            if 'n.' not in rem:\n",
    "                #print(ind,rem)\n",
    "                seg_id = int(rem.split('.')[0])\n",
    "                segs[seg_id].append(ind)\n",
    "                #print(seg_id,ind)\n",
    "        final_segs = []\n",
    "        #import pdb;pdb.set_trace()\n",
    "        segids = sorted(segs.keys())\n",
    "        for segid in segids:\n",
    "            final_segs.append((min(segs[segid]),max(segs[segid])))\n",
    "        return final_segs\n",
    "        \n",
    "    def get_ids(self):\n",
    "        annotns_file='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/annotations/youcookii_annotations_trainval.json'\n",
    "        data_dir = '/common/users/vk405/Youcook/'\n",
    "        vid_locs,_,sents,_ = get_split_files(self.split,annotns_file,data_dir)\n",
    "        ids = [ele.split('/')[-2] for ele in vid_locs]\n",
    "        files = set(os.listdir(self.feat_dir/self.split))\n",
    "        finids = []\n",
    "        missing = []\n",
    "        for id in ids:\n",
    "            if f'vid_{id}.joblib' in files:\n",
    "                finids.append(id)\n",
    "            else:missing.append(id)\n",
    "        print(f\"missing:{missing}\")\n",
    "        return finids,sents\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /common/users/vk405/CLIP_FEAT/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing:[]\n",
      "95WMX64RIBc\n",
      "segs are not matching:['95WMX64RIBc']\n"
     ]
    }
   ],
   "source": [
    "d  = Dset(RAWFRAME_DIR,FEAT_DIR,'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xHr8X2Wpmno', 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipLstm(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.vid_encoder,self.text_encoder,self.finlin = self.get_nets(**self.hparams.network_params)\n",
    "        \n",
    "    def get_nets(self,bidirectional = True,vid_lyrs = 2,vid_hidim=64,vid_fsz=64\\\n",
    "        ,txt_lyrs=2,txt_fsz=64,act='Relu'):\n",
    "        vid_enc = []\n",
    "        last_dim = 512\n",
    "        \n",
    "        self.lstm = nn.LSTM(last_dim,vid_hidim,\\\n",
    "        vid_lyrs,bidirectional=bidirectional,batch_first=True)\n",
    "        vid_enc.append(self.lstm)\n",
    "        if act == 'Relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        vid_enc.append(self.activation)\n",
    "        self.vidfinlyr = nn.Linear(vid_hidim,vid_fsz)\n",
    "        vid_enc.append(self.vidfinlyr)\n",
    "        self.txtlyr = nn.Linear(512,txt_fsz)\n",
    "        txt_enc = nn.Sequential(self.txtlyr,self.activation)\n",
    "        \n",
    "        return vid_enc,txt_enc,nn.Linear(txt_fsz,2)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,vid,txt):\n",
    "        # modeling end = st+diff(positive/sigmoid)\n",
    "        #fixing for now\n",
    "        #torch.squeeze(self.start(self.shared(input)))\n",
    "        lstm,act,lin = self.vid_encoder\n",
    "        hiddens, (final_h, final_c) = lstm(vid.float())\n",
    "        vid_out = lin(act(torch.mean(final_h,dim=0)))\n",
    "        txt_out = self.text_encoder(txt.float())\n",
    "        preds = self.finlin(vid_out+txt_out)\n",
    "        st_p = preds[:,0]\n",
    "        diff = torch.sigmoid(preds[:,-1])\n",
    "        fin_pred = torch.stack([st_p,st_p+diff],-1)\n",
    "        return fin_pred\n",
    "\n",
    "    def giou(self,p,g):\n",
    "        x1_p,_ = torch.min(p,1)\n",
    "        x2_p,_ = torch.max(p,1)\n",
    "\n",
    "        x1_g,_ = torch.min(g,1)\n",
    "        x2_g,_ = torch.max(g,1)\n",
    "\n",
    "        x_1_i,_ =  torch.max(torch.stack([x1_g,x1_p],1),1)\n",
    "        x_2_i,_ = torch.min(torch.stack([x2_g,x2_p],1),1)\n",
    "\n",
    "        x_1_c,_ = torch.min(torch.stack([x1_p,x1_g],1),1)\n",
    "        x_2_c,_ = torch.max(torch.stack([x2_p,x2_g],1),1)\n",
    "\n",
    "        I = x_2_i - x_1_i\n",
    "        U = (x2_p-x1_p) + (x2_g-x1_g) - I\n",
    "        AC = x_2_c-x_1_c\n",
    "\n",
    "        return (I/U) - ((AC-U)/AC),(I/U)\n",
    "\n",
    "\n",
    "        \n",
    "    def training_step(self,batch,batch_idx):\n",
    "\n",
    "        vid,txt,st,end = batch\n",
    "        loss_fn = nn.MSELoss()\n",
    "        # forward\n",
    "        lstm,act,lin = self.vid_encoder\n",
    "        hiddens, (final_h, final_c) = lstm(vid.float())\n",
    "        vid_out = lin(act(torch.mean(final_h,dim=0)))\n",
    "        txt_out = self.text_encoder(txt.float())\n",
    "        preds = self.finlin(vid_out+txt_out)\n",
    "        st_p = preds[:,0]\n",
    "        diff = torch.sigmoid(preds[:,-1])\n",
    "        fin_preds = torch.stack([st_p,st_p+diff],-1)\n",
    "\n",
    "\n",
    "        grounds = torch.stack([torch.squeeze(st),torch.squeeze(end)],1)\n",
    "        giou,iou = self.giou(fin_preds,grounds)\n",
    "        loss = torch.mean(-1*giou)\n",
    "        #preds = self(vid,txt)\n",
    "        #st_loss = loss_fn(preds[:,0].float(),torch.squeeze(st).float())\n",
    "        #end_loss = loss_fn(preds[:,1].float(),torch.squeeze(end).float())\n",
    "        #loss_end = nn.CrossEntropyLoss()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #st_l = loss_st(torch.squeeze(self.start(self.shared(input))).float(),st.float())\n",
    "        #end_l = loss_st(torch.squeeze(self.end(self.shared(input))).float(),end.float())\n",
    "        #loss = st_loss + end_loss\n",
    "        self.log(\"train_loss\",loss,on_step=True)\n",
    "        self.log(\"train_iou\",torch.mean(iou),on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "\n",
    "\n",
    "        vid,txt,st,end = batch\n",
    "        loss_fn = nn.MSELoss()\n",
    "        # forward\n",
    "        lstm,act,lin = self.vid_encoder\n",
    "        hiddens, (final_h, final_c) = lstm(vid.float())\n",
    "        vid_out = lin(act(torch.mean(final_h,dim=0)))\n",
    "        txt_out = self.text_encoder(txt.float())\n",
    "        preds = self.finlin(vid_out+txt_out)\n",
    "        st_p = preds[:,0]\n",
    "        diff = torch.sigmoid(preds[:,-1])\n",
    "        fin_preds = torch.stack([st_p,st_p+diff],-1)\n",
    "\n",
    "        grounds = torch.stack([torch.squeeze(st),torch.squeeze(end)],1)\n",
    "        giou,iou = self.giou(fin_preds,grounds)\n",
    "        loss = torch.mean(-1*giou)\n",
    "        #loss = torch.mean(-1*self.giou(preds,grounds))\n",
    "        #preds = self(vid,txt)\n",
    "        #st_loss = loss_fn(preds[:,0].float(),torch.squeeze(st).float())\n",
    "        #end_loss = loss_fn(preds[:,1].float(),torch.squeeze(end).float())\n",
    "        #loss_end = nn.CrossEntropyLoss()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #st_l = loss_st(torch.squeeze(self.start(self.shared(input))).float(),st.float())\n",
    "        #end_l = loss_st(torch.squeeze(self.end(self.shared(input))).float(),end.float())\n",
    "        #loss = st_loss + end_loss\n",
    "        self.log(\"val_loss\",loss,on_step=False,on_epoch=True)\n",
    "        self.log(\"val_iou\",torch.mean(iou),on_step=False,on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def noam_decay(step, num_warmup_steps, dim):\n",
    "  # Eq. (3) of the Transfomer paper. \n",
    "  return (dim ** (-0.5) * min(step ** (-0.5), step * num_warmup_steps**(-1.5)))\n",
    "\n",
    "plt.plot([step + 1 for step in range(10000)], [noam_decay(step + 1, 1000, 1000) for step in range(10000)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.Adam(betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "v,t,st,end = d[0]\n",
    "m = ClipLstm(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_t = torch.unsqueeze(torch.tensor(v),0)\n",
    "t_t = torch.unsqueeze(torch.tensor(t),0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m(v_t,t_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = preds.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034938157"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_p = preds[:,0]\n",
    "diff = torch.sigmoid(preds[:,-1])\n",
    "\n",
    "fin_pred = torch.stack([st_p,diff],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0659,  0.4928]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_t = model(v_t,t_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "FEAT_DIR = pathlib.Path('/common/users/vk405/CLIP_FEAT')\n",
    "RAWFRAME_DIR = pathlib.Path('/common/users/vk405/Youcook/')\n",
    "#early_stop\n",
    "cfg = Namespace(\n",
    "    version = 'clip_lstm',\n",
    "    id = 0,\n",
    "    FEAT_DIR = FEAT_DIR,\n",
    "    RAWFRAME_DIR = RAWFRAME_DIR,\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/YouCookII/\",\n",
    "    trn_split = 0.8,\n",
    "    mode = 'train',\n",
    "    split = 'training',\n",
    "    loggers = [\"csv\"],\n",
    "    seed = 0,\n",
    "    network_params = {'bidirectional':True,'vid_lyrs':2,\\\n",
    "        'vid_hidim':64,'vid_fsz':64\\\n",
    "        ,'txt_lyrs':2,'txt_fsz':64,'act':'Relu'},\n",
    "    cbs = [\"checkpoint\"],\n",
    "    trainer = {'log_every_n_steps': 1, \n",
    "    'max_epochs': 150},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"val_loss\"},\n",
    "    early_stop = {\"monitor\":\"val_loss\",\"mode\":\"min\",\"patience\":5},\n",
    "    lr = 1e-4\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_iou</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_iou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.402520</td>\n",
       "      <td>-0.402520</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.417671</td>\n",
       "      <td>-0.417671</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.351238</td>\n",
       "      <td>-0.351238</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.376945</td>\n",
       "      <td>-0.376945</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.385207</td>\n",
       "      <td>-0.385207</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss  train_iou  epoch  step  val_loss  val_iou\n",
       "0    0.402520  -0.402520      0     0       NaN      NaN\n",
       "1    0.417671  -0.417671      0     1       NaN      NaN\n",
       "2    0.351238  -0.351238      0     2       NaN      NaN\n",
       "3    0.376945  -0.376945      0     3       NaN      NaN\n",
       "4    0.385207  -0.385207      0     4       NaN      NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = cfg.artifacts_loc+'csvlogs'+f'/{cfg.version}/metrics.csv'\n",
    "df = pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12210    0.098493\n",
       "12211    0.116805\n",
       "Name: train_iou, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train_iou'].dropna().tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12063    0.075753\n",
       "12179    0.069663\n",
       "Name: val_iou, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['val_iou'].dropna().tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing\n",
    "# m = ClipLstm(cfg)\n",
    "# vid = torch.unsqueeze(torch.tensor(v),dim=0)\n",
    "# txt = torch.unsqueeze(torch.tensor(t),dim=0)\n",
    "# lstm = nn.LSTM(512,64,\\\n",
    "#         2,bidirectional=True,batch_first=True)\n",
    "\n",
    "# hiddens, (final_h, final_c) = lstm(vid)\n",
    "# out = m(vid,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cfg):\n",
    "    #pl.seed_everything(cfg.seed)\n",
    "    dir = cfg.artifacts_loc\n",
    "    version = str(cfg.version)\n",
    "    logger_list = get_loggers(dir, version,cfg.loggers)\n",
    "    cbs = []\n",
    "    if \"early_stop\" in cfg.cbs:\n",
    "        #? does'nt really work atm\n",
    "        params = cfg.early_stop\n",
    "        earlystopcb = EarlyStopping(**params, min_delta=0.00, verbose=False)\n",
    "        cbs.append(earlystopcb)\n",
    "    if \"checkpoint\" in cfg.cbs:\n",
    "        store_path = dir + \"ckpts/\" + str(cfg.version) + \"/\"\n",
    "        isExist = os.path.exists(store_path)\n",
    "        if not isExist:\n",
    "            os.makedirs(store_path)\n",
    "        fname = \"{epoch}-{val_loss:.2f}\"\n",
    "        params = cfg.checkpoint\n",
    "        checkptcb = ModelCheckpoint(**params, dirpath=store_path, filename=fname)\n",
    "        cbs.append(checkptcb)\n",
    "\n",
    "    #wandb.init(project=\"videoretrieval\", config=cfg)\n",
    "    if cfg.mode == 'train':\n",
    "        d = Dset(cfg.RAWFRAME_DIR,cfg.FEAT_DIR,cfg.split)\n",
    "        trn_sz = int(len(d)*cfg.trn_split)\n",
    "        val_sz = len(d)-trn_sz\n",
    "        trndset,valdset = random_split(d,[trn_sz,val_sz])\n",
    "        trnl = DataLoader(trndset,batch_size=64,shuffle=True,num_workers = 5)\n",
    "        vall = DataLoader(valdset,batch_size=64)\n",
    "        hparams = cfg    \n",
    "        net = ClipLstm(hparams)\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger_list,callbacks=cbs,gpus=1,deterministic=True, **cfg.trainer\n",
    "        )\n",
    "        trainer.fit(net, trnl,vall)\n",
    "        return trainer\n",
    "        #trainer.tune(net,train_loader)\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dset(cfg.RAWFRAME_DIR,cfg.FEAT_DIR,cfg.split)\n",
    "trn_sz = int(len(d)*cfg.trn_split)\n",
    "val_sz = len(d)-trn_sz\n",
    "trndset,valdset = random_split(d,[trn_sz,val_sz])\n",
    "trnl = DataLoader(trndset,batch_size=64,shuffle=True,num_workers = 5)\n",
    "vall = DataLoader(valdset,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid,txt,st,end = next(iter(trnl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def get_model(cfg):\n",
    "    hparams = cfg    \n",
    "    PATH = Path(cfg.artifacts_loc)/'ckpts'/cfg.version\n",
    "    ckpt = os.listdir(PATH)[-1]\n",
    "    net = ClipLstm(hparams)\n",
    "    print(f\"loading ckpt:{ckpt}\")\n",
    "    new_model = net.load_from_checkpoint(checkpoint_path=str(PATH/ckpt))\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(cfg,dset):\n",
    "    from collections import defaultdict\n",
    "    storage_p = defaultdict(list)\n",
    "    storage_g = defaultdict(list)\n",
    "    data = dset.data\n",
    "    model = get_model(cfg)\n",
    "    model.eval()\n",
    "    for i,ele in enumerate(data):\n",
    "        key,id = ele\n",
    "        v,t,st,end = dset[i]\n",
    "        storage_g[key].append((st*499,end*499))\n",
    "        v_t = torch.unsqueeze(torch.tensor(v),0)\n",
    "        t_t = torch.unsqueeze(torch.tensor(t),0)\n",
    "        preds = model(v_t,t_t)\n",
    "        p_s,p_en = preds.detach().cpu().numpy()[0]\n",
    "        storage_p[key].append((p_s*499,p_en*499))\n",
    "    return storage_g,storage_p\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ckpt:epoch=26-val_loss=-0.08.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = get_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(vid,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([257.2845, 272.2961], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]*499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(197., dtype=torch.float64), tensor(204., dtype=torch.float64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st[0]*499,end[0]*499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2],[8,18]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min,_ = torch.min(a,1)\n",
    "max,_ = torch.max(a,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,_ = torch.min(torch.stack([min,max],1),1)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "def giou(p,g):\n",
    "    x1_p,_ = torch.min(p,1)\n",
    "    x2_p,_ = torch.max(p,1)\n",
    "\n",
    "    x1_g,_ = torch.min(g,1)\n",
    "    x2_g,_ = torch.max(g,1)\n",
    "\n",
    "    x_1_i,_ =  torch.max(torch.stack([x1_g,x1_p],1),1)\n",
    "    x_2_i,_ = torch.min(torch.stack([x2_g,x2_p],1),1)\n",
    "\n",
    "    x_1_c,_ = torch.min(torch.stack([x1_p,x1_g],1),1)\n",
    "    x_2_c,_ = torch.max(torch.stack([x2_p,x2_g],1),1)\n",
    "\n",
    "    I = x_2_i - x_1_i\n",
    "    U = (x2_p-x1_p) + (x2_g-x1_g) - I\n",
    "    AC = x_2_c-x_1_c\n",
    "\n",
    "    return (I/U) - ((AC-U)/AC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[1,9]])\n",
    "g1 = torch.tensor([[2,5]])\n",
    "g2 = torch.tensor([[20,18]])\n",
    "g3 = torch.tensor([[10,18]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3750]), tensor([-0.4737]), tensor([-0.0588]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giou(p,g1),giou(p,g2),giou(p,g3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Crossmdl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
