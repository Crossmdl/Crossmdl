{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80fd68d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0249d514c7b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# testing gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_big\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx_big_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_big\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# testing gpu\n",
    "x_big = torch.randn(100000)\n",
    "x_big_gpu = x_big.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb6a4a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5254, -0.9146,  0.7285,  ...,  0.8204, -0.1264,  0.9060],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_big_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4977136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def get_vids(base_dir,split):\n",
    "    trn_split = base_dir+split\n",
    "    trn_idlst = []\n",
    "    trn_vidlst = []\n",
    "\n",
    "    f = open(trn_split,'r')\n",
    "    for line in f:\n",
    "        id_,vid = line.split('/')\n",
    "        vid = vid.strip('\\n')\n",
    "        trn_idlst.append(id_)\n",
    "        trn_vidlst.append(vid)\n",
    "        #print(vid)\n",
    "        #break\n",
    "    f.close()\n",
    "    return trn_idlst,trn_vidlst\n",
    "\n",
    "    \n",
    "def get_features(data_dir,split='val',feat_dir='/common/users/vk405/feat_csv/'):\n",
    "    #feat_dir = data_dir\n",
    "    splits_dir = data_dir+'splits/'\n",
    "    if split == 'val':\n",
    "        feat_split_dir = feat_dir+'val_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'val_list.txt')  \n",
    "    elif split == 'train':\n",
    "        feat_split_dir = feat_dir+'train_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'train_list.txt') \n",
    "    elif split == 'test':\n",
    "        feat_split_dir = feat_dir+'test_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'test_list.txt')\n",
    "    else:\n",
    "        raise NotImplementedError(f'unknown split: {split}')     \n",
    "    feat_list = {}\n",
    "    vid_dtls = []\n",
    "    for num,name in zip(vid_num,vid_name):\n",
    "        feat_loc = os.path.join(feat_split_dir, f'{num}/{name}/0001/')\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if os.path.isdir(feat_loc):\n",
    "            feat_files = feat_loc + os.listdir(feat_loc)[0]\n",
    "            feat_list[name] = feat_files\n",
    "            #feat_list.append(feat_files)\n",
    "            vid_dtls.append((num,name))\n",
    "        else:\n",
    "            print(f\"video : {num}/{name} not found\")\n",
    "    assert len(feat_list) == len(vid_dtls),\"get-features is giving incorrect features\"\n",
    "    return feat_list,vid_dtls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_raw_labels(ids,annotns_file):\n",
    "\n",
    "    label_info = {}\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)\n",
    "        print(annotns.keys())\n",
    "        for _,vidname in ids:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if vidname in annotns['database']:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                duration = annotns['database'][vidname]['duration']\n",
    "                annot = annotns['database'][vidname]['annotations']\n",
    "                labels = []\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for segment_info in annot:\n",
    "                    interval = segment_info['segment']\n",
    "                    sent = segment_info['sentence']\n",
    "                    labels.append((interval,sent,duration))\n",
    "\n",
    "                label_info[vidname] = labels\n",
    "            else:\n",
    "                print(f\"label for {vidname} not present\")\n",
    "    return label_info\n",
    "\n",
    "def regress_labels(raw_labels):\n",
    "    regress_labels = {}\n",
    "    for key in raw_labels:\n",
    "        new_labels = []\n",
    "        for item in raw_labels[key]:\n",
    "            rng,sent,vidlen = item\n",
    "            mid = sum(rng)/2\n",
    "            duration = rng[-1]-rng[0]\n",
    "            mid_pred = (1/vidlen)*mid # location of mid-point w.r.t video length\n",
    "            duration_pred = (1/vidlen)*duration\n",
    "            new_labels.append(([mid_pred,duration_pred],sent))\n",
    "        regress_labels[key] = new_labels\n",
    "    return regress_labels\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e6e83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vidlens = []\n",
    "        self.truebounds = []\n",
    "\n",
    "    def fit(self,raw_labels):\n",
    "        l = []\n",
    "        for key in raw_labels:\n",
    "            vid_len = raw_labels[key][0][-1]\n",
    "            sz = len(raw_labels[key])\n",
    "            for i in range(sz):l.append(vid_len)\n",
    "        self.vidlens = np.array(l)\n",
    "        return self\n",
    "        \n",
    "    def transform(self,raw_labels):\n",
    "        regress_labels = self._regress_labels(raw_labels)\n",
    "        return regress_labels\n",
    "\n",
    "    def decode(self,outputs):\n",
    "        cent_wid = np.expand_dims(self.vidlens,1)*outputs\n",
    "        width = cent_wid[:,-1]\n",
    "        center = cent_wid[:,0]\n",
    "        left = center - width//2\n",
    "        right = center + (width-(width//2))\n",
    "        return np.concatenate([np.expand_dims(left,1),np.expand_dims(right,1)],1)\n",
    "\n",
    "\n",
    "    def _regress_labels(self,raw_labels):\n",
    "        regress_labels = {}\n",
    "        bounds = []\n",
    "        for key in raw_labels:\n",
    "            new_labels = []\n",
    "            for item in raw_labels[key]:\n",
    "                rng,sent,vidlen = item\n",
    "                bounds.append(rng)\n",
    "                mid = sum(rng)/2\n",
    "                duration = rng[-1]-rng[0]\n",
    "                mid_pred = (1/vidlen)*mid # location of mid-point w.r.t video length\n",
    "                duration_pred = (1/vidlen)*duration\n",
    "                new_labels.append(([mid_pred,duration_pred],sent))\n",
    "            regress_labels[key] = new_labels\n",
    "        self.truebounds = np.array(bounds)\n",
    "        return regress_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f5f1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "527bc2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_labels = lenc.fit_transform(label_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0e08217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-62-b6331d5c019f>\u001b[0m(27)\u001b[0;36mdecode\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     25 \u001b[0;31m        \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sam = np.random.randn(10337,2)\n",
    "dec = lenc.decode(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03d2d71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10337, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca31fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d40ed949",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28470e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "\n",
    "def init_parameters_xavier_uniform(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "def scaled_dot(query, key, mask_key=None):  \n",
    "    score = torch.matmul(query, key.transpose(-2, -1))\n",
    "    score /= math.sqrt(query.size(-1))\n",
    "    if mask_key is not None:\n",
    "        score = score.masked_fill(mask_key, -1e18)  # Represents negative infinity\n",
    "    return score      \n",
    "            \n",
    "def attend(query, key, value, mask_key=None, dropout=None):\n",
    "    # TODO: Implement\n",
    "    # Use scaled_dot, be sure to mask key\n",
    "    #smax = nn.Softmax(-1)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    score = scaled_dot(query,key,mask_key)  \n",
    "    attention = F.softmax(score,dim=-1)\n",
    "    if dropout is not None:#do = nn.Dropout(dropout)\n",
    "        attention = dropout(attention)\n",
    "    answer = torch.matmul(attention,value) \n",
    "    # Convexly combine value embeddings using attention, this should be just a matrix-matrix multiplication.\n",
    "    return answer, attention\n",
    "\n",
    "\n",
    "\n",
    "def split_heads(batch, num_heads):  \n",
    "    (batch_size, length, dim) = batch.size()  # These are the expected batch dimensions.\n",
    "    assert dim % num_heads == 0  # Assert that dimension is divisible by the number of heads.\n",
    "    dim_head = dim // num_heads\n",
    "\n",
    "    # No new memory allocation\n",
    "    splitted = batch.view(batch_size, -1, num_heads, dim_head).transpose(1, 2)  \n",
    "    return splitted  # (batch_size, num_heads, length, dim_head), note that now the last two dimensions are compatible with our attention functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_heads(batch):  \n",
    "    (batch_size, num_heads, length, dim_head) = batch.size()  # These are the expected batch dimensions.\n",
    "\n",
    "    # New memory allocation (reshape), can't avoid.\n",
    "    merged = batch.transpose(1, 2).reshape(batch_size, -1, num_heads * dim_head)\n",
    "    return merged  # (batch_size, length, dim)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "\n",
    "        self.linear_query = nn.Linear(dim, dim)\n",
    "        self.linear_key = nn.Linear(dim, dim)\n",
    "        self.linear_value = nn.Linear(dim, dim)\n",
    "        self.linear_final = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, query, key, value, mask_key=None, layer_cache=None,\n",
    "              memory_attention=False):\n",
    "        \"\"\"\n",
    "        INPUT\n",
    "          query: (batch_size, length_query, dim)\n",
    "          key: (batch_size, length_key, dim)\n",
    "          value: (batch_size, length_key, dim_value)\n",
    "          mask_key: (*, 1, length_key) if queries share the same mask, else\n",
    "                    (*, length_query, length_key)\n",
    "          layer_cache: if not None, stepwise decoding (cache of key/value)\n",
    "          memory_attention: doing memory attention in stepwise decoding?\n",
    "        OUTPUT\n",
    "          answer: (batch_size, length_query, dim_value)\n",
    "          attention: (batch_size, num_heads, length_query, length_key) else\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.linear_query(query)\n",
    "        query = split_heads(query, self.num_heads)  # (batch_size, num_heads, -1, dim_head)\n",
    "\n",
    "        def process_key_value(key, value):  # Only called when necessary.\n",
    "            key = self.linear_key(key)\n",
    "            key = split_heads(key, self.num_heads)\n",
    "            value = self.linear_value(value)\n",
    "            value = split_heads(value, self.num_heads)\n",
    "            return key, value\n",
    "\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if layer_cache is None:\n",
    "            key, value = process_key_value(key, value)\n",
    "        else:\n",
    "            assert query.size(2) == 1  # Stepwise decoding\n",
    "            \n",
    "            if memory_attention:\n",
    "                if layer_cache['memory_key'] is None:  # One-time calculation\n",
    "                    key, value = process_key_value(key, value)\n",
    "                    # (batch_size, num_heads, length_memory, dim)\n",
    "                    layer_cache['memory_key'] = key\n",
    "                    layer_cache['memory_value'] = value\n",
    "\n",
    "                key = layer_cache['memory_key']\n",
    "                value = layer_cache['memory_value']\n",
    "\n",
    "            else:  # Self-attention during decoding\n",
    "                key, value = process_key_value(key, value)\n",
    "                assert key.size(2) == 1 and value.size(2) == 1\n",
    "                \n",
    "                # Append to previous.\n",
    "                if layer_cache['self_key'] is not None:\n",
    "                    key = torch.cat((layer_cache['self_key'], key), dim=2)\n",
    "                    value = torch.cat((layer_cache['self_value'], value), dim=2)\n",
    "                    \n",
    "                 # (batch_size, num_heads, length_decoded, dim)\n",
    "                layer_cache['self_key'] = key  # Recache.\n",
    "                layer_cache['self_value'] = value\n",
    "        # Because we've splitted embeddings into heads, we must also split the mask. \n",
    "        # And because each query uses the same mask for all heads (we don't use different masking for different heads), \n",
    "        # we can specify length 1 for the head dimension.\n",
    "        if mask_key is not None:  \n",
    "            mask_key = mask_key.unsqueeze(1)  # (batch_size, 1, -1, length_key)\n",
    "\n",
    "        answer, attention = attend(query, key, value, mask_key, self.dropout)\n",
    "\n",
    "        answer = merge_heads(answer)  # (batch_size, length_key, dim)\n",
    "        answer = self.linear_final(answer)\n",
    "\n",
    "        return answer, attention\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_hidden, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, dim_hidden)\n",
    "        self.w2 = nn.Linear(dim_hidden, dim)\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.drop1 = nn.Dropout(drop_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(drop_rate)\n",
    "    def forward(self, x):\n",
    "        inter = self.drop1(self.relu(self.w1(self.layer_norm(x))))\n",
    "        output = self.drop2(self.w2(inter))\n",
    "        return output + x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SinusoidalPositioner(nn.Module):\n",
    "    def __init__(self, dim, drop_rate=0.1, length_max=5000):\n",
    "        super().__init__()\n",
    "        frequency = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.) / dim))  # Using different frequency for each dim\n",
    "        positions = torch.arange(0, length_max).unsqueeze(1)\n",
    "        wave = torch.zeros(length_max, dim)\n",
    "        wave[:, 0::2] = torch.sin(frequency * positions)\n",
    "        wave[:, 1::2] = torch.cos(frequency * positions)\n",
    "        self.register_buffer('wave', wave.unsqueeze(0))  # (1, length_max, dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.dim = dim\n",
    "        self.length_max = length_max\n",
    "    def forward(self, x, step=-1):\n",
    "        assert x.size(-2) <= self.length_max\n",
    "\n",
    "        if step < 0:  # Take the corresponding leftmost embeddings.\n",
    "            position_encoding = self.wave[:, :x.size(-2), :]\n",
    "        else:  # Take the embedding at the step.\n",
    "            position_encoding = self.wave[:, step, :]\n",
    "\n",
    "        x = x * math.sqrt(self.dim)\n",
    "        return self.dropout(x + position_encoding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, dim, num_heads, dim_hidden, drop_rate):\n",
    "    super().__init__()\n",
    "    self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "    self.self_attention = MultiHeadAttention(dim, num_heads, drop_rate)\n",
    "    self.drop = nn.Dropout(drop_rate)\n",
    "    self.feedforward = PositionwiseFeedForward(dim, dim_hidden, drop_rate)\n",
    "\n",
    "  def forward(self, source, mask_source=None):\n",
    "    # TODO: Implement\n",
    "    #print(source.shape)\n",
    "    normed = self.layer_norm(source)  \n",
    "    # Apply layer norm on source\n",
    "\n",
    "    attended, attention = self.self_attention(normed,normed,normed,mask_source)\n",
    "    #None, None  # Apply self-attention on normed (be sure to use mask_source).\n",
    "    attended = self.drop(attended) + source  \n",
    "    # Re-write attended by applying dropout and adding a residual connection to source.\n",
    "    return self.feedforward(attended), attention\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self,dim,num_heads,dim_hidden,drop_rate):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.context_attention = MultiHeadAttention(dim, num_heads, drop_rate)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "        self.feedforward = PositionwiseFeedForward(dim, dim_hidden, drop_rate)\n",
    "        \n",
    "    def forward(self,target,memory,layer_cache=None):\n",
    "        \n",
    "        cross_attn_target = self.layer_norm(target)\n",
    "        attended, attention = self.context_attention(cross_attn_target,memory,memory,layer_cache=layer_cache,memory_attention=True)\n",
    "        \n",
    "        attended = target + self.drop(attended)\n",
    "        \n",
    "        return self.feedforward(attended),attention\n",
    "\n",
    "\n",
    "\n",
    "layer_cache = {'memory_key': None, 'memory_value': None, 'self_key': None, 'self_value': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "750ed488",
   "metadata": {},
   "outputs": [],
   "source": [
    "positioner = SinusoidalPositioner(4, drop_rate=0., length_max=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "860d90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_crossatn = CrossAttentionLayer(500,2,50,0.0)\n",
    "\n",
    "source = torch.randn(1,500,500)\n",
    "tgt = torch.randn(1,1,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "67a5fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output,_ = mh_crossatn(tgt,source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "10e48165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 500])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eee296a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = lambda model : sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#toomany params for a single layer.\n",
    "total_params(mh_crossatn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a0613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "332a5ba5",
   "metadata": {},
   "source": [
    "# Data-loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9051f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fdc70c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['database'])\n"
     ]
    }
   ],
   "source": [
    "#testing functions\n",
    "data_dir = '/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\n",
    "annotns_file = data_dir+'annotations/youcookii_annotations_trainval.json'\n",
    "\n",
    "splits_dir = data_dir+'splits/'\n",
    "splits = ['test_list.txt','train_list.txt','val_list.txt']\n",
    "trn_feats,trn_vids = get_features(data_dir,split='train')\n",
    "label_info = get_raw_labels(trn_vids,annotns_file)\n",
    "final_labels = regress_labels(label_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a57c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ec94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a36d9ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(trn_feats['Ysh60eirChU']).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9eb8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset/loader\n",
    "\n",
    "class YoucookDset(Dataset):\n",
    "    def __init__(self,data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\\\n",
    "        ,split='train',use_precomp_emb=True):\n",
    "        self.feat_locs = {}\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        self.use_precomp_emb = use_precomp_emb\n",
    "        self.text_emb = None\n",
    "        if self.split != 'test':\n",
    "            self.annotns_file = data_dir+'annotations/youcookii_annotations_trainval.json'\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Split:{self.split},not yet correctly implemented\")\n",
    "        if self.use_precomp_emb:\n",
    "            self.txt_emb = joblib.load(os.path.join(self.data_dir,'emb.joblib'))\n",
    "\n",
    "        self.feat_locs,vids = get_features(self.data_dir,split=self.split)\n",
    "        assert len(vids) == len(self.feat_locs),\"features are wrong\"\n",
    "        #import pdb;pdb.set_trace()\n",
    "        label_info = get_raw_labels(vids,self.annotns_file)\n",
    "        self.labelencoder = LabelEncoder()\n",
    "        self.final_labels = self.labelencoder.fit_transform(label_info)\n",
    "        #regress_labels(label_info)\n",
    "        #(vid_id,seg_id)\n",
    "        self.data = []\n",
    "        #self.vid_len = []\n",
    "        for key in self.final_labels:\n",
    "            annot_len = len(self.final_labels[key])\n",
    "            if key in self.feat_locs:\n",
    "                file_loc = self.feat_locs[key]\n",
    "                segments = list(zip(repeat(key,annot_len),repeat(file_loc,annot_len),\\\n",
    "                    range(annot_len)))\n",
    "                self.data.extend(segments)\n",
    "            else:\n",
    "                print(f\"video:{key} not found\")\n",
    "                \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if self.use_precomp_emb:\n",
    "            vidname,file_loc,seg_ind = self.data[idx]\n",
    "            #import pdb;pdb.set_trace()\n",
    "            #self.txt_emb[vidname][seg_ind],\n",
    "            return pd.read_csv(file_loc).values.astype(np.float32),(self.txt_emb[vidname][seg_ind]).astype(np.float32),\\\n",
    "                np.array(self.final_labels[vidname][seg_ind][0],dtype=np.float32)\n",
    "        else:\n",
    "            raise NotImplementedError(\"not yet correctly implemented\")\n",
    "\n",
    "        \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e48e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['database'])\n"
     ]
    }
   ],
   "source": [
    "youcookdata = YoucookDset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dc569667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10337"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(youcookdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a663e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b79cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "80eb16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_emb,txt_emb,label = youcookdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8d2a9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768,), (2,))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_emb.shape,label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86e48684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['database'])\n"
     ]
    }
   ],
   "source": [
    "youcookvld = YoucookDset(split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0265955d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3492"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(youcookvld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ceaf41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "youcookdl = DataLoader(youcookdata,batch_size=32,shuffle=True)\n",
    "\n",
    "youcookvld_dl = DataLoader(youcookvld,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e36afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_feat,trn_wemb,trn_labels = next(iter(youcookdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e91e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_feat.shape,trn_wemb.shape,trn_labels.shape\n",
    "trn_wemb.unsqueeze_(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_feat.transpose_(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_wemb.transpose_(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c1518658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 512, 499]), torch.Size([32, 768, 1]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_feat.shape,trn_wemb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d7d23bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrdcnn.weight.dtype,trn_feat.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "af8c9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrdcnn = nn.Conv1d(768, 100, 1, stride=1)\n",
    "vidcnn = nn.Conv1d(512,100,1,stride=1)\n",
    "\n",
    "trn_feat_red = vidcnn(trn_feat.float())\n",
    "trn_wemb_red = wrdcnn(trn_wemb.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_feat_red.transpose_(1,2),trn_wemb_red.transpose_(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3c37b1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 499, 100])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_feat_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e83694ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 100])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_wemb_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fb09340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlhattn = MultiHeadAttention(100,10,dropout_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "973ef991",
   "metadata": {},
   "outputs": [],
   "source": [
    "out,attn = mlhattn(trn_wemb_red,trn_feat_red,trn_feat_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4fe6e65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 100])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7ba61b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40400"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params(mlhattn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77653897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "hparams = Namespace(\n",
    "    edim = 100,\n",
    "    attnhdim = 50,\n",
    "    nheads = 10,\n",
    "    wrdim = 768,\n",
    "    vidim = 512,\n",
    "    hdim = 30,\n",
    "    dropoutp=0.0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b81786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f2c8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossattnModel(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        #self.hparams = hparams\n",
    "        #import pdb;pdb.set_trace()\n",
    "        self.attn = CrossAttentionLayer(self.hparams.edim,self.hparams.nheads,\\\n",
    "                           self.hparams.attnhdim,self.hparams.dropoutp)\n",
    "        self.wrdcnn =  nn.Conv1d(self.hparams.wrdim, self.hparams.edim, 1, stride=1)\n",
    "        self.vidcnn =  nn.Conv1d(self.hparams.vidim, self.hparams.edim, 1, stride=1)\n",
    "        self.hid_layer = nn.Linear(self.hparams.edim,self.hparams.hdim)\n",
    "        self.out_layer = nn.Linear(self.hparams.hdim,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #keep this for inference\n",
    "        out = self._model(x)\n",
    "        return out\n",
    "        \n",
    "    def _model(self,x):\n",
    "        vid_x,wrd_x = x\n",
    "        #import pdb;pdb.set_trace()\n",
    "        wrd_x = wrd_x.unsqueeze(1).transpose(1,2)\n",
    "        vid_x = vid_x.transpose(1,2)\n",
    "        #print(f\"inside model, wrd_x:{wrd_x.shape},vi\")\n",
    "        tgt = self.wrdcnn(wrd_x.float()).transpose(1,2)\n",
    "        src = self.vidcnn(vid_x.float()).transpose(1,2)\n",
    "        attended,attn_score = self.attn(tgt,src)\n",
    "        out = self.out_layer(F.relu(self.hid_layer(F.relu(attended))))\n",
    "        return out\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        #for tranining\n",
    "        vid_feat,wrd_feat,labels = batch\n",
    "        x_hat = self._model((vid_feat.float(),wrd_feat.float()))\n",
    "        #import pdb;pdb.set_trace()\n",
    "        loss = F.mse_loss(x_hat.squeeze().float(), labels.squeeze().float())\n",
    "        #print(f\"inside train step, loss:{loss}\")\n",
    "        self.log(\"train_loss\",loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        #for validation\n",
    "        vid_feat,wrd_feat,labels = batch\n",
    "        x_hat = self._model((vid_feat.float(),wrd_feat.float()))\n",
    "        #import pdb;pdb.set_trace()\n",
    "        loss = F.mse_loss(x_hat.squeeze().float(), labels.squeeze().float())\n",
    "        #print(f\"inside train step, loss:{loss}\")\n",
    "        self.log(\"val_loss\",loss,on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b242b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossattnModel(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f658edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/common/home/vk405/Projects/Crossmdl/nbs/lightning_logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5df3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7f37f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "version = '1'\n",
    "csvlogger = CSVLogger(log_dir,version)\n",
    "trainer = pl.Trainer(logger= csvlogger,\\\n",
    "    gpus=1,max_epochs=5)\n",
    "youcookdl = DataLoader(youcookdata,batch_size=64,shuffle=True,num_workers=10)\n",
    "youcoodvld_dl = DataLoader(youcookvld,batch_size=64,shuffle=False,num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "06cff007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | attn      | CrossAttentionLayer | 51.0 K\n",
      "1 | wrdcnn    | Conv1d              | 76.9 K\n",
      "2 | vidcnn    | Conv1d              | 51.3 K\n",
      "3 | hid_layer | Linear              | 3.0 K \n",
      "4 | out_layer | Linear              | 62    \n",
      "--------------------------------------------------\n",
      "182 K     Trainable params\n",
      "0         Non-trainable params\n",
      "182 K     Total params\n",
      "0.729     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  84%|████████▍ | 182/217 [01:20<00:15,  2.25it/s, loss=0.0248, v_num=0]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,youcookdl,youcoodvld_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e40a003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'epoch=4-step=809.ckpt'\n"
     ]
    }
   ],
   "source": [
    "!ls /common/home/vk405/Projects/Crossmdl/nbs/lightning_logs/1/version_0/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "857f60ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"/common/home/vk405/Projects/Crossmdl/nbs/lightning_logs/1/version_0/checkpoints/epoch=4-step=809.ckpt\")\n",
    "model.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4af737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "logfile = pd.read_csv('/common/home/vk405/Projects/Crossmdl/nbs/lightning_logs/1/version_0/metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42ef443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.026618\n",
       "1      0.021022\n",
       "2      0.024978\n",
       "59     0.018742\n",
       "60     0.020773\n",
       "61     0.025293\n",
       "118    0.023904\n",
       "119    0.022377\n",
       "120    0.022719\n",
       "177    0.024133\n",
       "178    0.023531\n",
       "179    0.020826\n",
       "236    0.020842\n",
       "237    0.019654\n",
       "238    0.016795\n",
       "239    0.018659\n",
       "Name: train_loss, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logfile['train_loss'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43a1dad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3      0.030767\n",
       "4      0.020502\n",
       "5      0.026679\n",
       "6      0.018508\n",
       "7      0.024753\n",
       "         ...   \n",
       "290    0.029358\n",
       "291    0.019617\n",
       "292    0.019272\n",
       "293    0.018304\n",
       "294    0.019156\n",
       "Name: val_loss_step, Length: 275, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logfile['val_loss_step'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d551a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    out = []\n",
    "    for batch in youcoodvld_dl:\n",
    "        x_vid,x_wemb,label = batch\n",
    "        predictions = model((x_vid,x_wemb))\n",
    "        p = predictions.squeeze().detach().cpu().numpy()\n",
    "        out.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64d2efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(out,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "817a8670",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bounds = youcookvld.labelencoder.decode(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed876972",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_bounds = youcookvld.labelencoder.truebounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a54a7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[130.22195002, 159.99705858],\n",
       "       [161.5540221 , 189.22471128],\n",
       "       [137.47250343, 171.91168169],\n",
       "       ...,\n",
       "       [231.36430942, 268.12362212],\n",
       "       [454.45702744, 490.36992094],\n",
       "       [410.01993721, 439.09408276]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98401ae7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_bounds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-720d6f7eb967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrue_bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'true_bounds' is not defined"
     ]
    }
   ],
   "source": [
    "true_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8ee1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b25b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "523625c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(youcookdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63723793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.fit(model,youcookdl)\n",
    "x_vid,x_wemb,label = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b999730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model((x_vid,x_wemb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fe2daed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da8e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Crossmdl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
