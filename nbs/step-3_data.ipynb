{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409009c1-8b50-444e-868b-7ade0bd62ff0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a21e739-362d-4b79-b763-a7306f56909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad your sequences\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import linalg as LA\n",
    "from argparse import Namespace\n",
    "from numpy import genfromtxt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d27f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4096 works\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "wandb_logger = lambda dir, version: WandbLogger(\n",
    "    name=\"wandb\", save_dir=dir, version=version\n",
    ")\n",
    "csvlogger = lambda dir, version: CSVLogger(dir, name=\"csvlogs\", version=version)\n",
    "tblogger = lambda dir, version: TensorBoardLogger(dir, name=\"tblogs\", version=version)\n",
    "\n",
    "def get_loggers(dir,version,lis=[\"csv\"]):\n",
    "    lgrs = []\n",
    "    if \"wandb\" in lis:\n",
    "        lgrs.append(wandb_logger(dir, version))\n",
    "    if \"csv\" in lis:\n",
    "        lgrs.append(csvlogger(dir, version))\n",
    "    if \"tb\" in lis:\n",
    "        lgrs.append(tblogger(dir, version))\n",
    "    return lgrs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14f030ec-9e7e-4cea-8ea6-e9625b5cdfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #global variables\n",
    "# data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\n",
    "# annotns_file = data_dir+'annotations/segment_youcookii_annotations_trainval.json'\n",
    "# base_annotns_file = data_dir+'annotations/youcookii_annotations_trainval.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2090811-24af-4862-bc0a-b8f0f2c727b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(base_annotns_file) as json_file:\n",
    "#     base_annotns = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e818b-96b3-41ee-907a-b32905cac01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a5d4db3-a493-4736-8f32-475a617ddb4e",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d3809c-725e-4c9b-b7e8-7c5fcd4ad01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#helper functions\n",
    "def get_vids(base_dir,split):\n",
    "    trn_split = base_dir+split\n",
    "    trn_idlst = []\n",
    "    trn_vidlst = []\n",
    "\n",
    "    f = open(trn_split,'r')\n",
    "    for line in f:\n",
    "        id_,vid = line.split('/')\n",
    "        vid = vid.strip('\\n')\n",
    "        trn_idlst.append(id_)\n",
    "        trn_vidlst.append(vid)\n",
    "        #print(vid)\n",
    "        #break\n",
    "    f.close()\n",
    "    return trn_idlst,trn_vidlst\n",
    "   \n",
    "    \n",
    "#gives video feats\n",
    "def get_features(data_dir,split='val',feat_dir='/common/users/vk405/feat_csv/'):\n",
    "    #feat_dir = data_dir\n",
    "    splits_dir = data_dir+'splits/'\n",
    "    if split == 'val':\n",
    "        feat_split_dir = feat_dir+'val_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'val_list.txt')  \n",
    "    elif split == 'train':\n",
    "        feat_split_dir = feat_dir+'train_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'train_list.txt') \n",
    "    elif split == 'test':\n",
    "        feat_split_dir = feat_dir+'test_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'test_list.txt')\n",
    "    else:\n",
    "        raise NotImplementedError(f'unknown split: {split}')     \n",
    "    feat_list = {}\n",
    "    vid_dtls = []\n",
    "    for num,name in zip(vid_num,vid_name):\n",
    "        feat_loc = os.path.join(feat_split_dir, f'{num}/{name}/0001/')\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if os.path.isdir(feat_loc):\n",
    "            feat_files = feat_loc + os.listdir(feat_loc)[0]\n",
    "            feat_list[name] = feat_files\n",
    "            #feat_list.append(feat_files)\n",
    "            vid_dtls.append((num,name))\n",
    "        else:\n",
    "            print(f\"video : {num}/{name} not found\")\n",
    "    assert len(feat_list) == len(vid_dtls),\"get-features is giving incorrect features\"\n",
    "    return feat_list,vid_dtls\n",
    "\n",
    "\n",
    "def get_labels(ids,annotns_file):\n",
    "    #gives annotations corresponding to the ids(videos) given\n",
    "    label_info = {}\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)\n",
    "        #print(annotns.keys())\n",
    "        for _,vidname in ids:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if vidname in annotns:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                duration = annotns[vidname]['duration']\n",
    "                annot = annotns[vidname]['annotations']\n",
    "                labels = []\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for segment_info in annot:\n",
    "                    interval = segment_info['segment']\n",
    "                    st_end = [interval[0],interval[-1]]\n",
    "                    sent = segment_info['sentence']\n",
    "                    labels.append((st_end,sent,duration))\n",
    "\n",
    "                label_info[vidname] = labels\n",
    "            else:\n",
    "                print(f\"label for {vidname} not present\")\n",
    "    return label_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9731aa9-ef3a-4d16-9185-574661ac63d9",
   "metadata": {},
   "source": [
    "Raw time annotations are converted to corresponding frame-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b8c429-6fbd-451e-8d74-0c43f37d64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feat_locs,vids = get_features(data_dir,split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f452854-bfb1-4de7-ab21-4531bf5deec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9ee3379-164d-42e9-b25c-401f177822ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = get_labels(vids,annotns_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5431b4-2a40-484f-baba-8ec153e51907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244000d3-632a-4d59-8615-a7442377bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels['GLd3aX16zBg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533421d6-4bf8-4d09-a039-4887fea3d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_annotns['database']['GLd3aX16zBg']['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dbf2c51-8fcd-45da-8640-03cb49cd2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid_file= feat_locs['GLd3aX16zBg']\n",
    "# vid = genfromtxt(vid_file, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd40243-f6e7-446c-b956-cd4c70c7cfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86d28638-5a29-45ce-af52-ce7fab60d0c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a87b395-cb12-46b8-8050-b310339a9153",
   "metadata": {},
   "source": [
    "Use contrastive/triplet loss. Train the model to reduce this loss over every negative sample of video frames. Keep It simple and sample a random video frame(non-alighning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e755a3c6-8c3b-4105-a866-bf9f99c753dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#dataset\n",
    "# Dataset/loader\n",
    "# This is newer version\n",
    "class YoucookDset2(Dataset):\n",
    "    global_txt = joblib.load(os.path.join('/common/home/vk405/Projects/Crossmdl/Data/YouCookII/','emb.joblib'))\n",
    "    global_imgfeats = joblib.load('/common/users/vk405/feat_csv/global_imgfeats.joblib')\n",
    "    global_map = joblib.load('/common/users/vk405/feat_csv/global_map.joblib')\n",
    "    def __init__(self,data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\\\n",
    "        ,split='train',framecnt=499):\n",
    "        self.feat_locs = {}\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        self.framecnt = framecnt\n",
    "        #self.use_precomp_emb = use_precomp_emb\n",
    "        self.text_emb = None\n",
    "        if self.split != 'test':\n",
    "            self.annotns_file = data_dir+'annotations/segment_youcookii_annotations_trainval.json'\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Split:{self.split},not yet correctly implemented\")\n",
    "        # if self.use_precomp_emb:\n",
    "        #     self.txt_emb = joblib.load(os.path.join(self.data_dir,'emb.joblib'))\n",
    "        #feat_locs = {'Ysh60eirChU': location of the video}\n",
    "        #import pdb;pdb.set_trace()\n",
    "        self.feat_locs,vids = get_features(self.data_dir,split=self.split)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        assert len(vids) == len(self.feat_locs),\"features are wrong\"\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #label_info = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder = LabelEncoder2()\n",
    "        self.final_labels = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder.fit_transform(label_info)\n",
    "        \n",
    "        #regress_labels(label_info)\n",
    "        #(vid_id,seg_id)\n",
    "        self.data,self.max_seq_ln= self.update_data()\n",
    "\n",
    "                \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def overlap_frac(self,base_rng,tst_rng):\n",
    "        #1.Returns the fraction of frames that are overlapping in tst_rng with base_rng\n",
    "        #2.both ends inclusive\n",
    "        sz = tst_rng[-1]-tst_rng[0]+1\n",
    "        lbl_ids = set(np.arange(base_rng[0],base_rng[-1]+1))\n",
    "        frame_ids = set(np.arange(tst_rng[0],tst_rng[-1]+1))\n",
    "        inter = frame_ids.intersection(lbl_ids)\n",
    "        assert sz != 0,\"base frame rng is zero\"\n",
    "        return len(inter)/sz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_data(self):\n",
    "        data = []\n",
    "        max_cnt = 50\n",
    "        mx_seq_ln = 0\n",
    "        for key in self.final_labels:\n",
    "            segments = self.final_labels[key]\n",
    "            for ind,seg in enumerate(segments):\n",
    "                #trn_points = []\n",
    "                st_end,txt,vid_len = seg\n",
    "                main_seg = (key,self.feat_locs[key],st_end[0],st_end[-1],ind,1.0)\n",
    "                # to keep track of max sequence length in the data\n",
    "                sq_len = st_end[-1]-st_end[0]+1\n",
    "                mx_seq_ln = max(mx_seq_ln,sq_len)\n",
    "\n",
    "                data.append(main_seg)\n",
    "                frame_width = st_end[-1]-st_end[0] + 1\n",
    "                extra_frames = []\n",
    "                #st_end[-1]+1\n",
    "                for cnt,new_st in enumerate(range(st_end[0]+1,499)):\n",
    "                    #forward sliding\n",
    "                    new_end = new_st+frame_width\n",
    "                    if (cnt<max_cnt)and (0<=new_st<self.framecnt and 0<=new_st<self.framecnt):\n",
    "                        extra_frames.append((new_st,new_end))\n",
    "                #st_end[0]\n",
    "                for cnt,new_end in enumerate(range(st_end[-1],0,-1)):\n",
    "                    #backward sliding\n",
    "                    new_st = new_end-frame_width\n",
    "                    if (cnt<max_cnt)and (0<=new_st<self.framecnt and 0<=new_st<self.framecnt):\n",
    "                        extra_frames.append((new_st,new_end))\n",
    "                \n",
    "                #import pdb;pdb.set_trace()\n",
    "                zero_overlap_frames = []\n",
    "                for ex_seg in extra_frames:\n",
    "                    label = self.overlap_frac(st_end,ex_seg)\n",
    "                    # only negative samples\n",
    "                    if label == 0.00:\n",
    "                        zero_overlap_frames.append(ex_seg)\n",
    "                        #data.append((key,self.feat_locs[key],ex_seg[0],ex_seg[-1],ind,label))\n",
    "                        #break\n",
    "                if len(zero_overlap_frames):\n",
    "                    frame_id = np.random.randint(len(zero_overlap_frames))\n",
    "                    ex_seg = zero_overlap_frames[frame_id]\n",
    "                    #print(f\"main_seg:{main_seg},ex_seg:{ex_seg}\")\n",
    "                    data.append((key,self.feat_locs[key],ex_seg[0],ex_seg[-1],ind,0.0))\n",
    "                    sq_len = ex_seg[-1]-ex_seg[0]+1\n",
    "                    mx_seq_ln = max(mx_seq_ln,sq_len)\n",
    "                    \n",
    "        return data,mx_seq_ln\n",
    "    \n",
    "    def read_data_global(self,data):\n",
    "        vid_id,vid_loc,start,end,segid,label = data\n",
    "        ind = self.global_map[vid_id]\n",
    "        img_feat = self.global_imgfeats[ind][start:end+1]\n",
    "        txt_feat = self.global_txt[vid_id][segid]\n",
    "        pad_img = np.concatenate([img_feat,np.zeros((self.max_seq_ln-img_feat.shape[0],img_feat.shape[-1]))])\n",
    "        return pad_img,txt_feat,len(img_feat),label\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.read_data_global(self.data[idx])\n",
    "        \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba92473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ydata = YoucookDset2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5da67c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_loader = DataLoader(ydata,\\\n",
    "#             batch_size=170,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b690b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb6aaae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img,txt,lengths,labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edba3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.shape,lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "758489ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilstm = nn.LSTM(512, 100, 2, bidirectional=True,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a63dce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packed = nn.utils.rnn.pack_padded_sequence(img.float(), \\\n",
    "#     lengths, batch_first=True, enforce_sorted=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5700792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71c488a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiddens, (final_h, final_c) = bilstm(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a515cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d052258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiddens, lengths = nn.utils.rnn.pad_packed_sequence(hiddens, batch_first=True)  # (batch_size, max_length, 2*dim_lstm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16d09b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoLstmEncoder(nn.Module):\n",
    "    # LSTM ENCODER OVER VIDEO FRAMES\n",
    "    def __init__(self,vid_frm_sz=512,vid_hid_sz=512,\\\n",
    "        lyrs=2,bidirectional=True,drop=0.5,fin_sz=256):\n",
    "        super().__init__()\n",
    "        self.vid_frm_sz=vid_frm_sz\n",
    "        self.vid_hid_sz=vid_hid_sz\n",
    "        self.lyrs=lyrs\n",
    "        self.bidirectional=bidirectional\n",
    "        self.fin_sz=fin_sz\n",
    "        ####lstm##\n",
    "        #import pdb;pdb.set_trace()\n",
    "        self.lstm = nn.LSTM(self.vid_frm_sz,self.vid_hid_sz,\\\n",
    "            self.lyrs,bidirectional=self.bidirectional,batch_first=True)\n",
    "            \n",
    "        self.drop = nn.Dropout(drop)\n",
    "        # This gives a fixed size output and adds a non-linearity.\n",
    "        if self.bidirectional:\n",
    "            self.lin = nn.Linear(self.vid_hid_sz*2,self.fin_sz)\n",
    "        else:\n",
    "            self.lin = nn.Linear(self.vid_hid_sz,self.fin_sz)\n",
    "        self.act  = nn.ReLU()\n",
    "        \n",
    "    def forward(self,frame_chunk,lengths):\n",
    "        packed_frames = nn.utils.rnn.pack_padded_sequence(frame_chunk.float(), \\\n",
    "               lengths, batch_first=True, enforce_sorted=False)\n",
    "        hiddens, (final_h, final_c) = self.lstm(packed_frames)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        hiddens, lengths = nn.utils.rnn.pad_packed_sequence(hiddens, batch_first=True) \n",
    "        vid_emb = hiddens.sum(1)/lengths.unsqueeze(1)  \n",
    "        \n",
    "        vid_emb = self.act(self.lin(self.drop(vid_emb)))\n",
    "        return vid_emb\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd425387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing\n",
    "\n",
    "# lstmencoder = VideoLstmEncoder()\n",
    "# out = lstmencoder(img,lengths)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840fad8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ae232a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTextEncoder(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        self.videoencoder = VideoLstmEncoder(**hparams.video_cfg)\n",
    "        self.textencoder = nn.Sequential(*self.linear_encoder(**hparams.text_cfg))\n",
    "        \n",
    "        self.shared = nn.Linear(hparams.shared_inptsz,hparams.shared_outsz)\n",
    "\n",
    "    def forward(self,vid_frames,lengths,txts):\n",
    "        txt_emb = self.textencoder(txts.float())\n",
    "        vid_emb = self.videoencoder(vid_frames,lengths)\n",
    "        loss_cfg = self.hparams.loss_cfg\n",
    "\n",
    "        if loss_cfg['loss_type'] == 'classification':\n",
    "            if loss_cfg['mode'] == 'concat':\n",
    "                #late fusion of two modalities to give class-probability\n",
    "                #import pdb;pdb.set_trace()\n",
    "                fin_emb = self.shared(torch.concat([txt_emb,vid_emb],axis=-1))\n",
    "                return F.sigmoid(fin_emb)\n",
    "                \n",
    "        elif loss_cfg['loss_type'] == 'triplet':\n",
    "            txt_emb = self.shared(txt_emb)\n",
    "            vid_emb = self.shared(vid_emb)\n",
    "            return vid_emb,txt_emb\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        loss_cfg = self.hparams.loss_cfg\n",
    "        vid_frames,txts,lengths,labels = batch\n",
    "        if loss_cfg['loss_type'] == 'classification':\n",
    "            sim_score = self(vid_frames,lengths,txts)\n",
    "            loss = F.binary_cross_entropy(sim_score.squeeze().float(),\\\n",
    "                     labels.squeeze().float())\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.log(\"train_loss\",loss,on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        # same as training_step but happens with in torch.no_grad()\n",
    "        loss_cfg = self.hparams.loss_cfg\n",
    "        vid_frames,txts,lengths,labels = batch\n",
    "        if loss_cfg['loss_type'] == 'classification':\n",
    "            sim_score = self(vid_frames,lengths,txts)\n",
    "            loss = F.binary_cross_entropy(sim_score.squeeze().float(), \\\n",
    "                labels.squeeze().float())\n",
    "            s = sim_score.squeeze().float()\n",
    "            l = labels.squeeze().float()\n",
    "            preds = (torch.where(s > 0.5, 1.0, 0.0) == l)\n",
    "            val_acc = torch.mean(preds.type(torch.DoubleTensor))\n",
    "\n",
    "\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.log(\"validation_loss\",loss,on_step=False, on_epoch=True)\n",
    "        self.log(\"validation_acc\",val_acc,on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, 'min',\\\n",
    "            factor=0.8,patience=5,min_lr=5e-5)\n",
    "        return {\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_scheduler\": {\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            \"monitor\": \"validation_loss\"\n",
    "               }\n",
    "              }\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def triplet_loss(self,img,txt,anch,reduce='mean'):\n",
    "        #ð¿ð‘ð‘œð‘ (ð’‚,ð’‘,ð’)=max[ð‘‘(ð’‚,ð’)âˆ’ð‘‘(ð’‚,ð’‘)+ðœ–,0]\n",
    "        loss_cfg = self.hparams.loss_cfg\n",
    "        eps = loss_cfg['eps'] if 'eps' in loss_cfg else 0.3\n",
    "        \n",
    "        im_norm,txt_norm,anch_norm = LA.norm(img,dim=-1).reshape(img.shape[0],1),\\\n",
    "        LA.norm(txt,dim=-1).reshape(txt.shape[0],1),\\\n",
    "            LA.norm(anch,dim=-1).reshape(anch.shape[0],1)\n",
    "        normd_img = img/im_norm\n",
    "        normd_txt = txt/txt_norm\n",
    "        normd_anch = anch/anch_norm\n",
    "\n",
    "        cos_sim_p = torch.sum(normd_img*normd_txt,dim=-1)\n",
    "        cos_sim_n = torch.sum(normd_anch*normd_txt,dim=-1)\n",
    "        if 'use_mse' in loss_cfg and loss_cfg['use_mse']:\n",
    "            #print(f\"USING MSE LOSS\")\n",
    "            loss  = F.mse_loss(img,txt)\n",
    "        else:\n",
    "            unclipped_loss = cos_sim_n-cos_sim_p+eps\n",
    "            loss = torch.relu(unclipped_loss)\n",
    "        if reduce == 'mean':\n",
    "            return torch.mean(loss),(torch.mean(cos_sim_n),torch.mean(cos_sim_p))\n",
    "\n",
    "    \n",
    "    def linear_encoder(self,init_sz,fin_sz,lyrs,activation='relu'):\n",
    "        # Gives #lyrs of liner+activation layers with hidden size as a linear interpolation\n",
    "        #between init_sz and fin_sz\n",
    "        lyr_lst = []\n",
    "        hd_sz = [round(ele) for ele in np.linspace(init_sz,fin_sz,lyrs+1)][1:]\n",
    "        lst_sz = init_sz\n",
    "        for i in range(lyrs):\n",
    "            lyr_lst.append(nn.Linear(lst_sz,hd_sz[i]))\n",
    "            if activation == 'relu':\n",
    "                lyr_lst.append(nn.ReLU())\n",
    "            lst_sz = hd_sz[i]\n",
    "        return lyr_lst\n",
    "\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "246ee012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010737418240000006"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.8**10)*(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9fe8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg\n",
    "cfg = Namespace(\n",
    "    seed = 0,\n",
    "    version = 'retrieval',\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/Recipe/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/Recipe/\",\n",
    "    mode = 'train',\n",
    "    video_cfg = {'vid_frm_sz':512,'vid_hid_sz':128,\\\n",
    "        'lyrs':1,'bidirectional':True,'drop':0.5,'fin_sz':256},\n",
    "    text_cfg = {'init_sz':768,\\\n",
    "        'fin_sz':256,'lyrs':2,'activation':'relu'},\n",
    "    loss_cfg = {'loss_type':'classification','mode':'concat'},\n",
    "    shared_inptsz = 512,\n",
    "    shared_outsz = 1,\n",
    "    lr = 0.0004,\n",
    "    loggers = [\"csv\"],\n",
    "    cbs = [\"checkpoint\",\"early_stop\"],\n",
    "    trainer = {'log_every_n_steps': 50,\n",
    "    'max_epochs': 100},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"val_recall_1\"},\n",
    "    early_stop = {\"monitor\":\"val_recall_1\",\"patience\":2,\"mode\":'max'},\n",
    "    batch_size=512,\n",
    "    val_batch_size = 2000\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd958cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing\n",
    "\n",
    "# ydata = YoucookDset2()\n",
    "\n",
    "# train_loader = DataLoader(ydata,\\\n",
    "#             batch_size=10,shuffle=True)\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# img,txt,lengths,labels = batch\n",
    "\n",
    "# net = VideoTextEncoder(cfg)\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e16548e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing\n",
    "# net.textencoder(txt.float()).shape,net.videoencoder(img,lengths).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50b5c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = net(img,lengths,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d24ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing\n",
    "# out,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "822dd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d053902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing\n",
    "# ydata = YoucookDset2()\n",
    "\n",
    "\n",
    "# ydata_ss = torch.utils.data.Subset(ydata,range(2000))\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(ydata_ss,\\\n",
    "#             batch_size=170,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68ce1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing\n",
    "# #track_grad_norm=2,gradient_clip_val=0.5\n",
    "\n",
    "\n",
    "# net = VideoTextEncoder(cfg)\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61e1eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(deterministic=True,**cfg.trainer\n",
    "#         )\n",
    "# trainer.fit(net, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cd8f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_d = []\n",
    "# cnt = 0\n",
    "# for ele in train_loader:\n",
    "#     batch_d.append(ele)\n",
    "#     cnt += 1\n",
    "#     if cnt >= 5:\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f059341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img,txt,lengths,labels = batch_d[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f0b474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.model(img,lengths,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74c9b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cfg):\n",
    "    pl.seed_everything(cfg.seed)\n",
    "    dir = cfg.artifacts_loc\n",
    "    version = str(cfg.version)\n",
    "    logger_list = get_loggers(dir, version,cfg.loggers)\n",
    "    cbs = []\n",
    "    if \"early_stop\" in cfg.cbs:\n",
    "        #? does'nt really work atm\n",
    "        params = cfg.model.cbs.early_stop\n",
    "        earlystopcb = EarlyStopping(**params, min_delta=0.00, verbose=False)\n",
    "        cbs.append(earlystopcb)\n",
    "    if \"checkpoint\" in cfg.cbs:\n",
    "        store_path = dir + \"ckpts/\" + str(cfg.version) + \"/\"\n",
    "        isExist = os.path.exists(store_path)\n",
    "        if isExist and os.path.isdir(store_path) and ('retrain' not in cfg and not cfg.retrain):\n",
    "            shutil.rmtree(store_path)\n",
    "        # then create fresh\n",
    "        if not isExist:\n",
    "            os.makedirs(store_path)\n",
    "        fname = \"{epoch}-{train_loss:.2f}\"\n",
    "        params = cfg.checkpoint\n",
    "        checkptcb = ModelCheckpoint(**params, dirpath=store_path, filename=fname)\n",
    "        cbs.append(checkptcb)\n",
    "    if \"wandb\" in cfg.loggers:\n",
    "        wandb.init(project=\"videoretrieval\", config=cfg)\n",
    "    if cfg.mode == 'train':\n",
    "        split = cfg.data_split if 'data_split' in cfg else cfg.mode\n",
    "        youcookdata = YoucookDset2(data_dir=cfg.data_dir,split=split)\n",
    "        vld_sz = int(cfg.val_split*len(youcookdata))\n",
    "        trn_sz = len(youcookdata)-vld_sz\n",
    "        trn_data,vld_data = random_split(youcookdata,[trn_sz,vld_sz])\n",
    "        \n",
    "        if cfg.use_precomp_emb:\n",
    "            pass\n",
    "            #global_txt = joblib.load(os.path.join(data_dir,'emb.joblib'))\n",
    "            #collate_wrapper = MyCollator(txtemb=global_txt)\n",
    "        #pin_memory=True\n",
    "        train_loader = DataLoader(trn_data,\\\n",
    "            batch_size=cfg.batch_size,shuffle=True)\n",
    "\n",
    "        valid_loader = DataLoader(vld_data,batch_size=cfg.batch_size)\n",
    "           \n",
    "        net = VideoTextEncoder(cfg)\n",
    "        #gpus=1,gpus=3,accelerator='ddp'\n",
    "        if 'retrain' in cfg and cfg.retrain:\n",
    "            path = cfg.artifacts_loc+'ckpts/'+cfg.version +'/'\n",
    "            weight = os.listdir(path)[-1]\n",
    "            ck_path = path+weight\n",
    "            print(f\"loading from:{ck_path}\")\n",
    "            net = net.load_from_checkpoint(ck_path)\n",
    "\n",
    "        lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger_list,callbacks=cbs+[lr_monitor],deterministic=True,\\\n",
    "                gradient_clip_val=0.8,**cfg.trainer\n",
    "        )\n",
    "        trainer.fit(net, train_loader,valid_loader)\n",
    "        return trainer\n",
    "        #trainer.tune(net,train_loader)\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16d71c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg\n",
    "cfg = Namespace(\n",
    "    seed = 0,\n",
    "    version = 'LstmEncoder',\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/YouCookII/\",\n",
    "    mode = 'train',\n",
    "    val_split = 0.2,\n",
    "    video_cfg = {'vid_frm_sz':512,'vid_hid_sz':128,\\\n",
    "        'lyrs':1,'bidirectional':True,'drop':0.5,'fin_sz':256},\n",
    "    text_cfg = {'init_sz':768,\\\n",
    "        'fin_sz':256,'lyrs':2,'activation':'relu'},\n",
    "    loss_cfg = {'loss_type':'classification','mode':'concat'},\n",
    "    shared_inptsz = 512,\n",
    "    shared_outsz = 1,\n",
    "    lr = 0.0004,\n",
    "    loggers = [\"csv\",\"wandb\"],\n",
    "    cbs = [\"checkpoint\"],\n",
    "    trainer = {'log_every_n_steps': 5,\n",
    "    'max_epochs': 200},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"validation_loss\"},\n",
    "    early_stop = {\"monitor\":\"validation_loss\",\"patience\":2,\"mode\":'min'},\n",
    "    batch_size=512,\n",
    "    val_batch_size = 2000,\n",
    "    retrain=False,\n",
    "    use_precomp_emb=True\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82f79bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvin136\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/vin136/videoretrieval/runs/1dh2lt9y\" target=\"_blank\">curious-cosmos-24</a></strong> to <a href=\"https://wandb.ai/vin136/videoretrieval\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | videoencoder | VideoLstmEncoder | 723 K \n",
      "1 | textencoder  | Sequential       | 525 K \n",
      "2 | shared       | Linear           | 513   \n",
      "--------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.995     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/.local/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157:  13%|â–ˆâ–Ž        | 5/38 [02:15<14:52, 27.03s/it, loss=0.165, v_num=oder] "
     ]
    }
   ],
   "source": [
    "trainer = run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a09b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a37acb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
