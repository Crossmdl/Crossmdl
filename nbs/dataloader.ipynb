{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad your sequences\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "target = torch.randn(3,4)\n",
    "query = torch.randn(1,4)\n",
    "\n",
    "\n",
    "\n",
    "out = pad_sequence([target,query],batch_first=True)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of data in your dataloader\n",
    "# len(data) is not divisible by batch_size on purpose to verify consistency across batch sizes\n",
    "#data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False,\\\n",
    "#  collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = [torch.rand(23,4),torch.rand(10,4),torch.rand(2,4)]\n",
    "\n",
    "sq_lens = list(map(lambda x:x.size(0),batch))\n",
    "ln_key = batch[0].size(-1)\n",
    "# mask = (batch_size, 1, length_key)(all queries have same mask)\n",
    "mask = torch.ones(len(sq_lens),1,max(sq_lens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 23])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,ele in enumerate(sq_lens):\n",
    "    mask[ind,:,:ele] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 23])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 10, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #sample masking\n",
    "\n",
    "# target = torch.randn(2,3,4)\n",
    "# query = torch.randn(2,1,4)\n",
    "# mask = torch.tensor(np.stack([np.array([[True,True,False]]),np.array([[True,False,False]])]))\n",
    "\n",
    "# attn_dec_layer = MultiHeadAttention(4, 1, dropout_rate=0) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the dataset of balanced size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def get_vids(base_dir,split):\n",
    "    trn_split = base_dir+split\n",
    "    trn_idlst = []\n",
    "    trn_vidlst = []\n",
    "\n",
    "    f = open(trn_split,'r')\n",
    "    for line in f:\n",
    "        id_,vid = line.split('/')\n",
    "        vid = vid.strip('\\n')\n",
    "        trn_idlst.append(id_)\n",
    "        trn_vidlst.append(vid)\n",
    "        #print(vid)\n",
    "        #break\n",
    "    f.close()\n",
    "    return trn_idlst,trn_vidlst\n",
    "\n",
    "    \n",
    "def get_features(data_dir,split='val',feat_dir='/common/users/vk405/feat_csv/'):\n",
    "    #feat_dir = data_dir\n",
    "    splits_dir = data_dir+'splits/'\n",
    "    if split == 'val':\n",
    "        feat_split_dir = feat_dir+'val_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'val_list.txt')  \n",
    "    elif split == 'train':\n",
    "        feat_split_dir = feat_dir+'train_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'train_list.txt') \n",
    "    elif split == 'test':\n",
    "        feat_split_dir = feat_dir+'test_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'test_list.txt')\n",
    "    else:\n",
    "        raise NotImplementedError(f'unknown split: {split}')     \n",
    "    feat_list = {}\n",
    "    vid_dtls = []\n",
    "    for num,name in zip(vid_num,vid_name):\n",
    "        feat_loc = os.path.join(feat_split_dir, f'{num}/{name}/0001/')\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if os.path.isdir(feat_loc):\n",
    "            feat_files = feat_loc + os.listdir(feat_loc)[0]\n",
    "            feat_list[name] = feat_files\n",
    "            #feat_list.append(feat_files)\n",
    "            vid_dtls.append((num,name))\n",
    "        else:\n",
    "            print(f\"video : {num}/{name} not found\")\n",
    "    assert len(feat_list) == len(vid_dtls),\"get-features is giving incorrect features\"\n",
    "    return feat_list,vid_dtls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_raw_labels(ids,annotns_file):\n",
    "\n",
    "    label_info = {}\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)\n",
    "        print(annotns.keys())\n",
    "        for _,vidname in ids:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if vidname in annotns['database']:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                duration = annotns['database'][vidname]['duration']\n",
    "                annot = annotns['database'][vidname]['annotations']\n",
    "                labels = []\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for segment_info in annot:\n",
    "                    interval = segment_info['segment']\n",
    "                    sent = segment_info['sentence']\n",
    "                    labels.append((interval,sent,duration))\n",
    "\n",
    "                label_info[vidname] = labels\n",
    "            else:\n",
    "                print(f\"label for {vidname} not present\")\n",
    "    return label_info\n",
    "\n",
    "def regress_labels(raw_labels):\n",
    "    regress_labels = {}\n",
    "    for key in raw_labels:\n",
    "        new_labels = []\n",
    "        for item in raw_labels[key]:\n",
    "            rng,sent,vidlen = item\n",
    "            mid = sum(rng)/2\n",
    "            duration = rng[-1]-rng[0]\n",
    "            mid_pred = (1/vidlen)*mid # location of mid-point w.r.t video length\n",
    "            duration_pred = (1/vidlen)*duration\n",
    "            new_labels.append(([mid_pred,duration_pred],sent))\n",
    "        regress_labels[key] = new_labels\n",
    "    return regress_labels\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "def get_labels(ids,annotns_file):\n",
    "\n",
    "    label_info = {}\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)\n",
    "        #print(annotns.keys())\n",
    "        for _,vidname in ids:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if vidname in annotns:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                duration = annotns[vidname]['duration']\n",
    "                annot = annotns[vidname]['annotations']\n",
    "                labels = []\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for segment_info in annot:\n",
    "                    interval = segment_info['segment']\n",
    "                    st_end = [interval[0],interval[-1]]\n",
    "                    sent = segment_info['sentence']\n",
    "                    labels.append((st_end,sent,duration))\n",
    "\n",
    "                label_info[vidname] = labels\n",
    "            else:\n",
    "                print(f\"label for {vidname} not present\")\n",
    "    return label_info\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#dataset\n",
    "# Dataset/loader\n",
    "# This is newer version\n",
    "class YoucookDset2(Dataset):\n",
    "    def __init__(self,data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\\\n",
    "        ,split='train',use_precomp_emb=True,seqlen=26,framecnt=499,id=0):\n",
    "        self.id = id\n",
    "        self.feat_locs = {}\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        self.use_precomp_emb = use_precomp_emb\n",
    "        self.text_emb = None\n",
    "        self.seqlen = seqlen\n",
    "        self.framecnt = framecnt\n",
    "        if self.split != 'test':\n",
    "            self.annotns_file = data_dir+'annotations/segment_youcookii_annotations_trainval.json'\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Split:{self.split},not yet correctly implemented\")\n",
    "        if self.use_precomp_emb:\n",
    "            self.txt_emb = joblib.load(os.path.join(self.data_dir,'emb.joblib'))\n",
    "        #feat_locs = {'Ysh60eirChU': location of the video}\n",
    "        self.feat_locs,vids = get_features(self.data_dir,split=self.split)\n",
    "        assert len(vids) == len(self.feat_locs),\"features are wrong\"\n",
    "        #import pdb;pdb.set_trace()\n",
    "        label_info = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder = LabelEncoder2()\n",
    "        self.final_labels = label_info\n",
    "        #self.labelencoder.fit_transform(label_info)\n",
    "        \n",
    "        #regress_labels(label_info)\n",
    "        #(vid_id,seg_id)\n",
    "        self.update_data()\n",
    "\n",
    "                \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def update_data(self,id=None):\n",
    "        self.data = []\n",
    "        #self.vid_len = []\n",
    "        if not id:\n",
    "            id = self.id\n",
    "            \n",
    "        starting_pnt = np.arange(id,self.framecnt,self.seqlen)\n",
    "\n",
    "        for key in self.final_labels:\n",
    "            annot_len = len(self.final_labels[key])\n",
    "            if key in self.feat_locs:\n",
    "                file_loc = self.feat_locs[key]\n",
    "                #for stpnt in starting_pnt:\n",
    "                segments = list(zip(repeat(key,annot_len),repeat(file_loc,annot_len),\\\n",
    "                        range(annot_len)))\n",
    "                for seg in segments:\n",
    "                    for stpnt in starting_pnt:\n",
    "                        if stpnt+self.seqlen<=self.framecnt:\n",
    "                            datapnt = seg[:-1]+(stpnt,)+seg[-1:]\n",
    "                            self.data.append(datapnt)\n",
    "                    \n",
    "                #self.data.extend(segments)\n",
    "            else:\n",
    "                print(f\"video:{key} not found\")\n",
    "\n",
    "\n",
    "    def getclass_prob(self,lbl_rng,frame_rng):\n",
    "        lbl_ids = set(np.arange(lbl_rng[0],lbl_rng[-1]+1))\n",
    "        frame_ids = set(np.arange(frame_rng[0],frame_rng[-1]+1))\n",
    "        inter = lbl_ids.intersection(frame_ids)\n",
    "        if len(inter) == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return len(inter)/len(lbl_ids.union(frame_ids))\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if self.use_precomp_emb:\n",
    "            vidname,file_loc,stid,seg_ind = self.data[idx]\n",
    "            #import pdb;pdb.set_trace()\n",
    "            #self.txt_emb[vidname][seg_ind],\n",
    "            txt_info = self.final_labels[vidname][seg_ind]\n",
    "            label_value = self.getclass_prob(txt_info[0],(stid,stid+self.seqlen-1))\n",
    "            #import pdb;pdb.set_trace()\n",
    "            return pd.read_csv(file_loc).values.astype(np.float32)[stid:stid+self.seqlen,:],(self.txt_emb[vidname][seg_ind]).astype(np.float32),\\\n",
    "                label_value\n",
    "            #np.array(self.final_labels[vidname][seg_ind][0],dtype=np.float32)\n",
    "        else:\n",
    "            raise NotImplementedError(\"not yet correctly implemented\")\n",
    "\n",
    "        \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "youcookdata = YoucookDset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([98, 102], 'heat 2 tbsp ghee in a pan', 609.97),\n",
       " ([105, 162],\n",
       "  'add cinnamon bay leaves green cardamoms black cardamoms green chillies and saute',\n",
       "  609.97),\n",
       " ([182, 200], 'add onions and saute for 3-4 minutes', 609.97),\n",
       " ([215, 245], 'add mutton and saute for 2 minutes', 609.97),\n",
       " ([247, 252], 'add ginger-garlic paste and mix well', 609.97),\n",
       " ([253, 281],\n",
       "  'add salt 2 cup water and cover to pressure cook on high heat for 5 minutes',\n",
       "  609.97),\n",
       " ([323, 350],\n",
       "  'heat crushed peppercorns and cashew nut paste with remaining ghee in a pan',\n",
       "  609.97),\n",
       " ([389, 409],\n",
       "  'add the cooked mutton with stock and spices and mix everything well',\n",
       "  609.97),\n",
       " ([416, 433], 'add garam masala powder cream and stir to mix', 609.97),\n",
       " ([466, 469], 'sprinkle crushed peppercorns on top and serve', 609.97)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youcookdata.final_labels['Ysh60eirChU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ysh60eirChU',\n",
       " '/common/users/vk405/feat_csv/train_frame_feat_csv/405/Ysh60eirChU/0001/resnet_34_feat_mscoco.csv',\n",
       " 0,\n",
       " 0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youcookdata.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/common/users/vk405/feat_csv/train_frame_feat_csv/405/Ysh60eirChU/0001/resnet_34_feat_mscoco.csv'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youcookdata.feat_locs['Ysh60eirChU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spl = pd.read_csv('/common/users/vk405/feat_csv/train_frame_feat_csv/405/Ysh60eirChU/0001/resnet_34_feat_mscoco.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 512)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ysh60eirChU',\n",
       " '/common/users/vk405/feat_csv/train_frame_feat_csv/405/Ysh60eirChU/0001/resnet_34_feat_mscoco.csv',\n",
       " 26,\n",
       " 0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youcookdata.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 10\n",
    "end = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "#Point = namedtuple(\"vid_id\", \"vid_loc\",\"start\",\"end\",\"segid\",\"label\")\n",
    "\n",
    "def overlap_frac(base_rng,tst_rng):\n",
    "    #1.Returns the fraction of frames that are overlapping in tst_rng with base_rng\n",
    "    #2.both ends inclusive\n",
    "    sz = tst_rng[-1]-tst_rng[0]+1\n",
    "    lbl_ids = set(np.arange(base_rng[0],base_rng[-1]+1))\n",
    "    frame_ids = set(np.arange(tst_rng[0],tst_rng[-1]+1))\n",
    "    inter = frame_ids.intersection(lbl_ids)\n",
    "    assert sz != 0,\"base frame rng is zero\"\n",
    "    return len(inter)/sz\n",
    "    \n",
    "\n",
    "data = []\n",
    "max_cnt = 50\n",
    "for key in youcookdata.final_labels:\n",
    "    segments = youcookdata.final_labels[key]\n",
    "    for ind,seg in enumerate(segments):\n",
    "        #trn_points = []\n",
    "        st_end,txt,vid_len = seg\n",
    "        main_seg = (key,youcookdata.feat_locs[key],st_end[0],st_end[-1],ind,1.0)\n",
    "        data.append(main_seg)\n",
    "        frame_width = st_end[-1]-st_end[0] + 1\n",
    "        extra_frames = []\n",
    "        for cnt,new_st in enumerate(range(st_end[0]+1,st_end[-1]+1)):\n",
    "            #forward sliding\n",
    "            new_end = new_st+frame_width\n",
    "            if (cnt<max_cnt)and (0<=new_st<youcookdata.framecnt and 0<=new_st<youcookdata.framecnt):\n",
    "                extra_frames.append((new_st,new_end))\n",
    "        for cnt,new_end in enumerate(range(st_end[-1],st_end[0],-1)):\n",
    "            #backward sliding\n",
    "            new_st = new_end-frame_width\n",
    "            if (cnt<max_cnt)and (0<=new_st<youcookdata.framecnt and 0<=new_st<youcookdata.framecnt):\n",
    "                extra_frames.append((new_st,new_end))\n",
    "        #import pdb;pdb.set_trace()\n",
    "        for ex_seg in extra_frames:\n",
    "            label = overlap_frac(st_end,ex_seg)\n",
    "            data.append((key,youcookdata.feat_locs[key],ex_seg[0],ex_seg[-1],ind,label))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573897"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dist = [ele[-1] for ele in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame({'percentile_ind':np.linspace(0,100,25),\n",
    " 'percentile':[np.percentile(label_dist,p) for p in np.linspace(0,100,25)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all values are equally distributed\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#dataset\n",
    "# Dataset/loader\n",
    "# This is newer version\n",
    "class YoucookDset2(Dataset):\n",
    "    def __init__(self,data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\\\n",
    "        ,split='train',use_precomp_emb=True,seqlen=26,framecnt=499,id=0):\n",
    "        self.id = id\n",
    "        self.feat_locs = {}\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        self.use_precomp_emb = use_precomp_emb\n",
    "        self.text_emb = None\n",
    "        self.seqlen = seqlen\n",
    "        self.framecnt = framecnt\n",
    "        if self.split != 'test':\n",
    "            self.annotns_file = data_dir+'annotations/segment_youcookii_annotations_trainval.json'\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Split:{self.split},not yet correctly implemented\")\n",
    "        if self.use_precomp_emb:\n",
    "            self.txt_emb = joblib.load(os.path.join(self.data_dir,'emb.joblib'))\n",
    "        #feat_locs = {'Ysh60eirChU': location of the video}\n",
    "        self.feat_locs,vids = get_features(self.data_dir,split=self.split)\n",
    "        assert len(vids) == len(self.feat_locs),\"features are wrong\"\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #label_info = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder = LabelEncoder2()\n",
    "        self.final_labels = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder.fit_transform(label_info)\n",
    "        \n",
    "        #regress_labels(label_info)\n",
    "        #(vid_id,seg_id)\n",
    "        self.data = self.update_data()\n",
    "\n",
    "                \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def overlap_frac(self,base_rng,tst_rng):\n",
    "        #1.Returns the fraction of frames that are overlapping in tst_rng with base_rng\n",
    "        #2.both ends inclusive\n",
    "        sz = tst_rng[-1]-tst_rng[0]+1\n",
    "        lbl_ids = set(np.arange(base_rng[0],base_rng[-1]+1))\n",
    "        frame_ids = set(np.arange(tst_rng[0],tst_rng[-1]+1))\n",
    "        inter = frame_ids.intersection(lbl_ids)\n",
    "        assert sz != 0,\"base frame rng is zero\"\n",
    "        return len(inter)/sz\n",
    "\n",
    "\n",
    "    def update_data(self):\n",
    "        data = []\n",
    "        max_cnt = 50\n",
    "        for key in self.final_labels:\n",
    "            segments = self.final_labels[key]\n",
    "            for ind,seg in enumerate(segments):\n",
    "                #trn_points = []\n",
    "                st_end,txt,vid_len = seg\n",
    "                main_seg = (key,self.feat_locs[key],st_end[0],st_end[-1],ind,1.0)\n",
    "                data.append(main_seg)\n",
    "                frame_width = st_end[-1]-st_end[0] + 1\n",
    "                extra_frames = []\n",
    "                for cnt,new_st in enumerate(range(st_end[0]+1,st_end[-1]+1)):\n",
    "                    #forward sliding\n",
    "                    new_end = new_st+frame_width\n",
    "                    if (cnt<max_cnt)and (0<=new_st<self.framecnt and 0<=new_st<self.framecnt):\n",
    "                        extra_frames.append((new_st,new_end))\n",
    "                for cnt,new_end in enumerate(range(st_end[-1],st_end[0],-1)):\n",
    "                    #backward sliding\n",
    "                    new_st = new_end-frame_width\n",
    "                    if (cnt<max_cnt)and (0<=new_st<self.framecnt and 0<=new_st<self.framecnt):\n",
    "                        extra_frames.append((new_st,new_end))\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for ex_seg in extra_frames:\n",
    "                    label = self.overlap_frac(st_end,ex_seg)\n",
    "                    data.append((key,self.feat_locs[key],ex_seg[0],ex_seg[-1],ind,label))\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]\n",
    "        \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata = YoucookDset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ysh60eirChU',\n",
       " '/common/users/vk405/feat_csv/train_frame_feat_csv/405/Ysh60eirChU/0001/resnet_34_feat_mscoco.csv',\n",
       " 98,\n",
       " 102,\n",
       " 0,\n",
       " 1.0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,TensorDataset\n",
    "inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "dataset = TensorDataset(inps, tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = ['h','l','e','o']\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = ToyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\n",
    "global_txt = joblib.load(os.path.join(data_dir,'emb.joblib'))\n",
    "def collate_wrapper(data):\n",
    "    labels = []\n",
    "    vid_embs = []\n",
    "    txt_embs = []\n",
    "    #namedtuple(\"vid_id\", \"vid_loc\",\"start\",\"end\",\"segid\",\"label\")\n",
    "    batched_data = pd.DataFrame(data,columns=[\"vid_id\", \"vid_loc\",\"start\",\"end\",\"segid\",\"label\"])\n",
    "    unique_locs = batched_data['vid_loc'].unique()\n",
    "    for loc in unique_locs:\n",
    "        locwise = batched_data[batched_data['vid_loc']==loc]\n",
    "        tot_vid = pd.read_csv(loc).values\n",
    "        txtemb = None\n",
    "        for ind,ele in locwise.iterrows():\n",
    "            vid_id,_,st,end,segid,label = ele\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if not txtemb:\n",
    "                txtemb = global_txt[ele['vid_id']]\n",
    "            vid_embs.append(torch.tensor(tot_vid[ele['start']:ele['end']+1]))\n",
    "            txt_embs.append(torch.tensor(txtemb[ele['segid']]))\n",
    "            labels.append(torch.tensor(ele['label']))\n",
    "    #return (np.stack(vid_embs),np.stack(txt_embs)),np.stack(labels)\n",
    "    return (vid_embs,txt_embs),labels\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = [ydata[i] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = collate_wrapper(toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][1][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_o = pad_sequence(out[0][0],batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 512])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_txt['GLd3aX16zBg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(ydata, batch_size=2, collate_fn=collate_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [(1,'st'),(1,'john'),(3,'st')]\n",
    "\n",
    "out = pd.DataFrame(d,columns=['cnt','str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnt</th>\n",
       "      <th>str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>st</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cnt   str\n",
       "0    1    st\n",
       "1    1  john\n",
       "2    3    st"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st\n",
      "john\n",
      "st\n"
     ]
    }
   ],
   "source": [
    "for id,ele in out.iterrows():\n",
    "    print(ele['str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ysh60eirChU'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydata[0][1].split('/')[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,8,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 8, 8, 6])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[0,0,1,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnt</th>\n",
       "      <th>str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>st</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cnt   str\n",
       "0    1    st\n",
       "1    1  john\n",
       "2    3    st"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCustomBatch:\n",
    "    def __init__(self, data):\n",
    "        transposed_data = list(zip(*data))\n",
    "        self.inp = torch.stack(transposed_data[0], 0)\n",
    "        self.tgt = torch.stack(transposed_data[1], 0)\n",
    "\n",
    "    # custom memory pinning method on custom type\n",
    "    def pin_memory(self):\n",
    "        self.inp = self.inp.pin_memory()\n",
    "        self.tgt = self.tgt.pin_memory()\n",
    "        return self\n",
    "\n",
    "def collate_wrapper(batch):\n",
    "    return SimpleCustomBatch(batch)\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n",
    "                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_frac(base_rng,tst_rng):\n",
    "    #1.Returns the fraction of frames that are overlapping in tst_rng with base_rng\n",
    "    #2.both ends inclusive\n",
    "    sz = tst_rng[-1]-tst_rng[0]+1\n",
    "    lbl_ids = set(np.arange(base_rng[0],base_rng[-1]+1))\n",
    "    frame_ids = set(np.arange(tst_rng[0],tst_rng[-1]+1))\n",
    "    inter = frame_ids.intersection(lbl_ids)\n",
    "    assert sz != 0,\"base frame rng is zero\"\n",
    "    return len(inter)/sz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{15, 16, 17, 18, 19, 20}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap_frac((10,20),(15,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Crossmdl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
