{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad your sequences\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = torch.randn(3,4)\n",
    "# query = torch.randn(1,4)\n",
    "\n",
    "\n",
    "\n",
    "# out = pad_sequence([target,query],batch_first=True)\n",
    "# out.shape\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of data in your dataloader\n",
    "# len(data) is not divisible by batch_size on purpose to verify consistency across batch sizes\n",
    "#data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False,\\\n",
    "#  collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch = [torch.rand(23,4),torch.rand(10,4),torch.rand(2,4)]\n",
    "\n",
    "# sq_lens = list(map(lambda x:x.size(0),batch))\n",
    "# ln_key = batch[0].size(-1)\n",
    "# # mask = (batch_size, 1, length_key)(all queries have same mask)\n",
    "# mask = torch.ones(len(sq_lens),1,max(sq_lens))\n",
    "\n",
    "# for ind,ele in enumerate(sq_lens):\n",
    "#     mask[ind,:,:ele] = 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sample masking\n",
    "\n",
    "# target = torch.randn(2,3,4)\n",
    "# query = torch.randn(2,1,4)\n",
    "# mask = torch.tensor(np.stack([np.array([[True,True,False]]),np.array([[True,False,False]])]))\n",
    "\n",
    "# attn_dec_layer = MultiHeadAttention(4, 1, dropout_rate=0) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the dataset of balanced size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def get_vids(base_dir,split):\n",
    "    trn_split = base_dir+split\n",
    "    trn_idlst = []\n",
    "    trn_vidlst = []\n",
    "\n",
    "    f = open(trn_split,'r')\n",
    "    for line in f:\n",
    "        id_,vid = line.split('/')\n",
    "        vid = vid.strip('\\n')\n",
    "        trn_idlst.append(id_)\n",
    "        trn_vidlst.append(vid)\n",
    "        #print(vid)\n",
    "        #break\n",
    "    f.close()\n",
    "    return trn_idlst,trn_vidlst\n",
    "\n",
    "    \n",
    "def get_features(data_dir,split='val',feat_dir='/common/users/vk405/feat_csv/'):\n",
    "    #feat_dir = data_dir\n",
    "    splits_dir = data_dir+'splits/'\n",
    "    if split == 'val':\n",
    "        feat_split_dir = feat_dir+'val_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'val_list.txt')  \n",
    "    elif split == 'train':\n",
    "        feat_split_dir = feat_dir+'train_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'train_list.txt') \n",
    "    elif split == 'test':\n",
    "        feat_split_dir = feat_dir+'test_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'test_list.txt')\n",
    "    else:\n",
    "        raise NotImplementedError(f'unknown split: {split}')     \n",
    "    feat_list = {}\n",
    "    vid_dtls = []\n",
    "    for num,name in zip(vid_num,vid_name):\n",
    "        feat_loc = os.path.join(feat_split_dir, f'{num}/{name}/0001/')\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if os.path.isdir(feat_loc):\n",
    "            feat_files = feat_loc + os.listdir(feat_loc)[0]\n",
    "            feat_list[name] = feat_files\n",
    "            #feat_list.append(feat_files)\n",
    "            vid_dtls.append((num,name))\n",
    "        else:\n",
    "            print(f\"video : {num}/{name} not found\")\n",
    "    assert len(feat_list) == len(vid_dtls),\"get-features is giving incorrect features\"\n",
    "    return feat_list,vid_dtls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_raw_labels(ids,annotns_file):\n",
    "\n",
    "    label_info = {}\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)\n",
    "        print(annotns.keys())\n",
    "        for _,vidname in ids:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if vidname in annotns['database']:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                duration = annotns['database'][vidname]['duration']\n",
    "                annot = annotns['database'][vidname]['annotations']\n",
    "                labels = []\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for segment_info in annot:\n",
    "                    interval = segment_info['segment']\n",
    "                    sent = segment_info['sentence']\n",
    "                    labels.append((interval,sent,duration))\n",
    "\n",
    "                label_info[vidname] = labels\n",
    "            else:\n",
    "                print(f\"label for {vidname} not present\")\n",
    "    return label_info\n",
    "\n",
    "def regress_labels(raw_labels):\n",
    "    regress_labels = {}\n",
    "    for key in raw_labels:\n",
    "        new_labels = []\n",
    "        for item in raw_labels[key]:\n",
    "            rng,sent,vidlen = item\n",
    "            mid = sum(rng)/2\n",
    "            duration = rng[-1]-rng[0]\n",
    "            mid_pred = (1/vidlen)*mid # location of mid-point w.r.t video length\n",
    "            duration_pred = (1/vidlen)*duration\n",
    "            new_labels.append(([mid_pred,duration_pred],sent))\n",
    "        regress_labels[key] = new_labels\n",
    "    return regress_labels\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "def get_labels(ids,annotns_file):\n",
    "\n",
    "    label_info = {}\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)\n",
    "        #print(annotns.keys())\n",
    "        for _,vidname in ids:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if vidname in annotns:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                duration = annotns[vidname]['duration']\n",
    "                annot = annotns[vidname]['annotations']\n",
    "                labels = []\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for segment_info in annot:\n",
    "                    interval = segment_info['segment']\n",
    "                    st_end = [interval[0],interval[-1]]\n",
    "                    sent = segment_info['sentence']\n",
    "                    labels.append((st_end,sent,duration))\n",
    "\n",
    "                label_info[vidname] = labels\n",
    "            else:\n",
    "                print(f\"label for {vidname} not present\")\n",
    "    return label_info\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #dataset\n",
    "# # Dataset/loader\n",
    "# # This is newer version\n",
    "# class YoucookDset2(Dataset):\n",
    "#     def __init__(self,data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\\\n",
    "#         ,split='train',use_precomp_emb=True,seqlen=26,framecnt=499,id=0):\n",
    "#         self.id = id\n",
    "#         self.feat_locs = {}\n",
    "#         self.split = split\n",
    "#         self.data_dir = data_dir\n",
    "#         self.use_precomp_emb = use_precomp_emb\n",
    "#         self.text_emb = None\n",
    "#         self.seqlen = seqlen\n",
    "#         self.framecnt = framecnt\n",
    "#         if self.split != 'test':\n",
    "#             self.annotns_file = data_dir+'annotations/segment_youcookii_annotations_trainval.json'\n",
    "#         else:\n",
    "#             raise NotImplementedError(f\"Split:{self.split},not yet correctly implemented\")\n",
    "#         if self.use_precomp_emb:\n",
    "#             self.txt_emb = joblib.load(os.path.join(self.data_dir,'emb.joblib'))\n",
    "#         #feat_locs = {'Ysh60eirChU': location of the video}\n",
    "#         self.feat_locs,vids = get_features(self.data_dir,split=self.split)\n",
    "#         assert len(vids) == len(self.feat_locs),\"features are wrong\"\n",
    "#         #import pdb;pdb.set_trace()\n",
    "#         label_info = get_labels(vids,self.annotns_file)\n",
    "#         #self.labelencoder = LabelEncoder2()\n",
    "#         self.final_labels = label_info\n",
    "#         #self.labelencoder.fit_transform(label_info)\n",
    "        \n",
    "#         #regress_labels(label_info)\n",
    "#         #(vid_id,seg_id)\n",
    "#         self.update_data()\n",
    "\n",
    "                \n",
    "            \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def update_data(self,id=None):\n",
    "#         self.data = []\n",
    "#         #self.vid_len = []\n",
    "#         if not id:\n",
    "#             id = self.id\n",
    "            \n",
    "#         starting_pnt = np.arange(id,self.framecnt,self.seqlen)\n",
    "\n",
    "#         for key in self.final_labels:\n",
    "#             annot_len = len(self.final_labels[key])\n",
    "#             if key in self.feat_locs:\n",
    "#                 file_loc = self.feat_locs[key]\n",
    "#                 #for stpnt in starting_pnt:\n",
    "#                 segments = list(zip(repeat(key,annot_len),repeat(file_loc,annot_len),\\\n",
    "#                         range(annot_len)))\n",
    "#                 for seg in segments:\n",
    "#                     for stpnt in starting_pnt:\n",
    "#                         if stpnt+self.seqlen<=self.framecnt:\n",
    "#                             datapnt = seg[:-1]+(stpnt,)+seg[-1:]\n",
    "#                             self.data.append(datapnt)\n",
    "                    \n",
    "#                 #self.data.extend(segments)\n",
    "#             else:\n",
    "#                 print(f\"video:{key} not found\")\n",
    "\n",
    "\n",
    "#     def getclass_prob(self,lbl_rng,frame_rng):\n",
    "#         lbl_ids = set(np.arange(lbl_rng[0],lbl_rng[-1]+1))\n",
    "#         frame_ids = set(np.arange(frame_rng[0],frame_rng[-1]+1))\n",
    "#         inter = lbl_ids.intersection(frame_ids)\n",
    "#         if len(inter) == 0:\n",
    "#             return 0.0\n",
    "#         else:\n",
    "#             return len(inter)/len(lbl_ids.union(frame_ids))\n",
    "\n",
    "#     def __getitem__(self,idx):\n",
    "#         if self.use_precomp_emb:\n",
    "#             vidname,file_loc,stid,seg_ind = self.data[idx]\n",
    "#             #import pdb;pdb.set_trace()\n",
    "#             #self.txt_emb[vidname][seg_ind],\n",
    "#             txt_info = self.final_labels[vidname][seg_ind]\n",
    "#             label_value = self.getclass_prob(txt_info[0],(stid,stid+self.seqlen-1))\n",
    "#             #import pdb;pdb.set_trace()\n",
    "#             return pd.read_csv(file_loc).values.astype(np.float32)[stid:stid+self.seqlen,:],(self.txt_emb[vidname][seg_ind]).astype(np.float32),\\\n",
    "#                 label_value\n",
    "#             #np.array(self.final_labels[vidname][seg_ind][0],dtype=np.float32)\n",
    "#         else:\n",
    "#             raise NotImplementedError(\"not yet correctly implemented\")\n",
    "\n",
    "        \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youcookdata = YoucookDset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youcookdata.final_labels['Ysh60eirChU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youcookdata.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youcookdata.feat_locs['Ysh60eirChU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import namedtuple\n",
    "# #Point = namedtuple(\"vid_id\", \"vid_loc\",\"start\",\"end\",\"segid\",\"label\")\n",
    "\n",
    "# def overlap_frac(base_rng,tst_rng):\n",
    "#     #1.Returns the fraction of frames that are overlapping in tst_rng with base_rng\n",
    "#     #2.both ends inclusive\n",
    "#     sz = tst_rng[-1]-tst_rng[0]+1\n",
    "#     lbl_ids = set(np.arange(base_rng[0],base_rng[-1]+1))\n",
    "#     frame_ids = set(np.arange(tst_rng[0],tst_rng[-1]+1))\n",
    "#     inter = frame_ids.intersection(lbl_ids)\n",
    "#     assert sz != 0,\"base frame rng is zero\"\n",
    "#     return len(inter)/sz\n",
    "    \n",
    "\n",
    "# data = []\n",
    "# max_cnt = 50\n",
    "# for key in youcookdata.final_labels:\n",
    "#     segments = youcookdata.final_labels[key]\n",
    "#     for ind,seg in enumerate(segments):\n",
    "#         #trn_points = []\n",
    "#         st_end,txt,vid_len = seg\n",
    "#         main_seg = (key,youcookdata.feat_locs[key],st_end[0],st_end[-1],ind,1.0)\n",
    "#         data.append(main_seg)\n",
    "#         frame_width = st_end[-1]-st_end[0] + 1\n",
    "#         extra_frames = []\n",
    "#         for cnt,new_st in enumerate(range(st_end[0]+1,st_end[-1]+1)):\n",
    "#             #forward sliding\n",
    "#             new_end = new_st+frame_width\n",
    "#             if (cnt<max_cnt)and (0<=new_st<youcookdata.framecnt and 0<=new_st<youcookdata.framecnt):\n",
    "#                 extra_frames.append((new_st,new_end))\n",
    "#         for cnt,new_end in enumerate(range(st_end[-1],st_end[0],-1)):\n",
    "#             #backward sliding\n",
    "#             new_st = new_end-frame_width\n",
    "#             if (cnt<max_cnt)and (0<=new_st<youcookdata.framecnt and 0<=new_st<youcookdata.framecnt):\n",
    "#                 extra_frames.append((new_st,new_end))\n",
    "#         #import pdb;pdb.set_trace()\n",
    "#         for ex_seg in extra_frames:\n",
    "#             label = overlap_frac(st_end,ex_seg)\n",
    "#             data.append((key,youcookdata.feat_locs[key],ex_seg[0],ex_seg[-1],ind,label))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all values are equally distributed\n",
    "#df\n",
    "\n",
    "\n",
    "#utils\n",
    "\n",
    "\n",
    "\n",
    "def get_labels(ids,annotns_file):\n",
    "\n",
    "    label_info = {}\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)\n",
    "        #print(annotns.keys())\n",
    "        for _,vidname in ids:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if vidname in annotns:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                duration = annotns[vidname]['duration']\n",
    "                annot = annotns[vidname]['annotations']\n",
    "                labels = []\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for segment_info in annot:\n",
    "                    interval = segment_info['segment']\n",
    "                    st_end = [interval[0],interval[-1]]\n",
    "                    sent = segment_info['sentence']\n",
    "                    labels.append((st_end,sent,duration))\n",
    "\n",
    "                label_info[vidname] = labels\n",
    "            else:\n",
    "                print(f\"label for {vidname} not present\")\n",
    "    return label_info\n",
    "\n",
    "def get_vids(base_dir,split):\n",
    "    trn_split = base_dir+split\n",
    "    trn_idlst = []\n",
    "    trn_vidlst = []\n",
    "\n",
    "    f = open(trn_split,'r')\n",
    "    for line in f:\n",
    "        id_,vid = line.split('/')\n",
    "        vid = vid.strip('\\n')\n",
    "        trn_idlst.append(id_)\n",
    "        trn_vidlst.append(vid)\n",
    "        #print(vid)\n",
    "        #break\n",
    "    f.close()\n",
    "    return trn_idlst,trn_vidlst\n",
    "\n",
    "    \n",
    "def get_features(data_dir,split='val',feat_dir='/common/users/vk405/feat_csv/'):\n",
    "    #feat_dir = data_dir\n",
    "    splits_dir = data_dir+'splits/'\n",
    "    if split == 'val':\n",
    "        feat_split_dir = feat_dir+'val_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'val_list.txt')  \n",
    "    elif split == 'train':\n",
    "        feat_split_dir = feat_dir+'train_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'train_list.txt') \n",
    "    elif split == 'test':\n",
    "        feat_split_dir = feat_dir+'test_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'test_list.txt')\n",
    "    else:\n",
    "        raise NotImplementedError(f'unknown split: {split}')     \n",
    "    feat_list = {}\n",
    "    vid_dtls = []\n",
    "    for num,name in zip(vid_num,vid_name):\n",
    "        feat_loc = os.path.join(feat_split_dir, f'{num}/{name}/0001/')\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if os.path.isdir(feat_loc):\n",
    "            feat_files = feat_loc + os.listdir(feat_loc)[0]\n",
    "            feat_list[name] = feat_files\n",
    "            #feat_list.append(feat_files)\n",
    "            vid_dtls.append((num,name))\n",
    "        else:\n",
    "            print(f\"video : {num}/{name} not found\")\n",
    "    assert len(feat_list) == len(vid_dtls),\"get-features is giving incorrect features\"\n",
    "    return feat_list,vid_dtls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_raw_labels(ids,annotns_file):\n",
    "\n",
    "    label_info = {}\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)\n",
    "        print(annotns.keys())\n",
    "        for _,vidname in ids:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if vidname in annotns['database']:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                duration = annotns['database'][vidname]['duration']\n",
    "                annot = annotns['database'][vidname]['annotations']\n",
    "                labels = []\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for segment_info in annot:\n",
    "                    interval = segment_info['segment']\n",
    "                    sent = segment_info['sentence']\n",
    "                    labels.append((interval,sent,duration))\n",
    "\n",
    "                label_info[vidname] = labels\n",
    "            else:\n",
    "                print(f\"label for {vidname} not present\")\n",
    "    return label_info\n",
    "\n",
    "def regress_labels(raw_labels):\n",
    "    regress_labels = {}\n",
    "    for key in raw_labels:\n",
    "        new_labels = []\n",
    "        for item in raw_labels[key]:\n",
    "            rng,sent,vidlen = item\n",
    "            mid = sum(rng)/2\n",
    "            duration = rng[-1]-rng[0]\n",
    "            mid_pred = (1/vidlen)*mid # location of mid-point w.r.t video length\n",
    "            duration_pred = (1/vidlen)*duration\n",
    "            new_labels.append(([mid_pred,duration_pred],sent))\n",
    "        regress_labels[key] = new_labels\n",
    "    return regress_labels\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#dataset\n",
    "# Dataset/loader\n",
    "# This is newer version\n",
    "class YoucookDset2(Dataset):\n",
    "    def __init__(self,data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\\\n",
    "        ,split='train',framecnt=499):\n",
    "        self.feat_locs = {}\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        self.framecnt = framecnt\n",
    "        #self.use_precomp_emb = use_precomp_emb\n",
    "        self.text_emb = None\n",
    "        if self.split != 'test':\n",
    "            self.annotns_file = data_dir+'annotations/segment_youcookii_annotations_trainval.json'\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Split:{self.split},not yet correctly implemented\")\n",
    "        # if self.use_precomp_emb:\n",
    "        #     self.txt_emb = joblib.load(os.path.join(self.data_dir,'emb.joblib'))\n",
    "        #feat_locs = {'Ysh60eirChU': location of the video}\n",
    "        self.feat_locs,vids = get_features(self.data_dir,split=self.split)\n",
    "        assert len(vids) == len(self.feat_locs),\"features are wrong\"\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #label_info = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder = LabelEncoder2()\n",
    "        self.final_labels = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder.fit_transform(label_info)\n",
    "        \n",
    "        #regress_labels(label_info)\n",
    "        #(vid_id,seg_id)\n",
    "        self.data = self.update_data()\n",
    "\n",
    "                \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def overlap_frac(self,base_rng,tst_rng):\n",
    "        #1.Returns the fraction of frames that are overlapping in tst_rng with base_rng\n",
    "        #2.both ends inclusive\n",
    "        sz = tst_rng[-1]-tst_rng[0]+1\n",
    "        lbl_ids = set(np.arange(base_rng[0],base_rng[-1]+1))\n",
    "        frame_ids = set(np.arange(tst_rng[0],tst_rng[-1]+1))\n",
    "        inter = frame_ids.intersection(lbl_ids)\n",
    "        assert sz != 0,\"base frame rng is zero\"\n",
    "        return len(inter)/sz\n",
    "\n",
    "\n",
    "    def update_data(self):\n",
    "        data = []\n",
    "        max_cnt = 50\n",
    "        for key in self.final_labels:\n",
    "            segments = self.final_labels[key]\n",
    "            for ind,seg in enumerate(segments):\n",
    "                #trn_points = []\n",
    "                st_end,txt,vid_len = seg\n",
    "                main_seg = (key,self.feat_locs[key],st_end[0],st_end[-1],ind,1.0)\n",
    "                data.append(main_seg)\n",
    "                frame_width = st_end[-1]-st_end[0] + 1\n",
    "                extra_frames = []\n",
    "                for cnt,new_st in enumerate(range(st_end[0]+1,st_end[-1]+1)):\n",
    "                    #forward sliding\n",
    "                    new_end = new_st+frame_width\n",
    "                    if (cnt<max_cnt)and (0<=new_st<self.framecnt and 0<=new_st<self.framecnt):\n",
    "                        extra_frames.append((new_st,new_end))\n",
    "                for cnt,new_end in enumerate(range(st_end[-1],st_end[0],-1)):\n",
    "                    #backward sliding\n",
    "                    new_st = new_end-frame_width\n",
    "                    if (cnt<max_cnt)and (0<=new_st<self.framecnt and 0<=new_st<self.framecnt):\n",
    "                        extra_frames.append((new_st,new_end))\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for ex_seg in extra_frames:\n",
    "                    label = self.overlap_frac(st_end,ex_seg)\n",
    "                    data.append((key,self.feat_locs[key],ex_seg[0],ex_seg[-1],ind,label))\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]\n",
    "        \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata = YoucookDset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset,TensorDataset\n",
    "# inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "# tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "# dataset = TensorDataset(inps, tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ToyDataset(Dataset):\n",
    "#     def __init__(self):\n",
    "#         self.data = ['h','l','e','o']\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "#     def __getitem__(self,idx):\n",
    "#         return self.data[idx]\n",
    "\n",
    "# dataset = ToyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\n",
    "# global_txt = joblib.load(os.path.join(data_dir,'emb.joblib'))\n",
    "def collate_wrapper(data):\n",
    "    labels = []\n",
    "    vid_embs = []\n",
    "    txt_embs = []\n",
    "    #namedtuple(\"vid_id\", \"vid_loc\",\"start\",\"end\",\"segid\",\"label\")\n",
    "    batched_data = pd.DataFrame(data,columns=[\"vid_id\", \"vid_loc\",\"start\",\"end\",\"segid\",\"label\"])\n",
    "    unique_locs = batched_data['vid_loc'].unique()\n",
    "    for loc in unique_locs:\n",
    "        locwise = batched_data[batched_data['vid_loc']==loc]\n",
    "        tot_vid = pd.read_csv(loc).values\n",
    "        txtemb = None\n",
    "        for ind,ele in locwise.iterrows():\n",
    "            vid_id,_,st,end,segid,label = ele\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if not txtemb:\n",
    "                txtemb = global_txt[ele['vid_id']]\n",
    "            vid_embs.append(torch.tensor(tot_vid[ele['start']:ele['end']+1]))\n",
    "            txt_embs.append(torch.tensor(txtemb[ele['segid']]))\n",
    "            labels.append(torch.tensor(ele['label']))\n",
    "    #return (np.stack(vid_embs),np.stack(txt_embs)),np.stack(labels)\n",
    "    return vid_embs,torch.stack(txt_embs),torch.stack(labels)\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get a dataloader\n",
    "# ydata = YoucookDset2()\n",
    "# dl_new = DataLoader(ydata,batch_size=1000,shuffle=True,collate_fn=collate_wrapper)\n",
    "# batch = next(iter(dl_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now changing the model to account for this change\n",
    "\n",
    "\n",
    "#model utils\n",
    "\n",
    "#!pip install transformers\n",
    "\n",
    "def init_parameters_xavier_uniform(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "def scaled_dot(query, key, mask_key=None):  \n",
    "    score = torch.matmul(query, key.transpose(-2, -1))\n",
    "    score /= math.sqrt(query.size(-1))\n",
    "    if mask_key is not None:\n",
    "        score = score.masked_fill(mask_key, -1e18)  # Represents negative infinity\n",
    "    return score      \n",
    "            \n",
    "def attend(query, key, value, mask_key=None, dropout=None):\n",
    "    # TODO: Implement\n",
    "    # Use scaled_dot, be sure to mask key\n",
    "    #smax = nn.Softmax(-1)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    score = scaled_dot(query,key,mask_key)  \n",
    "    attention = F.softmax(score,dim=-1)\n",
    "    if dropout is not None:#do = nn.Dropout(dropout)\n",
    "        attention = dropout(attention)\n",
    "    answer = torch.matmul(attention,value) \n",
    "    # Convexly combine value embeddings using attention, this should be just a matrix-matrix multiplication.\n",
    "    return answer, attention\n",
    "\n",
    "\n",
    "\n",
    "def split_heads(batch, num_heads):  \n",
    "    (batch_size, length, dim) = batch.size()  # These are the expected batch dimensions.\n",
    "    assert dim % num_heads == 0  # Assert that dimension is divisible by the number of heads.\n",
    "    dim_head = dim // num_heads\n",
    "\n",
    "    # No new memory allocation\n",
    "    splitted = batch.view(batch_size, -1, num_heads, dim_head).transpose(1, 2)  \n",
    "    return splitted  # (batch_size, num_heads, length, dim_head), note that now the last two dimensions are compatible with our attention functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_heads(batch):  \n",
    "    (batch_size, num_heads, length, dim_head) = batch.size()  # These are the expected batch dimensions.\n",
    "\n",
    "    # New memory allocation (reshape), can't avoid.\n",
    "    merged = batch.transpose(1, 2).reshape(batch_size, -1, num_heads * dim_head)\n",
    "    return merged  # (batch_size, length, dim)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "\n",
    "        self.linear_query = nn.Linear(dim, dim)\n",
    "        self.linear_key = nn.Linear(dim, dim)\n",
    "        self.linear_value = nn.Linear(dim, dim)\n",
    "        self.linear_final = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, query, key, value, mask_key=None, layer_cache=None,\n",
    "              memory_attention=False):\n",
    "        \"\"\"\n",
    "        INPUT\n",
    "          query: (batch_size, length_query, dim)\n",
    "          key: (batch_size, length_key, dim)\n",
    "          value: (batch_size, length_key, dim_value)\n",
    "          mask_key: (*, 1, length_key) if queries share the same mask, else\n",
    "                    (*, length_query, length_key)\n",
    "          layer_cache: if not None, stepwise decoding (cache of key/value)\n",
    "          memory_attention: doing memory attention in stepwise decoding?\n",
    "        OUTPUT\n",
    "          answer: (batch_size, length_query, dim_value)\n",
    "          attention: (batch_size, num_heads, length_query, length_key) else\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.linear_query(query)\n",
    "        query = split_heads(query, self.num_heads)  # (batch_size, num_heads, -1, dim_head)\n",
    "\n",
    "        def process_key_value(key, value):  # Only called when necessary.\n",
    "            key = self.linear_key(key)\n",
    "            key = split_heads(key, self.num_heads)\n",
    "            value = self.linear_value(value)\n",
    "            value = split_heads(value, self.num_heads)\n",
    "            return key, value\n",
    "\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if layer_cache is None:\n",
    "            key, value = process_key_value(key, value)\n",
    "        else:\n",
    "            assert query.size(2) == 1  # Stepwise decoding\n",
    "            \n",
    "            if memory_attention:\n",
    "                if layer_cache['memory_key'] is None:  # One-time calculation\n",
    "                    key, value = process_key_value(key, value)\n",
    "                    # (batch_size, num_heads, length_memory, dim)\n",
    "                    layer_cache['memory_key'] = key\n",
    "                    layer_cache['memory_value'] = value\n",
    "\n",
    "                key = layer_cache['memory_key']\n",
    "                value = layer_cache['memory_value']\n",
    "\n",
    "            else:  # Self-attention during decoding\n",
    "                key, value = process_key_value(key, value)\n",
    "                assert key.size(2) == 1 and value.size(2) == 1\n",
    "                \n",
    "                # Append to previous.\n",
    "                if layer_cache['self_key'] is not None:\n",
    "                    key = torch.cat((layer_cache['self_key'], key), dim=2)\n",
    "                    value = torch.cat((layer_cache['self_value'], value), dim=2)\n",
    "                    \n",
    "                 # (batch_size, num_heads, length_decoded, dim)\n",
    "                layer_cache['self_key'] = key  # Recache.\n",
    "                layer_cache['self_value'] = value\n",
    "        # Because we've splitted embeddings into heads, we must also split the mask. \n",
    "        # And because each query uses the same mask for all heads (we don't use different masking for different heads), \n",
    "        # we can specify length 1 for the head dimension.\n",
    "        if mask_key is not None:  \n",
    "            mask_key = mask_key.unsqueeze(1)  # (batch_size, 1, -1, length_key)\n",
    "\n",
    "        answer, attention = attend(query, key, value, mask_key, self.dropout)\n",
    "\n",
    "        answer = merge_heads(answer)  # (batch_size, length_key, dim)\n",
    "        answer = self.linear_final(answer)\n",
    "\n",
    "        return answer, attention\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_hidden, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, dim_hidden)\n",
    "        self.w2 = nn.Linear(dim_hidden, dim)\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.drop1 = nn.Dropout(drop_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(drop_rate)\n",
    "    def forward(self, x):\n",
    "        inter = self.drop1(self.relu(self.w1(self.layer_norm(x))))\n",
    "        output = self.drop2(self.w2(inter))\n",
    "        return output + x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SinusoidalPositioner(nn.Module):\n",
    "    def __init__(self, dim, drop_rate=0.1, length_max=5000):\n",
    "        super().__init__()\n",
    "        frequency = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.) / dim))  # Using different frequency for each dim\n",
    "        positions = torch.arange(0, length_max).unsqueeze(1)\n",
    "        wave = torch.zeros(length_max, dim)\n",
    "        wave[:, 0::2] = torch.sin(frequency * positions)\n",
    "        wave[:, 1::2] = torch.cos(frequency * positions)\n",
    "        self.register_buffer('wave', wave.unsqueeze(0))  # (1, length_max, dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.dim = dim\n",
    "        self.length_max = length_max\n",
    "    def forward(self, x, step=-1):\n",
    "        assert x.size(-2) <= self.length_max\n",
    "\n",
    "        if step < 0:  # Take the corresponding leftmost embeddings.\n",
    "            position_encoding = self.wave[:, :x.size(-2), :]\n",
    "        else:  # Take the embedding at the step.\n",
    "            position_encoding = self.wave[:, step, :]\n",
    "\n",
    "        x = x * math.sqrt(self.dim)\n",
    "        return self.dropout(x + position_encoding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self,dim,num_heads,dim_hidden,drop_rate):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.context_attention = MultiHeadAttention(dim, num_heads, drop_rate)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "        self.feedforward = PositionwiseFeedForward(dim, dim_hidden, drop_rate)\n",
    "        \n",
    "    def forward(self,target,memory,layer_cache=None,mask_key=None):\n",
    "        \n",
    "        cross_attn_target = self.layer_norm(target)\n",
    "        attended, attention = self.context_attention(cross_attn_target,memory,memory,layer_cache=layer_cache,memory_attention=True,mask_key=mask_key)\n",
    "        \n",
    "        attended = target + self.drop(attended)\n",
    "        \n",
    "        return self.feedforward(attended),attention\n",
    "\n",
    "\n",
    "\n",
    "layer_cache = {'memory_key': None, 'memory_value': None, 'self_key': None, 'self_value': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CrossattnModel(pl.LightningModule):\n",
    "    def __init__(self,hparams,dset=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        #self.hparams = hparams\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #self.net= Model(hparams)\n",
    "        #self.hparams  = hparams\n",
    "        \n",
    "        self.positioner = SinusoidalPositioner(self.hparams.edim, drop_rate=0., length_max=1000)\n",
    "        self.attn = CrossAttentionLayer(self.hparams.edim,self.hparams.nheads,\\\n",
    "                           self.hparams.attnhdim,self.hparams.dropoutp)\n",
    "        self.wrdcnn =  nn.Conv1d(self.hparams.wrdim, self.hparams.edim, 1, stride=1)\n",
    "        self.vidcnn =  nn.Conv1d(self.hparams.vidim, self.hparams.edim, 1, stride=1)\n",
    "        self.hid_layer = nn.Linear(self.hparams.edim,self.hparams.hdim)\n",
    "        self.out_layer = nn.Linear(self.hparams.hdim,1)\n",
    "        #self\n",
    "        self.init_parameters_xavier_uniform()\n",
    "        self.dset = dset\n",
    "\n",
    "    def forward(self,x):\n",
    "        #keep this for inference\n",
    "        vid_feat,wrd_feat = x\n",
    "        mask = self.get_mask(vid_feat)\n",
    "        # pad the features with zeros\n",
    "        vid_feat = pad_sequence(vid_feat,batch_first=True)\n",
    "        out = self.net((vid_feat.float(),wrd_feat.float()),mask_key=mask)\n",
    "        return out\n",
    "        \n",
    "    def net(self,x,mask_key=None):\n",
    "        vid_x,wrd_x = x\n",
    "        \n",
    "        wrd_x = wrd_x.unsqueeze(1).transpose(1,2)\n",
    "        vid_x = vid_x.transpose(1,2)\n",
    "        #print(f\"inside model, wrd_x:{wrd_x.shape},vi\")\n",
    "        tgt = self.wrdcnn(wrd_x.float()).transpose(1,2)\n",
    "        src = self.vidcnn(vid_x.float()).transpose(1,2)\n",
    "        src_posencode = self.positioner(src)\n",
    "        #for i in range(self.hparams.lyrs):\n",
    "        # ?create mask_key and send it\n",
    "        attended,attn_score = self.attn(tgt,src_posencode,mask_key=mask_key)\n",
    "            #tgt = \n",
    "        out = F.sigmoid(self.out_layer(F.relu(self.hid_layer(F.relu(attended)))))\n",
    "        return out\n",
    "\n",
    "    def get_mask(self,batch):\n",
    "        #device = batch.device\n",
    "        sq_lens = list(map(lambda x:x.size(0),batch))\n",
    "        ln_key = batch[0].size(-1)\n",
    "        # mask = (batch_size, 1, length_key)(all queries have same mask)\n",
    "        mask = torch.ones(len(sq_lens),1,max(sq_lens))\n",
    "        for ind,ele in enumerate(sq_lens):\n",
    "            mask[ind,:,:ele] = 0.0\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        #for tranining\n",
    "        vid_feat,wrd_feat,labels = batch\n",
    "        # here vid_feat is list of video frames of varying length\n",
    "        mask = self.get_mask(vid_feat)\n",
    "        # pad the features with zeros\n",
    "        vid_feat = pad_sequence(vid_feat,batch_first=True)\n",
    "        x_hat = self.net((vid_feat.float(),wrd_feat.float()),mask_key=mask)\n",
    "        if x_hat.shape[0] != self.hparams.batch_size:\n",
    "            print(f\"*****batch size mismatch: running bs:{x_hat.shape[0]}*****\")\n",
    "            #import pdb;pdb.set_trace()\n",
    "        #loss = nn.BCELoss()\n",
    "        loss = F.binary_cross_entropy(x_hat.squeeze().float(), labels.squeeze().float())\n",
    "        #print(f\"inside train step, loss:{loss}\")\n",
    "        self.log(\"train_loss\",loss,on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        #for validation\n",
    "        vid_feat,wrd_feat,labels = batch\n",
    "        mask = self.get_mask(vid_feat)\n",
    "        vid_feat = pad_sequence(vid_feat,batch_first=True)\n",
    "        x_hat = self.net((vid_feat.float(),wrd_feat.float()),mask_key=mask)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(x_hat.squeeze().float(), labels.squeeze().float())\n",
    "        #print(f\"inside train step, loss:{loss}\")\n",
    "        self.log(\"val_loss\",loss,on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #lr = self.hparams.lr if 'lr' in self.hparams else 1e-3\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "        # Apply lr scheduler per step\n",
    "        lr_scheduler = CosineWarmupScheduler(optimizer,\n",
    "                                             warmup=self.hparams.warmup,\n",
    "                                             max_iters=self.hparams.max_iters)\n",
    "        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
    "\n",
    "    \n",
    "    def init_parameters_xavier_uniform(self):\n",
    "        for name,p in self.named_parameters():\n",
    "            if 'attn' in name and p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "youcookdata = YoucookDset2()\n",
    "train_loader = DataLoader(youcookdata,\\\n",
    "            batch_size=cfg.batch_size,collate_fn=collate_w,\\\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_feat,wrd_feat,labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 512])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_feat[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_feat = pad_sequence(vid_feat,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 223, 512])"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 768])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrd_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d = nn.Conv1d(512, 256, 2, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=512,hidden_size=100,num_layers=2,batch_first=True,bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (hn, cn) = lstm(vid_feat.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 223, 100]),\n",
       " torch.Size([2, 512, 100]),\n",
       " torch.Size([2, 512, 100]))"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape,hn.shape,cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_feat,wrd_feat = x\n",
    "        mask = self.get_mask(vid_feat)\n",
    "        # pad the features with zeros\n",
    "        vid_feat = pad_sequence(vid_feat,batch_first=True)\n",
    "        out = self.net((vid_feat.float(),wrd_feat.float()),mask_key=mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ydata = YoucookDset2()\n",
    "# dl_new = DataLoader(ydata,batch_size=10,shuffle=True,collate_fn=collate_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = torch.tensor([1.0,1.0])\n",
    "# target = torch.tensor([0.0,0.8])\n",
    "# F.binary_cross_entropy(input,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4096 works\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "wandb_logger = lambda dir, version: WandbLogger(\n",
    "    name=\"wandb\", save_dir=dir, version=version\n",
    ")\n",
    "csvlogger = lambda dir, version: CSVLogger(dir, name=\"csvlogs\", version=version)\n",
    "tblogger = lambda dir, version: TensorBoardLogger(dir, name=\"tblogs\", version=version)\n",
    "\n",
    "def get_loggers(dir,version,lis=[\"csv\"]):\n",
    "    lgrs = []\n",
    "    if \"wandb\" in lis:\n",
    "        lgrs.append(wandb_logger(dir, version))\n",
    "    if \"csv\" in lis:\n",
    "        lgrs.append(csvlogger(dir, version))\n",
    "    if \"tb\" in lis:\n",
    "        lgrs.append(tblogger(dir, version))\n",
    "    return lgrs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#dataset\n",
    "# Dataset/loader\n",
    "# This is newer version\n",
    "class YoucookDset2(Dataset):\n",
    "    global_txt = joblib.load(os.path.join('/common/home/vk405/Projects/Crossmdl/Data/YouCookII/','emb.joblib'))\n",
    "    global_imgfeats = joblib.load('/common/users/vk405/feat_csv/global_imgfeats.joblib')\n",
    "    global_map = joblib.load('/common/users/vk405/feat_csv/global_map.joblib')\n",
    "    def __init__(self,data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\\\n",
    "        ,split='train',framecnt=499):\n",
    "        self.feat_locs = {}\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        self.framecnt = framecnt\n",
    "        #self.use_precomp_emb = use_precomp_emb\n",
    "        self.text_emb = None\n",
    "        if self.split != 'test':\n",
    "            self.annotns_file = data_dir+'annotations/segment_youcookii_annotations_trainval.json'\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Split:{self.split},not yet correctly implemented\")\n",
    "        # if self.use_precomp_emb:\n",
    "        #     self.txt_emb = joblib.load(os.path.join(self.data_dir,'emb.joblib'))\n",
    "        #feat_locs = {'Ysh60eirChU': location of the video}\n",
    "        self.feat_locs,vids = get_features(self.data_dir,split=self.split)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        assert len(vids) == len(self.feat_locs),\"features are wrong\"\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #label_info = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder = LabelEncoder2()\n",
    "        self.final_labels = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder.fit_transform(label_info)\n",
    "        \n",
    "        #regress_labels(label_info)\n",
    "        #(vid_id,seg_id)\n",
    "        self.data = self.update_data()\n",
    "\n",
    "                \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def overlap_frac(self,base_rng,tst_rng):\n",
    "        #1.Returns the fraction of frames that are overlapping in tst_rng with base_rng\n",
    "        #2.both ends inclusive\n",
    "        sz = tst_rng[-1]-tst_rng[0]+1\n",
    "        lbl_ids = set(np.arange(base_rng[0],base_rng[-1]+1))\n",
    "        frame_ids = set(np.arange(tst_rng[0],tst_rng[-1]+1))\n",
    "        inter = frame_ids.intersection(lbl_ids)\n",
    "        assert sz != 0,\"base frame rng is zero\"\n",
    "        return len(inter)/sz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_data(self):\n",
    "        data = []\n",
    "        max_cnt = 50\n",
    "        for key in self.final_labels:\n",
    "            segments = self.final_labels[key]\n",
    "            for ind,seg in enumerate(segments):\n",
    "                #trn_points = []\n",
    "                st_end,txt,vid_len = seg\n",
    "                main_seg = (key,self.feat_locs[key],st_end[0],st_end[-1],ind,1.0)\n",
    "                data.append(main_seg)\n",
    "                frame_width = st_end[-1]-st_end[0] + 1\n",
    "                extra_frames = []\n",
    "                #st_end[-1]+1\n",
    "                for cnt,new_st in enumerate(range(st_end[0]+1,499)):\n",
    "                    #forward sliding\n",
    "                    new_end = new_st+frame_width\n",
    "                    if (cnt<max_cnt)and (0<=new_st<self.framecnt and 0<=new_st<self.framecnt):\n",
    "                        extra_frames.append((new_st,new_end))\n",
    "                #st_end[0]\n",
    "                for cnt,new_end in enumerate(range(st_end[-1],0,-1)):\n",
    "                    #backward sliding\n",
    "                    new_st = new_end-frame_width\n",
    "                    if (cnt<max_cnt)and (0<=new_st<self.framecnt and 0<=new_st<self.framecnt):\n",
    "                        extra_frames.append((new_st,new_end))\n",
    "                \n",
    "                #import pdb;pdb.set_trace()\n",
    "                zero_overlap_frames = []\n",
    "                for ex_seg in extra_frames:\n",
    "                    label = self.overlap_frac(st_end,ex_seg)\n",
    "                    # only negative samples\n",
    "                    if label == 0.00:\n",
    "                        zero_overlap_frames.append(ex_seg)\n",
    "                        #data.append((key,self.feat_locs[key],ex_seg[0],ex_seg[-1],ind,label))\n",
    "                        #break\n",
    "                if len(zero_overlap_frames):\n",
    "                    frame_id = np.random.randint(len(zero_overlap_frames))\n",
    "                    ex_seg = zero_overlap_frames[frame_id]\n",
    "                    #print(f\"main_seg:{main_seg},ex_seg:{ex_seg}\")\n",
    "                    data.append((key,self.feat_locs[key],ex_seg[0],ex_seg[-1],ind,0.0))\n",
    "                    \n",
    "        return data\n",
    "    \n",
    "    def read_data_global(self,data):\n",
    "        vid_id,vid_loc,start,end,segid,label = data\n",
    "        ind = self.global_map[vid_id]\n",
    "        img_feat = self.global_imgfeats[ind][start:end+1]\n",
    "        txt_feat = self.global_txt[vid_id][segid]\n",
    "        return img_feat,txt_feat,label\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.read_data_global(self.data[idx])\n",
    "        \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ydata = YoucookDset2()\n",
    "# out = ydata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#dataset\n",
    "# Dataset/loader\n",
    "# This is newer version\n",
    "class YoucookDset2(Dataset):\n",
    "    global_txt = joblib.load(os.path.join('/common/home/vk405/Projects/Crossmdl/Data/YouCookII/','emb.joblib'))\n",
    "    global_imgfeats = joblib.load('/common/users/vk405/feat_csv/global_imgfeats.joblib')\n",
    "    global_map = joblib.load('/common/users/vk405/feat_csv/global_map.joblib')\n",
    "    def __init__(self,data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\\\n",
    "        ,split='train',framecnt=499):\n",
    "        self.feat_locs = {}\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        self.framecnt = framecnt\n",
    "        #self.use_precomp_emb = use_precomp_emb\n",
    "        self.text_emb = None\n",
    "        if self.split != 'test':\n",
    "            self.annotns_file = data_dir+'annotations/segment_youcookii_annotations_trainval.json'\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Split:{self.split},not yet correctly implemented\")\n",
    "        # if self.use_precomp_emb:\n",
    "        #     self.txt_emb = joblib.load(os.path.join(self.data_dir,'emb.joblib'))\n",
    "        #feat_locs = {'Ysh60eirChU': location of the video}\n",
    "        #import pdb;pdb.set_trace()\n",
    "        self.feat_locs,vids = get_features(self.data_dir,split=self.split)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        assert len(vids) == len(self.feat_locs),\"features are wrong\"\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #label_info = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder = LabelEncoder2()\n",
    "        self.final_labels = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder.fit_transform(label_info)\n",
    "        \n",
    "        #regress_labels(label_info)\n",
    "        #(vid_id,seg_id)\n",
    "        self.data,self.max_seq_ln= self.update_data()\n",
    "\n",
    "                \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def overlap_frac(self,base_rng,tst_rng):\n",
    "        #1.Returns the fraction of frames that are overlapping in tst_rng with base_rng\n",
    "        #2.both ends inclusive\n",
    "        sz = tst_rng[-1]-tst_rng[0]+1\n",
    "        lbl_ids = set(np.arange(base_rng[0],base_rng[-1]+1))\n",
    "        frame_ids = set(np.arange(tst_rng[0],tst_rng[-1]+1))\n",
    "        inter = frame_ids.intersection(lbl_ids)\n",
    "        assert sz != 0,\"base frame rng is zero\"\n",
    "        return len(inter)/sz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_data(self):\n",
    "        data = []\n",
    "        max_cnt = 50\n",
    "        mx_seq_ln = 0\n",
    "        for key in self.final_labels:\n",
    "            segments = self.final_labels[key]\n",
    "            for ind,seg in enumerate(segments):\n",
    "                #trn_points = []\n",
    "                st_end,txt,vid_len = seg\n",
    "                main_seg = (key,self.feat_locs[key],st_end[0],st_end[-1],ind,1.0)\n",
    "                # to keep track of max sequence length in the data\n",
    "                sq_len = st_end[-1]-st_end[0]+1\n",
    "                mx_seq_ln = max(mx_seq_ln,sq_len)\n",
    "\n",
    "                data.append(main_seg)\n",
    "                frame_width = st_end[-1]-st_end[0] + 1\n",
    "                extra_frames = []\n",
    "                #st_end[-1]+1\n",
    "                for cnt,new_st in enumerate(range(st_end[0]+1,499)):\n",
    "                    #forward sliding\n",
    "                    new_end = new_st+frame_width\n",
    "                    if (cnt<max_cnt)and (0<=new_st<self.framecnt and 0<=new_st<self.framecnt):\n",
    "                        extra_frames.append((new_st,new_end))\n",
    "                #st_end[0]\n",
    "                for cnt,new_end in enumerate(range(st_end[-1],0,-1)):\n",
    "                    #backward sliding\n",
    "                    new_st = new_end-frame_width\n",
    "                    if (cnt<max_cnt)and (0<=new_st<self.framecnt and 0<=new_st<self.framecnt):\n",
    "                        extra_frames.append((new_st,new_end))\n",
    "                \n",
    "                #import pdb;pdb.set_trace()\n",
    "                zero_overlap_frames = []\n",
    "                for ex_seg in extra_frames:\n",
    "                    label = self.overlap_frac(st_end,ex_seg)\n",
    "                    # only negative samples\n",
    "                    if label == 0.00:\n",
    "                        zero_overlap_frames.append(ex_seg)\n",
    "                        #data.append((key,self.feat_locs[key],ex_seg[0],ex_seg[-1],ind,label))\n",
    "                        #break\n",
    "                if len(zero_overlap_frames):\n",
    "                    frame_id = np.random.randint(len(zero_overlap_frames))\n",
    "                    ex_seg = zero_overlap_frames[frame_id]\n",
    "                    #print(f\"main_seg:{main_seg},ex_seg:{ex_seg}\")\n",
    "                    data.append((key,self.feat_locs[key],ex_seg[0],ex_seg[-1],ind,0.0))\n",
    "                    sq_len = ex_seg[-1]-ex_seg[0]+1\n",
    "                    mx_seq_ln = max(mx_seq_ln,sq_len)\n",
    "                    \n",
    "        return data,mx_seq_ln\n",
    "    \n",
    "    def read_data_global(self,data):\n",
    "        vid_id,vid_loc,start,end,segid,label = data\n",
    "        ind = self.global_map[vid_id]\n",
    "        img_feat = self.global_imgfeats[ind][start:end+1]\n",
    "        txt_feat = self.global_txt[vid_id][segid]\n",
    "        pad_img = np.concatenate([img_feat,np.zeros((self.max_seq_ln-img_feat.shape[0],img_feat.shape[-1]))])\n",
    "        return pad_img,txt_feat,len(img_feat),label\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.read_data_global(self.data[idx])\n",
    "        \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ydata = YoucookDset2()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#youcookdata.final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_w(data):\n",
    "    img_lis = []\n",
    "    txt_lis = []\n",
    "    label_lis = []\n",
    "    for ele in data:\n",
    "        img,txt,label = ele\n",
    "        img_lis.append(torch.tensor(img))\n",
    "        txt_lis.append(torch.tensor(txt))\n",
    "        label_lis.append(torch.tensor(label))\n",
    "    return img_lis,torch.stack(txt_lis),torch.stack(label_lis)\n",
    "        \n",
    "    \n",
    "\n",
    "#train_loader = DataLoader(ydata,\\\n",
    "            #batch_size=100,collate_fn=collate_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnin/training\n",
    "\n",
    "data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\n",
    "\n",
    "class MyCollator(object):\n",
    "    def __init__(self,**kwargs):\n",
    "        self.kwargs = kwargs\n",
    "    def __call__(self, data):\n",
    "        # do something with batch and self.params\n",
    "        labels = []\n",
    "        vid_embs = []\n",
    "        txt_embs = []\n",
    "        #namedtuple(\"vid_id\", \"vid_loc\",\"start\",\"end\",\"segid\",\"label\")\n",
    "        batched_data = pd.DataFrame(data,columns=[\"vid_id\", \"vid_loc\",\"start\",\"end\",\"segid\",\"label\"])\n",
    "        unique_locs = batched_data['vid_loc'].unique()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        for loc in unique_locs:\n",
    "            locwise = batched_data[batched_data['vid_loc']==loc]\n",
    "            tot_vid = pd.read_csv(loc).values\n",
    "            txtemb = None\n",
    "            for ind,ele in locwise.iterrows():\n",
    "                #vid_id,_,st,end,segid,label = ele\n",
    "                #import pdb;pdb.set_trace()\n",
    "                if not txtemb:\n",
    "                    txtemb = self.kwargs['txtemb'][ele['vid_id']]\n",
    "                vid_embs.append(torch.tensor(tot_vid[ele['start']:ele['end']+1]))\n",
    "                #import pdb;pdb.set_trace()\n",
    "                txt_embs.append(torch.tensor(txtemb[ele['segid']]))\n",
    "                labels.append(torch.tensor(ele['label']))\n",
    "        #return (np.stack(vid_embs),np.stack(txt_embs)),np.stack(labels)\n",
    "        return vid_embs,torch.stack(txt_embs),torch.stack(labels)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from numpy import genfromtxt\n",
    "# global_imgfeats = []\n",
    "# global_map = {}\n",
    "# i = 0\n",
    "# for ele in tqdm(ydata):\n",
    "#     img_id,loc,_,_,_,_ = ele\n",
    "#     if img_id not in global_map:\n",
    "#         global_map[img_id] = i\n",
    "#         data = genfromtxt(loc,delimiter=',')\n",
    "#         global_imgfeats.append(data)\n",
    "#         i += 1\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# store_p = '/common/users/vk405/feat_csv/'\n",
    "# _ = joblib.dump(global_map,store_p+'global_map.joblib')\n",
    "# _ = joblib.dump(global_imgfeats,store_p+'global_imgfeats.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_txt = joblib.load(os.path.join(data_dir,'emb.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_txt = joblib.load(os.path.join(data_dir,'emb.joblib'))\n",
    "# collator = MyCollator(txtemb=global_txt)\n",
    "# dl_new = DataLoader(ydata,batch_size=10,shuffle=True,collate_fn=collator)\n",
    "# batch = next(iter(dl_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cfg):\n",
    "    pl.seed_everything(cfg.seed)\n",
    "    dir = cfg.artifacts_loc\n",
    "    version = str(cfg.version)\n",
    "    logger_list = get_loggers(dir, version,cfg.loggers)\n",
    "    cbs = []\n",
    "    if \"early_stop\" in cfg.cbs:\n",
    "        #? does'nt really work atm\n",
    "        params = cfg.model.cbs.early_stop\n",
    "        earlystopcb = EarlyStopping(**params, min_delta=0.00, verbose=False)\n",
    "        cbs.append(earlystopcb)\n",
    "    if \"checkpoint\" in cfg.cbs:\n",
    "        store_path = dir + \"ckpts/\" + str(cfg.version) + \"/\"\n",
    "        isExist = os.path.exists(store_path)\n",
    "        if isExist and os.path.isdir(store_path) and ('retrain' not in cfg and not cfg.retrain):\n",
    "            shutil.rmtree(store_path)\n",
    "        # then create fresh\n",
    "        if not isExist:\n",
    "            os.makedirs(store_path)\n",
    "        fname = \"{epoch}-{train_loss:.2f}\"\n",
    "        params = cfg.checkpoint\n",
    "        checkptcb = ModelCheckpoint(**params, dirpath=store_path, filename=fname)\n",
    "        cbs.append(checkptcb)\n",
    "    if \"wandb\" in cfg.loggers:\n",
    "        wandb.init(project=\"videoretrieval\", config=cfg)\n",
    "    if cfg.mode == 'train':\n",
    "        split = cfg.data_split if 'data_split' in cfg else cfg.mode\n",
    "        youcookdata = YoucookDset2(data_dir=cfg.data_dir,split=split)\n",
    "        if cfg.use_precomp_emb:\n",
    "            pass\n",
    "            #global_txt = joblib.load(os.path.join(data_dir,'emb.joblib'))\n",
    "            #collate_wrapper = MyCollator(txtemb=global_txt)\n",
    "        #pin_memory=True\n",
    "        train_loader = DataLoader(youcookdata,\\\n",
    "            batch_size=cfg.batch_size,collate_fn=collate_w,\\\n",
    "                shuffle=True)\n",
    "           \n",
    "        net = CrossattnModel(cfg)\n",
    "        #gpus=1,gpus=3,accelerator='ddp'\n",
    "        if 'retrain' in cfg and cfg.retrain:\n",
    "            path = cfg.artifacts_loc+'ckpts/'+cfg.version +'/'\n",
    "            weight = os.listdir(path)[-1]\n",
    "            ck_path = path+weight\n",
    "            print(f\"loading from:{ck_path}\")\n",
    "            net = net.load_from_checkpoint(ck_path)\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger_list,callbacks=cbs,deterministic=True,track_grad_norm=2,gradient_clip_val=0.5,**cfg.trainer\n",
    "        )\n",
    "        trainer.fit(net, train_loader)\n",
    "        return trainer\n",
    "        #trainer.tune(net,train_loader)\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/common/home/vk405/Projects/Crossmdl/nbs/ckpts/trail_debug_balanced2/epoch=74-train_loss=0.31.ckpt'"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = cfg.artifacts_loc + \"ckpts/\" + str(cfg.version) + \"/\"\n",
    "ckpt = path + os.listdir(path)[0]\n",
    "ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "youcookdata = YoucookDset2(data_dir=cfg.data_dir,split=cfg.mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(youcookdata.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CrossattnModel(cfg)\n",
    "net = net.load_from_checkpoint(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vid = 'Ysh60eirChU'\n",
    "\n",
    "ind = youcookdata.global_map[sample_vid]\n",
    "\n",
    "img = youcookdata.global_imgfeats[ind]\n",
    "\n",
    "txts = youcookdata.global_txt[sample_vid]\n",
    "labels = youcookdata.final_labels[sample_vid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_l = [ele[-1] for ele in youcookdata.data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youcookdata.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = pd.DataFrame({'trn_labels':trn_l})\n",
    "# for ele in np.linspace(0,1,10):\n",
    "#     print(f\"quantile:{round(ele,2)},value:{d.quantile(ele)[0]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {}\n",
    "for start in list(range(0,500-50)):\n",
    "    s_i = torch.tensor(img[start:start+50])\n",
    "    s_t  = torch.tensor(txts[1]).unsqueeze(dim=0)\n",
    "    p = net(([s_i],s_t))\n",
    "    preds[start] = p.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = CrossattnModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name,param in net.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0,\n",
       " 1: 1.0,\n",
       " 2: 1.0,\n",
       " 3: 1.0,\n",
       " 4: 1.0,\n",
       " 5: 1.0,\n",
       " 6: 1.0,\n",
       " 7: 1.0,\n",
       " 8: 1.0,\n",
       " 9: 1.0,\n",
       " 10: 1.0,\n",
       " 11: 1.0,\n",
       " 12: 1.0,\n",
       " 13: 1.0,\n",
       " 14: 1.0,\n",
       " 15: 1.0,\n",
       " 16: 1.0,\n",
       " 17: 1.0,\n",
       " 18: 1.0,\n",
       " 19: 1.0,\n",
       " 20: 1.0,\n",
       " 21: 0.9999998807907104,\n",
       " 22: 1.0,\n",
       " 23: 1.0,\n",
       " 24: 1.0,\n",
       " 25: 1.0,\n",
       " 26: 1.0,\n",
       " 27: 1.0,\n",
       " 28: 0.9999998807907104,\n",
       " 29: 0.9999998807907104,\n",
       " 30: 1.0,\n",
       " 31: 1.0,\n",
       " 32: 0.9999998807907104,\n",
       " 33: 0.9999997615814209,\n",
       " 34: 0.9999998807907104,\n",
       " 35: 0.9999998807907104,\n",
       " 36: 0.9999998807907104,\n",
       " 37: 0.9999997615814209,\n",
       " 38: 0.9999998807907104,\n",
       " 39: 0.9999998807907104,\n",
       " 40: 0.9999998807907104,\n",
       " 41: 0.9999998807907104,\n",
       " 42: 0.9999992847442627,\n",
       " 43: 0.9999992847442627,\n",
       " 44: 0.9999997615814209,\n",
       " 45: 1.0,\n",
       " 46: 1.0,\n",
       " 47: 1.0,\n",
       " 48: 1.0,\n",
       " 49: 1.0,\n",
       " 50: 1.0,\n",
       " 51: 1.0,\n",
       " 52: 1.0,\n",
       " 53: 1.0,\n",
       " 54: 1.0,\n",
       " 55: 1.0,\n",
       " 56: 1.0,\n",
       " 57: 1.0,\n",
       " 58: 1.0,\n",
       " 59: 1.0,\n",
       " 60: 1.0,\n",
       " 61: 1.0,\n",
       " 62: 1.0,\n",
       " 63: 1.0,\n",
       " 64: 1.0,\n",
       " 65: 1.0,\n",
       " 66: 1.0,\n",
       " 67: 1.0,\n",
       " 68: 1.0,\n",
       " 69: 1.0,\n",
       " 70: 1.0,\n",
       " 71: 1.0,\n",
       " 72: 1.0,\n",
       " 73: 1.0,\n",
       " 74: 1.0,\n",
       " 75: 1.0,\n",
       " 76: 1.0,\n",
       " 77: 1.0,\n",
       " 78: 1.0,\n",
       " 79: 1.0,\n",
       " 80: 1.0,\n",
       " 81: 1.0,\n",
       " 82: 1.0,\n",
       " 83: 1.0,\n",
       " 84: 1.0,\n",
       " 85: 1.0,\n",
       " 86: 1.0,\n",
       " 87: 1.0,\n",
       " 88: 1.0,\n",
       " 89: 1.0,\n",
       " 90: 1.0,\n",
       " 91: 1.0,\n",
       " 92: 1.0,\n",
       " 93: 1.0,\n",
       " 94: 1.0,\n",
       " 95: 1.0,\n",
       " 96: 1.0,\n",
       " 97: 1.0,\n",
       " 98: 1.0,\n",
       " 99: 1.0,\n",
       " 100: 1.0,\n",
       " 101: 1.0,\n",
       " 102: 1.0,\n",
       " 103: 1.0,\n",
       " 104: 1.0,\n",
       " 105: 1.0,\n",
       " 106: 1.0,\n",
       " 107: 1.0,\n",
       " 108: 1.0,\n",
       " 109: 1.0,\n",
       " 110: 1.0,\n",
       " 111: 1.0,\n",
       " 112: 1.0,\n",
       " 113: 1.0,\n",
       " 114: 1.0,\n",
       " 115: 1.0,\n",
       " 116: 1.0,\n",
       " 117: 1.0,\n",
       " 118: 1.0,\n",
       " 119: 1.0,\n",
       " 120: 1.0,\n",
       " 121: 1.0,\n",
       " 122: 1.0,\n",
       " 123: 1.0,\n",
       " 124: 1.0,\n",
       " 125: 1.0,\n",
       " 126: 1.0,\n",
       " 127: 1.0,\n",
       " 128: 1.0,\n",
       " 129: 1.0,\n",
       " 130: 1.0,\n",
       " 131: 1.0,\n",
       " 132: 1.0,\n",
       " 133: 1.0,\n",
       " 134: 1.0,\n",
       " 135: 1.0,\n",
       " 136: 1.0,\n",
       " 137: 1.0,\n",
       " 138: 1.0,\n",
       " 139: 1.0,\n",
       " 140: 1.0,\n",
       " 141: 1.0,\n",
       " 142: 1.0,\n",
       " 143: 1.0,\n",
       " 144: 1.0,\n",
       " 145: 1.0,\n",
       " 146: 1.0,\n",
       " 147: 1.0,\n",
       " 148: 1.0,\n",
       " 149: 1.0,\n",
       " 150: 1.0,\n",
       " 151: 1.0,\n",
       " 152: 1.0,\n",
       " 153: 1.0,\n",
       " 154: 1.0,\n",
       " 155: 1.0,\n",
       " 156: 0.9999996423721313,\n",
       " 157: 0.9999998807907104,\n",
       " 158: 1.0,\n",
       " 159: 1.0,\n",
       " 160: 0.9999986886978149,\n",
       " 161: 0.9999994039535522,\n",
       " 162: 0.9999995231628418,\n",
       " 163: 0.9999997615814209,\n",
       " 164: 0.9999998807907104,\n",
       " 165: 0.9999998807907104,\n",
       " 166: 0.9999998807907104,\n",
       " 167: 0.9999998807907104,\n",
       " 168: 0.9999998807907104,\n",
       " 169: 0.9999998807907104,\n",
       " 170: 0.9999997615814209,\n",
       " 171: 0.9999977350234985,\n",
       " 172: 0.9999979734420776,\n",
       " 173: 0.9999979734420776,\n",
       " 174: 0.9999977350234985,\n",
       " 175: 0.9999988079071045,\n",
       " 176: 0.9999973773956299,\n",
       " 177: 0.9999890327453613,\n",
       " 178: 0.9999932050704956,\n",
       " 179: 0.999993085861206,\n",
       " 180: 0.9999978542327881,\n",
       " 181: 0.9999991655349731,\n",
       " 182: 0.999987006187439,\n",
       " 183: 0.9999920129776001,\n",
       " 184: 0.9999940395355225,\n",
       " 185: 0.9999920129776001,\n",
       " 186: 0.9999926090240479,\n",
       " 187: 0.9999966621398926,\n",
       " 188: 0.9999985694885254,\n",
       " 189: 0.9999995231628418,\n",
       " 190: 0.9999997615814209,\n",
       " 191: 0.9999997615814209,\n",
       " 192: 0.9999997615814209,\n",
       " 193: 0.9999995231628418,\n",
       " 194: 0.9999995231628418,\n",
       " 195: 0.9999994039535522,\n",
       " 196: 0.9999990463256836,\n",
       " 197: 0.9999988079071045,\n",
       " 198: 0.9999972581863403,\n",
       " 199: 0.999998927116394,\n",
       " 200: 0.9999995231628418,\n",
       " 201: 0.9999995231628418,\n",
       " 202: 0.9999953508377075,\n",
       " 203: 0.9999834299087524,\n",
       " 204: 0.999972939491272,\n",
       " 205: 0.9999837875366211,\n",
       " 206: 0.9999953508377075,\n",
       " 207: 0.9999885559082031,\n",
       " 208: 0.9999958276748657,\n",
       " 209: 0.9999979734420776,\n",
       " 210: 0.999998927116394,\n",
       " 211: 0.9999991655349731,\n",
       " 212: 0.9999994039535522,\n",
       " 213: 0.9999995231628418,\n",
       " 214: 0.9999996423721313,\n",
       " 215: 0.9999997615814209,\n",
       " 216: 0.9999996423721313,\n",
       " 217: 0.9999997615814209,\n",
       " 218: 0.9999998807907104,\n",
       " 219: 0.9999998807907104,\n",
       " 220: 0.9999998807907104,\n",
       " 221: 0.9999998807907104,\n",
       " 222: 0.9999997615814209,\n",
       " 223: 1.0,\n",
       " 224: 1.0,\n",
       " 225: 1.0,\n",
       " 226: 1.0,\n",
       " 227: 1.0,\n",
       " 228: 0.9999998807907104,\n",
       " 229: 0.9999998807907104,\n",
       " 230: 0.9999998807907104,\n",
       " 231: 0.9999998807907104,\n",
       " 232: 0.9999990463256836,\n",
       " 233: 0.9999983310699463,\n",
       " 234: 0.9999980926513672,\n",
       " 235: 0.9999990463256836,\n",
       " 236: 0.9999932050704956,\n",
       " 237: 0.9999970197677612,\n",
       " 238: 0.9999995231628418,\n",
       " 239: 0.9999997615814209,\n",
       " 240: 0.9999990463256836,\n",
       " 241: 0.999998927116394,\n",
       " 242: 0.9999990463256836,\n",
       " 243: 0.9999998807907104,\n",
       " 244: 1.0,\n",
       " 245: 1.0,\n",
       " 246: 1.0,\n",
       " 247: 1.0,\n",
       " 248: 0.9999997615814209,\n",
       " 249: 0.9999998807907104,\n",
       " 250: 1.0,\n",
       " 251: 1.0,\n",
       " 252: 1.0,\n",
       " 253: 1.0,\n",
       " 254: 1.0,\n",
       " 255: 1.0,\n",
       " 256: 1.0,\n",
       " 257: 1.0,\n",
       " 258: 1.0,\n",
       " 259: 1.0,\n",
       " 260: 1.0,\n",
       " 261: 1.0,\n",
       " 262: 1.0,\n",
       " 263: 1.0,\n",
       " 264: 1.0,\n",
       " 265: 1.0,\n",
       " 266: 1.0,\n",
       " 267: 1.0,\n",
       " 268: 1.0,\n",
       " 269: 1.0,\n",
       " 270: 1.0,\n",
       " 271: 1.0,\n",
       " 272: 1.0,\n",
       " 273: 1.0,\n",
       " 274: 1.0,\n",
       " 275: 1.0,\n",
       " 276: 1.0,\n",
       " 277: 1.0,\n",
       " 278: 1.0,\n",
       " 279: 1.0,\n",
       " 280: 1.0,\n",
       " 281: 1.0,\n",
       " 282: 1.0,\n",
       " 283: 1.0,\n",
       " 284: 1.0,\n",
       " 285: 1.0,\n",
       " 286: 1.0,\n",
       " 287: 1.0,\n",
       " 288: 1.0,\n",
       " 289: 1.0,\n",
       " 290: 1.0,\n",
       " 291: 1.0,\n",
       " 292: 1.0,\n",
       " 293: 1.0,\n",
       " 294: 1.0,\n",
       " 295: 1.0,\n",
       " 296: 1.0,\n",
       " 297: 1.0,\n",
       " 298: 1.0,\n",
       " 299: 1.0,\n",
       " 300: 1.0,\n",
       " 301: 1.0,\n",
       " 302: 1.0,\n",
       " 303: 1.0,\n",
       " 304: 1.0,\n",
       " 305: 1.0,\n",
       " 306: 1.0,\n",
       " 307: 1.0,\n",
       " 308: 1.0,\n",
       " 309: 1.0,\n",
       " 310: 1.0,\n",
       " 311: 1.0,\n",
       " 312: 1.0,\n",
       " 313: 1.0,\n",
       " 314: 1.0,\n",
       " 315: 1.0,\n",
       " 316: 1.0,\n",
       " 317: 1.0,\n",
       " 318: 1.0,\n",
       " 319: 1.0,\n",
       " 320: 1.0,\n",
       " 321: 1.0,\n",
       " 322: 1.0,\n",
       " 323: 1.0,\n",
       " 324: 1.0,\n",
       " 325: 1.0,\n",
       " 326: 1.0,\n",
       " 327: 1.0,\n",
       " 328: 1.0,\n",
       " 329: 1.0,\n",
       " 330: 1.0,\n",
       " 331: 1.0,\n",
       " 332: 1.0,\n",
       " 333: 1.0,\n",
       " 334: 1.0,\n",
       " 335: 1.0,\n",
       " 336: 1.0,\n",
       " 337: 1.0,\n",
       " 338: 1.0,\n",
       " 339: 1.0,\n",
       " 340: 1.0,\n",
       " 341: 1.0,\n",
       " 342: 1.0,\n",
       " 343: 1.0,\n",
       " 344: 1.0,\n",
       " 345: 1.0,\n",
       " 346: 1.0,\n",
       " 347: 1.0,\n",
       " 348: 1.0,\n",
       " 349: 1.0,\n",
       " 350: 1.0,\n",
       " 351: 1.0,\n",
       " 352: 1.0,\n",
       " 353: 1.0,\n",
       " 354: 1.0,\n",
       " 355: 1.0,\n",
       " 356: 1.0,\n",
       " 357: 1.0,\n",
       " 358: 1.0,\n",
       " 359: 1.0,\n",
       " 360: 1.0,\n",
       " 361: 1.0,\n",
       " 362: 1.0,\n",
       " 363: 1.0,\n",
       " 364: 1.0,\n",
       " 365: 1.0,\n",
       " 366: 1.0,\n",
       " 367: 1.0,\n",
       " 368: 1.0,\n",
       " 369: 1.0,\n",
       " 370: 1.0,\n",
       " 371: 1.0,\n",
       " 372: 1.0,\n",
       " 373: 1.0,\n",
       " 374: 1.0,\n",
       " 375: 1.0,\n",
       " 376: 1.0,\n",
       " 377: 1.0,\n",
       " 378: 1.0,\n",
       " 379: 1.0,\n",
       " 380: 1.0,\n",
       " 381: 1.0,\n",
       " 382: 1.0,\n",
       " 383: 1.0,\n",
       " 384: 1.0,\n",
       " 385: 0.9999998807907104,\n",
       " 386: 0.9999998807907104,\n",
       " 387: 0.9999998807907104,\n",
       " 388: 1.0,\n",
       " 389: 1.0,\n",
       " 390: 1.0,\n",
       " 391: 1.0,\n",
       " 392: 1.0,\n",
       " 393: 1.0,\n",
       " 394: 1.0,\n",
       " 395: 1.0,\n",
       " 396: 1.0,\n",
       " 397: 1.0,\n",
       " 398: 1.0,\n",
       " 399: 1.0,\n",
       " 400: 1.0,\n",
       " 401: 1.0,\n",
       " 402: 1.0,\n",
       " 403: 1.0,\n",
       " 404: 1.0,\n",
       " 405: 1.0,\n",
       " 406: 1.0,\n",
       " 407: 1.0,\n",
       " 408: 1.0,\n",
       " 409: 1.0,\n",
       " 410: 1.0,\n",
       " 411: 0.9999967813491821,\n",
       " 412: 0.9999992847442627,\n",
       " 413: 0.9999998807907104,\n",
       " 414: 1.0,\n",
       " 415: 1.0,\n",
       " 416: 1.0,\n",
       " 417: 1.0,\n",
       " 418: 1.0,\n",
       " 419: 1.0,\n",
       " 420: 1.0,\n",
       " 421: 1.0,\n",
       " 422: 1.0,\n",
       " 423: 1.0,\n",
       " 424: 1.0,\n",
       " 425: 1.0,\n",
       " 426: 1.0,\n",
       " 427: 1.0,\n",
       " 428: 1.0,\n",
       " 429: 1.0,\n",
       " 430: 1.0,\n",
       " 431: 1.0,\n",
       " 432: 1.0,\n",
       " 433: 1.0,\n",
       " 434: 1.0,\n",
       " 435: 1.0,\n",
       " 436: 1.0,\n",
       " 437: 1.0,\n",
       " 438: 1.0,\n",
       " 439: 1.0,\n",
       " 440: 1.0,\n",
       " 441: 1.0,\n",
       " 442: 1.0,\n",
       " 443: 1.0,\n",
       " 444: 1.0,\n",
       " 445: 1.0,\n",
       " 446: 1.0,\n",
       " 447: 1.0,\n",
       " 448: 1.0,\n",
       " 449: 1.0}"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(list(preds.items()),key=lambda x:x[-1])[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot train for smooth interpolation..use some kind of contrastive loss/triplet loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "cfg = Namespace(\n",
    "    version = 'trail_debug_balanced2',\n",
    "    id = 0,\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/YouCookII/\",\n",
    "    mode = 'train',\n",
    "    data_split = 'train',\n",
    "    loggers = [\"csv\"],\n",
    "    seed = 0,\n",
    "    cbs = [\"checkpoint\"],\n",
    "    trainer = {'log_every_n_steps': 50,\n",
    "    'max_epochs': 150},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"train_loss\"},\n",
    "    use_precomp_emb = True,\n",
    "    edim = 100,\n",
    "    attnhdim = 50,\n",
    "    nheads = 10,\n",
    "    wrdim = 768,\n",
    "    vidim = 512,\n",
    "    hdim = 30,\n",
    "    dropoutp=0.0,\n",
    "    batch_size=512,\n",
    "    framecnt=499,\n",
    "    retrain=False,\n",
    "    lr = 5e-4,\n",
    "    warmup=50,\n",
    "    max_iters = 2000\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.689\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
