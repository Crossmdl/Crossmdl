{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pad your sequences\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import joblib\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numpy import linalg as LA\n",
    "from argparse import Namespace\n",
    "from numpy import genfromtxt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint,LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import clip\n",
    "\n",
    "\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "wandb_logger = lambda dir, version: WandbLogger(\n",
    "    name=\"wandb\", save_dir=dir, version=version\n",
    ")\n",
    "csvlogger = lambda dir, version: CSVLogger(dir, name=\"csvlogs\", version=version)\n",
    "tblogger = lambda dir, version: TensorBoardLogger(dir, name=\"tblogs\", version=version)\n",
    "\n",
    "def get_loggers(dir,version,lis=[\"csv\"]):\n",
    "    lgrs = []\n",
    "    if \"wandb\" in lis:\n",
    "        lgrs.append(wandb_logger(dir, version))\n",
    "    if \"csv\" in lis:\n",
    "        lgrs.append(csvlogger(dir, version))\n",
    "    if \"tb\" in lis:\n",
    "        lgrs.append(tblogger(dir, version))\n",
    "    return lgrs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vid_ids(split='training',\\\n",
    "    annotns_file='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/annotations/youcookii_annotations_trainval.json'):\n",
    "    # Returns vid_ids corresponding to the split: 'training'/'validation'\n",
    "    \n",
    "    vid_lis = []\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)['database']\n",
    "        for key in annotns:\n",
    "            if annotns[key]['subset'] == split:\n",
    "                vid_lis.append(key)\n",
    "    return vid_lis\n",
    "\n",
    "\n",
    "def get_split_files(split='training',\\\n",
    "    annotns_file='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/annotations/youcookii_annotations_trainval.json',\\\n",
    "        data_dir = '/common/users/vk405/Youcook/'):\n",
    "    total_ids = get_vid_ids(split,annotns_file)\n",
    "    downloaded_ids = set([dir for dir in os.listdir(data_dir) if 'joblib' not in dir])\n",
    "    vid_locs = []\n",
    "    sents = {}\n",
    "    segs = {}\n",
    "    incomplete = []\n",
    "    for id in total_ids:\n",
    "        if id in downloaded_ids:\n",
    "            vid_loc = data_dir+id + '/'\n",
    "            if len(os.listdir(vid_loc))>=495:\n",
    "                vid_locs.append(vid_loc)\n",
    "                seg = joblib.load(data_dir+f'{id}global_segs.joblib')\n",
    "                sent = joblib.load(data_dir+f'{id}global_sents.joblib')\n",
    "                try:\n",
    "                    sents[id] = sent[id]\n",
    "                    segs[id] = seg[id]\n",
    "                except:\n",
    "                    print(f\"{id} is no corresponding global sent/seg\")\n",
    "            else:\n",
    "                #print(f\"{id} has only imgs {len(os.listdir(vid_loc))}\")\n",
    "                incomplete.append(id)\n",
    "    return vid_locs,segs,sents,incomplete \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "FEAT_DIR = pathlib.Path('/common/users/vk405/CLIP_FEAT')\n",
    "RAWFRAME_DIR = pathlib.Path('/common/users/vk405/Youcook/')\n",
    "tokens =  {'O':0,'B':1,'I':2}\n",
    "\n",
    "# class Dset(Dataset):\n",
    "#     def __init__(self,data_dir,feat_dir,split):\n",
    "#         self.data_dir = data_dir\n",
    "#         self.feat_dir = feat_dir\n",
    "#         self.split = split\n",
    "#         self.vid_ids,self.sents = self.get_ids()\n",
    "#         self.labels = self.getlabels()\n",
    "#         self.sanitycheck()\n",
    "#         #self.data = self.getdata()\n",
    "        \n",
    "#     def segs_class(self,segs):\n",
    "#         out = np.zeros(500)\n",
    "#         for seg in segs:\n",
    "#             st,end = seg\n",
    "#             out[st] = tokens['B']\n",
    "#             out[st+1:end+1] = tokens['I']\n",
    "#         return out    \n",
    "\n",
    "\n",
    "#     def sanitycheck(self):\n",
    "#         mis = []\n",
    "#         #import pdb;pdb.set_trace()\n",
    "#         for key in self.labels.keys():\n",
    "#             txt_loc = self.feat_dir/self.split/f'txt_{key}.joblib'\n",
    "#             txt = joblib.load(txt_loc)\n",
    "#             if len(self.labels[key]) == len(self.sents[key]) == len(txt):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 print(key)\n",
    "#                 mis.append(key)\n",
    "#         print(f\"segs are not matching:{mis}\")\n",
    "#         for key in mis:\n",
    "#             self.vid_ids.remove(key)\n",
    "#         self.sents = None\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.vid_ids)\n",
    "\n",
    "#     def __getitem__(self,idx):\n",
    "#         return self.load(self.vid_ids[idx])\n",
    "\n",
    "#     def load(self,vid_id):\n",
    "#         vid_frames_loc = self.feat_dir/self.split/f'vid_{vid_id}.joblib'\n",
    "#         txt_loc = self.feat_dir/self.split/f'txt_{vid_id}.joblib'\n",
    "#         vid = joblib.load(vid_frames_loc)\n",
    "#         try:\n",
    "#             txt = joblib.load(txt_loc)\n",
    "#         except:\n",
    "#             import pdb;pdb.set_trace()\n",
    "#         segs = self.labels[vid_id]\n",
    "#         labels = self.segs_class(segs)\n",
    "#         return vid,txt,labels\n",
    "\n",
    "#     def getlabels(self):\n",
    "#         label_dict = {}\n",
    "#         for vidid in self.vid_ids:\n",
    "#             vidloc = self.data_dir/vidid\n",
    "#             segs = self.extract_seg(vidloc)\n",
    "#             label_dict[vidid] = segs\n",
    "#         return label_dict\n",
    "    \n",
    "#     def extract_seg(self,vid_loc):\n",
    "#         imgs = sorted(os.listdir(vid_loc),key=lambda x: int(x.split('_')[0]))\n",
    "#         segs = defaultdict(list)\n",
    "#         for img in imgs:\n",
    "#             ind,rem = int(img.split('_')[0]),img.split('_')[-1]\n",
    "            \n",
    "#             if 'n.' not in rem:\n",
    "#                 #print(ind,rem)\n",
    "#                 seg_id = int(rem.split('.')[0])\n",
    "#                 segs[seg_id].append(ind)\n",
    "#                 #print(seg_id,ind)\n",
    "#         final_segs = []\n",
    "#         #import pdb;pdb.set_trace()\n",
    "#         segids = sorted(segs.keys())\n",
    "#         for segid in segids:\n",
    "#             final_segs.append((min(segs[segid]),max(segs[segid])))\n",
    "#         return final_segs\n",
    "        \n",
    "#     def get_ids(self):\n",
    "#         annotns_file='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/annotations/youcookii_annotations_trainval.json'\n",
    "#         data_dir = '/common/users/vk405/Youcook/'\n",
    "#         vid_locs,_,sents,_ = get_split_files(self.split,annotns_file,data_dir)\n",
    "#         ids = [ele.split('/')[-2] for ele in vid_locs]\n",
    "#         files = set(os.listdir(self.feat_dir/self.split))\n",
    "#         finids = []\n",
    "#         missing = []\n",
    "#         for id in ids:\n",
    "#             if f'vid_{id}.joblib' in files:\n",
    "#                 finids.append(id)\n",
    "#             else:missing.append(id)\n",
    "#         print(f\"missing:{missing}\")\n",
    "#         return finids,sents\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified with #nclasses\n",
    "class Dset(Dataset):\n",
    "    def __init__(self,data_dir,feat_dir,split):\n",
    "        self.data_dir = data_dir\n",
    "        self.feat_dir = feat_dir\n",
    "        self.split = split\n",
    "        self.vid_ids,self.sents = self.get_ids()\n",
    "        self.labels = self.getlabels()\n",
    "        self.sanitycheck()\n",
    "        #self.data = self.getdata()\n",
    "        \n",
    "    def segs_class(self,segs):\n",
    "        out = np.zeros(500)\n",
    "        for segid,seg in enumerate(segs):\n",
    "            st,end = seg\n",
    "            out[st:end+1] = segid+1\n",
    "        return out    \n",
    "\n",
    "\n",
    "    def sanitycheck(self):\n",
    "        mis = []\n",
    "        #import pdb;pdb.set_trace()\n",
    "        for key in self.labels.keys():\n",
    "            txt_loc = self.feat_dir/self.split/f'txt_{key}.joblib'\n",
    "            txt = joblib.load(txt_loc)\n",
    "            if len(self.labels[key]) == len(self.sents[key]) == len(txt):\n",
    "                pass\n",
    "            else:\n",
    "                print(key)\n",
    "                mis.append(key)\n",
    "        print(f\"segs are not matching:{mis}\")\n",
    "        for key in mis:\n",
    "            self.vid_ids.remove(key)\n",
    "        self.sents = None\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vid_ids)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.load(self.vid_ids[idx])\n",
    "\n",
    "    def load(self,vid_id):\n",
    "        vid_frames_loc = self.feat_dir/self.split/f'vid_{vid_id}.joblib'\n",
    "        txt_loc = self.feat_dir/self.split/f'txt_{vid_id}.joblib'\n",
    "        vid = joblib.load(vid_frames_loc)\n",
    "        try:\n",
    "            txt = joblib.load(txt_loc)\n",
    "        except:\n",
    "            import pdb;pdb.set_trace()\n",
    "        segs = self.labels[vid_id]\n",
    "        labels = self.segs_class(segs)\n",
    "        return vid,txt,labels\n",
    "\n",
    "    def getlabels(self):\n",
    "        label_dict = {}\n",
    "        for vidid in self.vid_ids:\n",
    "            vidloc = self.data_dir/vidid\n",
    "            segs = self.extract_seg(vidloc)\n",
    "            label_dict[vidid] = segs\n",
    "        return label_dict\n",
    "    \n",
    "    def extract_seg(self,vid_loc):\n",
    "        imgs = sorted(os.listdir(vid_loc),key=lambda x: int(x.split('_')[0]))\n",
    "        segs = defaultdict(list)\n",
    "        for img in imgs:\n",
    "            ind,rem = int(img.split('_')[0]),img.split('_')[-1]\n",
    "            \n",
    "            if 'n.' not in rem:\n",
    "                #print(ind,rem)\n",
    "                seg_id = int(rem.split('.')[0])\n",
    "                segs[seg_id].append(ind)\n",
    "                #print(seg_id,ind)\n",
    "        final_segs = []\n",
    "        #import pdb;pdb.set_trace()\n",
    "        segids = sorted(segs.keys())\n",
    "        for segid in segids:\n",
    "            final_segs.append((min(segs[segid]),max(segs[segid])))\n",
    "        return final_segs\n",
    "        \n",
    "    def get_ids(self):\n",
    "        annotns_file='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/annotations/youcookii_annotations_trainval.json'\n",
    "        data_dir = '/common/users/vk405/Youcook/'\n",
    "        vid_locs,_,sents,_ = get_split_files(self.split,annotns_file,data_dir)\n",
    "        ids = [ele.split('/')[-2] for ele in vid_locs]\n",
    "        files = set(os.listdir(self.feat_dir/self.split))\n",
    "        finids = []\n",
    "        missing = []\n",
    "        for id in ids:\n",
    "            if f'vid_{id}.joblib' in files:\n",
    "                finids.append(id)\n",
    "            else:missing.append(id)\n",
    "        print(f\"missing:{missing}\")\n",
    "        return finids,sents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing:['ukfCQQpZ0k4', 'NK2xHVWojgY', 'mixdagZ-fwI']\n",
      "cwsDQ7M5OTI\n",
      "uf65nfh6X2U\n",
      "segs are not matching:['cwsDQ7M5OTI', 'uf65nfh6X2U']\n"
     ]
    }
   ],
   "source": [
    "d = Dset(RAWFRAME_DIR,FEAT_DIR,'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcnt = []\n",
    "for key in d.labels.keys():\n",
    "    sz = len(d.labels[key])\n",
    "    lcnt.append(sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1189.000000\n",
       "mean        7.747687\n",
       "std         2.824899\n",
       "min         3.000000\n",
       "25%         5.000000\n",
       "50%         7.000000\n",
       "75%        10.000000\n",
       "max        16.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(lcnt).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing:[]\n",
      "95WMX64RIBc\n",
      "segs are not matching:['95WMX64RIBc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    417.000000\n",
       "mean       7.683453\n",
       "std        2.792597\n",
       "min        3.000000\n",
       "25%        6.000000\n",
       "50%        7.000000\n",
       "75%        9.000000\n",
       "max       16.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Dset(RAWFRAME_DIR,FEAT_DIR,'validation')\n",
    "lcnt = []\n",
    "for key in d.labels.keys():\n",
    "    sz = len(d.labels[key])\n",
    "    lcnt.append(sz)\n",
    "pd.Series(lcnt).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid,txt,labels = d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 512), (6, 512), (500,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid.shape,txt.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS-ATTN LAYER\n",
    "#model utils\n",
    "\n",
    "#!pip install transformers\n",
    "\n",
    "def init_parameters_xavier_uniform(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "def scaled_dot(query, key, mask_key=None):  \n",
    "    score = torch.matmul(query, key.transpose(-2, -1))\n",
    "    score /= math.sqrt(query.size(-1))\n",
    "    if mask_key is not None:\n",
    "        score = score.masked_fill(mask_key, -1e18)  # Represents negative infinity\n",
    "    return score      \n",
    "            \n",
    "def attend(query, key, value, mask_key=None, dropout=None):\n",
    "    # TODO: Implement\n",
    "    # Use scaled_dot, be sure to mask key\n",
    "    #smax = nn.Softmax(-1)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    score = scaled_dot(query,key,mask_key)  \n",
    "    attention = F.softmax(score,dim=-1)\n",
    "    if dropout is not None:#do = nn.Dropout(dropout)\n",
    "        attention = dropout(attention)\n",
    "    answer = torch.matmul(attention,value) \n",
    "    # Convexly combine value embeddings using attention, this should be just a matrix-matrix multiplication.\n",
    "    return answer, attention\n",
    "\n",
    "\n",
    "\n",
    "def split_heads(batch, num_heads):  \n",
    "    (batch_size, length, dim) = batch.size()  # These are the expected batch dimensions.\n",
    "    assert dim % num_heads == 0  # Assert that dimension is divisible by the number of heads.\n",
    "    dim_head = dim // num_heads\n",
    "\n",
    "    # No new memory allocation\n",
    "    splitted = batch.view(batch_size, -1, num_heads, dim_head).transpose(1, 2)  \n",
    "    return splitted  # (batch_size, num_heads, length, dim_head), note that now the last two dimensions are compatible with our attention functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_heads(batch):  \n",
    "    (batch_size, num_heads, length, dim_head) = batch.size()  # These are the expected batch dimensions.\n",
    "\n",
    "    # New memory allocation (reshape), can't avoid.\n",
    "    merged = batch.transpose(1, 2).reshape(batch_size, -1, num_heads * dim_head)\n",
    "    return merged  # (batch_size, length, dim)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "\n",
    "        self.linear_query = nn.Linear(dim, dim)\n",
    "        self.linear_key = nn.Linear(dim, dim)\n",
    "        self.linear_value = nn.Linear(dim, dim)\n",
    "        self.linear_final = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, query, key, value, mask_key=None, layer_cache=None,\n",
    "              memory_attention=False):\n",
    "        \"\"\"\n",
    "        INPUT\n",
    "          query: (batch_size, length_query, dim)\n",
    "          key: (batch_size, length_key, dim)\n",
    "          value: (batch_size, length_key, dim_value)\n",
    "          mask_key: (*, 1, length_key) if queries share the same mask, else\n",
    "                    (*, length_query, length_key)\n",
    "          layer_cache: if not None, stepwise decoding (cache of key/value)\n",
    "          memory_attention: doing memory attention in stepwise decoding?\n",
    "        OUTPUT\n",
    "          answer: (batch_size, length_query, dim_value)\n",
    "          attention: (batch_size, num_heads, length_query, length_key) else\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.linear_query(query)\n",
    "        query = split_heads(query, self.num_heads)  # (batch_size, num_heads, -1, dim_head)\n",
    "\n",
    "        def process_key_value(key, value):  # Only called when necessary.\n",
    "            key = self.linear_key(key)\n",
    "            key = split_heads(key, self.num_heads)\n",
    "            value = self.linear_value(value)\n",
    "            value = split_heads(value, self.num_heads)\n",
    "            return key, value\n",
    "\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if layer_cache is None:\n",
    "            key, value = process_key_value(key, value)\n",
    "        else:\n",
    "            assert query.size(2) == 1  # Stepwise decoding\n",
    "            \n",
    "            if memory_attention:\n",
    "                if layer_cache['memory_key'] is None:  # One-time calculation\n",
    "                    key, value = process_key_value(key, value)\n",
    "                    # (batch_size, num_heads, length_memory, dim)\n",
    "                    layer_cache['memory_key'] = key\n",
    "                    layer_cache['memory_value'] = value\n",
    "\n",
    "                key = layer_cache['memory_key']\n",
    "                value = layer_cache['memory_value']\n",
    "\n",
    "            else:  # Self-attention during decoding\n",
    "                key, value = process_key_value(key, value)\n",
    "                assert key.size(2) == 1 and value.size(2) == 1\n",
    "                \n",
    "                # Append to previous.\n",
    "                if layer_cache['self_key'] is not None:\n",
    "                    key = torch.cat((layer_cache['self_key'], key), dim=2)\n",
    "                    value = torch.cat((layer_cache['self_value'], value), dim=2)\n",
    "                    \n",
    "                 # (batch_size, num_heads, length_decoded, dim)\n",
    "                layer_cache['self_key'] = key  # Recache.\n",
    "                layer_cache['self_value'] = value\n",
    "        # Because we've splitted embeddings into heads, we must also split the mask. \n",
    "        # And because each query uses the same mask for all heads (we don't use different masking for different heads), \n",
    "        # we can specify length 1 for the head dimension.\n",
    "        if mask_key is not None:  \n",
    "            mask_key = mask_key.unsqueeze(1)  # (batch_size, 1, -1, length_key)\n",
    "\n",
    "        answer, attention = attend(query, key, value, mask_key, self.dropout)\n",
    "\n",
    "        answer = merge_heads(answer)  # (batch_size, length_key, dim)\n",
    "        answer = self.linear_final(answer)\n",
    "\n",
    "        return answer, attention\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_hidden, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, dim_hidden)\n",
    "        self.w2 = nn.Linear(dim_hidden, dim)\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.drop1 = nn.Dropout(drop_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(drop_rate)\n",
    "    def forward(self, x):\n",
    "        inter = self.drop1(self.relu(self.w1(self.layer_norm(x))))\n",
    "        output = self.drop2(self.w2(inter))\n",
    "        return output + x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SinusoidalPositioner(nn.Module):\n",
    "    def __init__(self, dim, drop_rate=0.1, length_max=5000):\n",
    "        super().__init__()\n",
    "        frequency = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.) / dim))  # Using different frequency for each dim\n",
    "        positions = torch.arange(0, length_max).unsqueeze(1)\n",
    "        wave = torch.zeros(length_max, dim)\n",
    "        wave[:, 0::2] = torch.sin(frequency * positions)\n",
    "        wave[:, 1::2] = torch.cos(frequency * positions)\n",
    "        self.register_buffer('wave', wave.unsqueeze(0))  # (1, length_max, dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.dim = dim\n",
    "        self.length_max = length_max\n",
    "    def forward(self, x, step=-1):\n",
    "        assert x.size(-2) <= self.length_max\n",
    "\n",
    "        if step < 0:  # Take the corresponding leftmost embeddings.\n",
    "            position_encoding = self.wave[:, :x.size(-2), :]\n",
    "        else:  # Take the embedding at the step.\n",
    "            position_encoding = self.wave[:, step, :]\n",
    "\n",
    "        x = x * math.sqrt(self.dim)\n",
    "        return self.dropout(x + position_encoding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, dim, num_heads, dim_hidden, drop_rate):\n",
    "    super().__init__()\n",
    "    self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "    self.self_attention = MultiHeadAttention(dim, num_heads, drop_rate)\n",
    "    self.drop = nn.Dropout(drop_rate)\n",
    "    self.feedforward = PositionwiseFeedForward(dim, dim_hidden, drop_rate)\n",
    "\n",
    "  def forward(self, source, mask_source=None):\n",
    "    # TODO: Implement\n",
    "    #print(source.shape)\n",
    "    normed = self.layer_norm(source)  \n",
    "    # Apply layer norm on source\n",
    "\n",
    "    attended, attention = self.self_attention(normed,normed,normed,mask_source)\n",
    "    #None, None  # Apply self-attention on normed (be sure to use mask_source).\n",
    "    attended = self.drop(attended) + source  \n",
    "    # Re-write attended by applying dropout and adding a residual connection to source.\n",
    "    return self.feedforward(attended), attention\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self,dim,num_heads,dim_hidden,drop_rate):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.context_attention = MultiHeadAttention(dim, num_heads, drop_rate)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "        self.feedforward = PositionwiseFeedForward(dim, dim_hidden, drop_rate)\n",
    "        \n",
    "    def forward(self,target,memory,layer_cache=None):\n",
    "        \n",
    "        cross_attn_target = self.layer_norm(target)\n",
    "        attended, attention = self.context_attention(cross_attn_target,memory,memory,layer_cache=layer_cache,memory_attention=True)\n",
    "        \n",
    "        attended = target + self.drop(attended)\n",
    "        \n",
    "        return self.feedforward(attended),attention\n",
    "\n",
    "\n",
    "\n",
    "layer_cache = {'memory_key': None, 'memory_value': None, 'self_key': None, 'self_value': None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "crattnlyr = CrossAttentionLayer(10,1,5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor(np.random.randn(1,1,10))\n",
    "memory = torch.tensor(np.random.randn(1,4,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "attended_ = crattnlyr(target.float(),memory.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,attn = attended_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(512,256,\\\n",
    "        1,bidirectional=True,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_t = torch.unsqueeze(torch.tensor(vid),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 512])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-58f91acf25d3>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hiddens, (final_h, final_c) = lstm(torch.tensor(vid_t).float())\n"
     ]
    }
   ],
   "source": [
    "hiddens, (final_h, final_c) = lstm(torch.tensor(vid_t).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 512])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_t = torch.unsqueeze(torch.tensor(txt),0)\n",
    "txt_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "croattnlyr = CrossAttentionLayer(512,1,100,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,attn = croattnlyr(hiddens,txt_t.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 500, 512]), torch.Size([1, 1, 500, 6]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmAttn(nn.Module):\n",
    "    def __init__(self,lstm_lyrs,lstm_hdim,bidirectional,input_dim,nheads,attn_hdim,attn_dropout,tgt_dim):\n",
    "        self.lstm_lyrs,self.lstm_hdim,self.bidirectional,self.input_dim,self.nheads,self.attn_hdim,self.attn_dropout,self.tgt_dim = \\\n",
    "            lstm_lyrs,lstm_hdim,bidirectional,input_dim,nheads,attn_hdim,attn_dropout,tgt_dim\n",
    "        super().__init__()\n",
    "        if bidirectional:\n",
    "            #as lstm concates both directions\n",
    "            lstm_hdim = lstm_hdim//2\n",
    "        self.lstm = nn.LSTM(input_dim,lstm_hdim,\\\n",
    "        lstm_lyrs,bidirectional=bidirectional,batch_first=True)\n",
    "        self.crossattn = CrossAttentionLayer(self.lstm_hdim,nheads,attn_hdim,attn_dropout)\n",
    "        self.txt_enc = nn.Linear(input_dim,self.lstm_hdim)\n",
    "        self.attn_weights = None\n",
    "        self.act = nn.ReLU()\n",
    "        self.tgt_enc = nn.Linear(self.lstm_hdim,tgt_dim)\n",
    "\n",
    "    def forward(self,vid,txt):\n",
    "        #import pdb;pdb.set_trace()\n",
    "        hiddens, (final_h, final_c) = self.lstm(vid)\n",
    "        txt_inp = self.act(self.txt_enc(txt))\n",
    "        out_emb,self.attn_weights = self.crossattn(hiddens,txt_inp)\n",
    "        tgt_emb = self.tgt_enc(out_emb)\n",
    "        return tgt_emb\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = LstmAttn(**cfg.emission_model)\n",
    "# tgt = m(vid.float(),txt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    \"\"\"Conditional random field.\n",
    "    This module implements a conditional random field [LMP01]_. The forward computation\n",
    "    of this class computes the log likelihood of the given sequence of tags and\n",
    "    emission score tensor. This class also has `~CRF.decode` method which finds\n",
    "    the best tag sequence given an emission score tensor using `Viterbi algorithm`_.\n",
    "    Args:\n",
    "        num_tags: Number of tags.\n",
    "        batch_first: Whether the first dimension corresponds to the size of a minibatch.\n",
    "    Attributes:\n",
    "        start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size\n",
    "            ``(num_tags,)``.\n",
    "        end_transitions (`~torch.nn.Parameter`): End transition score tensor of size\n",
    "            ``(num_tags,)``.\n",
    "        transitions (`~torch.nn.Parameter`): Transition score tensor of size\n",
    "            ``(num_tags, num_tags)``.\n",
    "    .. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).\n",
    "       \"Conditional random fields: Probabilistic models for segmenting and\n",
    "       labeling sequence data\". *Proc. 18th International Conf. on Machine\n",
    "       Learning*. Morgan Kaufmann. pp. 282â€“289.\n",
    "    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_tags: int, batch_first: bool = False) -> None:\n",
    "        if num_tags <= 0:\n",
    "            raise ValueError(f'invalid number of tags: {num_tags}')\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.batch_first = batch_first\n",
    "        self.start_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.end_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Initialize the transition parameters.\n",
    "        The parameters will be initialized randomly from a uniform distribution\n",
    "        between -0.1 and 0.1.\n",
    "        \"\"\"\n",
    "        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "        \n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(num_tags={self.num_tags})'\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            emissions: torch.Tensor,\n",
    "            tags: torch.LongTensor,\n",
    "            mask: Optional[torch.ByteTensor] = None,\n",
    "            reduction: str = 'sum',\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\n",
    "        Args:\n",
    "            emissions (`~torch.Tensor`): Emission score tensor of size\n",
    "                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "            tags (`~torch.LongTensor`): Sequence of tags tensor of size\n",
    "                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length)`` otherwise.\n",
    "            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n",
    "                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n",
    "            reduction: Specifies  the reduction to apply to the output:\n",
    "                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.\n",
    "                ``sum``: the output will be summed over batches. ``mean``: the output will be\n",
    "                averaged over batches. ``token_mean``: the output will be averaged over tokens.\n",
    "        Returns:\n",
    "            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if\n",
    "            reduction is ``none``, ``()`` otherwise.\n",
    "        \"\"\"\n",
    "        self._validate(emissions, tags=tags, mask=mask)\n",
    "        if reduction not in ('none', 'sum', 'mean', 'token_mean'):\n",
    "            raise ValueError(f'invalid reduction: {reduction}')\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(tags, dtype=torch.uint8)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            tags = tags.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        numerator = self._compute_score(emissions, tags, mask)\n",
    "        # shape: (batch_size,)\n",
    "        denominator = self._compute_normalizer(emissions, mask)\n",
    "        # shape: (batch_size,)\n",
    "        llh = numerator - denominator\n",
    "\n",
    "        if reduction == 'none':\n",
    "            return llh\n",
    "        if reduction == 'sum':\n",
    "            return llh.sum()\n",
    "        if reduction == 'mean':\n",
    "            return llh.mean()\n",
    "        assert reduction == 'token_mean'\n",
    "        return llh.sum() / mask.type_as(emissions).sum()\n",
    "\n",
    "    def decode(self, emissions: torch.Tensor,\n",
    "               mask: Optional[torch.ByteTensor] = None) -> List[List[int]]:\n",
    "        \"\"\"Find the most likely tag sequence using Viterbi algorithm.\n",
    "        Args:\n",
    "            emissions (`~torch.Tensor`): Emission score tensor of size\n",
    "                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n",
    "                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n",
    "        Returns:\n",
    "            List of list containing the best tag sequence for each batch.\n",
    "        \"\"\"\n",
    "        self._validate(emissions, mask=mask)\n",
    "        if mask is None:\n",
    "            mask = emissions.new_ones(emissions.shape[:2], dtype=torch.uint8)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        return self._viterbi_decode(emissions, mask)\n",
    "\n",
    "    def _validate(\n",
    "            self,\n",
    "            emissions: torch.Tensor,\n",
    "            tags: Optional[torch.LongTensor] = None,\n",
    "            mask: Optional[torch.ByteTensor] = None) -> None:\n",
    "        if emissions.dim() != 3:\n",
    "            raise ValueError(f'emissions must have dimension of 3, got {emissions.dim()}')\n",
    "        if emissions.size(2) != self.num_tags:\n",
    "            raise ValueError(\n",
    "                f'expected last dimension of emissions is {self.num_tags}, '\n",
    "                f'got {emissions.size(2)}')\n",
    "\n",
    "        if tags is not None:\n",
    "            if emissions.shape[:2] != tags.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and tags must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(tags.shape)}')\n",
    "\n",
    "        if mask is not None:\n",
    "            if emissions.shape[:2] != mask.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and mask must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(mask.shape)}')\n",
    "            no_empty_seq = not self.batch_first and mask[0].all()\n",
    "            no_empty_seq_bf = self.batch_first and mask[:, 0].all()\n",
    "            if not no_empty_seq and not no_empty_seq_bf:\n",
    "                raise ValueError('mask of the first timestep must all be on')\n",
    "\n",
    "    def _compute_score(\n",
    "            self, emissions: torch.Tensor, tags: torch.LongTensor,\n",
    "            mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # tags: (seq_length, batch_size)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and tags.dim() == 2\n",
    "        assert emissions.shape[:2] == tags.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask.shape == tags.shape\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length, batch_size = tags.shape\n",
    "        mask = mask.type_as(emissions)\n",
    "\n",
    "        # Start transition score and first emission\n",
    "        # shape: (batch_size,)\n",
    "        score = self.start_transitions[tags[0]]\n",
    "        score += emissions[0, torch.arange(batch_size), tags[0]]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Transition score to next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += self.transitions[tags[i - 1], tags[i]] * mask[i]\n",
    "\n",
    "            # Emission score for next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        # shape: (batch_size,)\n",
    "        last_tags = tags[seq_ends, torch.arange(batch_size)]\n",
    "        # shape: (batch_size,)\n",
    "        score += self.end_transitions[last_tags]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _compute_normalizer(\n",
    "            self, emissions: torch.Tensor, mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and mask.dim() == 2\n",
    "        assert emissions.shape[:2] == mask.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length = emissions.size(0)\n",
    "\n",
    "        # Start transition score and first emission; score has size of\n",
    "        # (batch_size, num_tags) where for each batch, the j-th column stores\n",
    "        # the score that the first timestep has tag j\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emissions = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the sum of scores of all\n",
    "            # possible tag sequences so far that end with transitioning from tag i to tag j\n",
    "            # and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emissions\n",
    "\n",
    "            # Sum over all possible current tags, but we're in score space, so a sum\n",
    "            # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of\n",
    "            # all possible tag sequences so far, that end in tag i\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score = torch.logsumexp(next_score, dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Sum (log-sum-exp) over all possible tags\n",
    "        # shape: (batch_size,)\n",
    "        return torch.logsumexp(score, dim=1)\n",
    "\n",
    "    def _viterbi_decode(self, emissions: torch.FloatTensor,\n",
    "                        mask: torch.ByteTensor) -> List[List[int]]:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and mask.dim() == 2\n",
    "        assert emissions.shape[:2] == mask.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length, batch_size = mask.shape\n",
    "\n",
    "        # Start transition and first emission\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "        history = []\n",
    "\n",
    "        # score is a tensor of size (batch_size, num_tags) where for every batch,\n",
    "        # value at column j stores the score of the best tag sequence so far that ends\n",
    "        # with tag j\n",
    "        # history saves where the best tags candidate transitioned from; this is used\n",
    "        # when we trace back the best tag sequence\n",
    "\n",
    "        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n",
    "        # for every possible next tag\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast viterbi score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emission = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the score of the best\n",
    "            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emission\n",
    "\n",
    "            # Find the maximum score over all possible current tag\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score, indices = next_score.max(dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # and save the index that produces the next score\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "            history.append(indices)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Now, compute the best path for each sample\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        best_tags_list = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            # Find the tag which maximizes the score at the last timestep; this is our best tag\n",
    "            # for the last timestep\n",
    "            _, best_last_tag = score[idx].max(dim=0)\n",
    "            best_tags = [best_last_tag.item()]\n",
    "\n",
    "            # We trace back where the best last tag comes from, append that to our best tag\n",
    "            # sequence, and trace it back again, and so on\n",
    "            for hist in reversed(history[:seq_ends[idx]]):\n",
    "                best_last_tag = hist[idx][best_tags[-1]]\n",
    "                best_tags.append(best_last_tag.item())\n",
    "\n",
    "            # Reverse the order because we start from the last timestep\n",
    "            best_tags.reverse()\n",
    "            best_tags_list.append(best_tags)\n",
    "\n",
    "        return best_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = nn.Linear(512,3)\n",
    "fout = dec(out)\n",
    "fout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 1, 3])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size,seq_length,num_tags = fout.shape\n",
    "\n",
    "emissions = torch.reshape(fout,(seq_length, batch_size, num_tags))\n",
    "emissions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #(seq_length, batch_size)\n",
    "labels_t = torch.unsqueeze(torch.tensor(labels,dtype=torch.long),dim=-1)\n",
    "labels_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-70-c1eff8546114>:236: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:328.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    }
   ],
   "source": [
    "model = CRF(3)\n",
    "loglike = model(emissions,labels_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-576.7137, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP-CRF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim(torch.optim.Optimizer):\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, lr_mul, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.lr_mul = lr_mul\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients with the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        d_model = self.d_model\n",
    "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
    "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
    "\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_steps += 1\n",
    "        lr = self.lr_mul * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipCRF(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.emission_model = LstmAttn(**self.hparams.emission_model)\n",
    "        self.crf = CRF(self.hparams.emission_model['tgt_dim'])\n",
    "\n",
    "    def forward(self,vid,txt):\n",
    "        tgt_emb = self.emission_model(vid.float(),txt.float())\n",
    "        batch_size,seq_length,num_tags = tgt_emb.shape\n",
    "        emissions = torch.reshape(tgt_emb,(seq_length, batch_size, num_tags))\n",
    "        return self.crf.decode(emissions)\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        vid,txt,labels = batch\n",
    "        batch_size,seq_length = labels.shape\n",
    "        labels = torch.reshape(torch.tensor(labels,dtype=torch.long),(seq_length,batch_size))\n",
    "        \n",
    "        tgt_emb = self.emission_model(vid.float(),txt.float())\n",
    "        batch_size,seq_length,num_tags = tgt_emb.shape\n",
    "        emissions = torch.reshape(tgt_emb,(seq_length, batch_size, num_tags))\n",
    "        loglike = self.crf(emissions,labels)\n",
    "        loss = -1*loglike\n",
    "        self.log(\"train_loglikelihood\",loglike,on_step=True)\n",
    "        self.log(\"train_loss\",loss,on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        vid,txt,labels = batch\n",
    "        batch_size,seq_length = labels.shape\n",
    "        labels = torch.reshape(torch.tensor(labels,dtype=torch.long),(seq_length,batch_size))\n",
    "        \n",
    "        tgt_emb = self.emission_model(vid.float(),txt.float())\n",
    "        batch_size,seq_length,num_tags = tgt_emb.shape\n",
    "        emissions = torch.reshape(tgt_emb,(seq_length, batch_size, num_tags))\n",
    "        loglike = self.crf(emissions,labels)\n",
    "        loss = -1*loglike\n",
    "        self.log(\"val_loss\",loss,on_step=False,on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        num_warmup_steps = self.hparams.num_warmup_steps\n",
    "        lr_dim = self.hparams.lr_dim\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr=self.hparams.lr,betas=(0.9, 0.98), eps=1e-9)\n",
    "        #scheduler = ScheduledOptim(optimizer,1,lr_dim,num_warmup_steps)\n",
    "        return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cfg = Namespace(\n",
    "        version = 'clip_crf_vld_ncls',\n",
    "        id = 0,\n",
    "        FEAT_DIR = FEAT_DIR,\n",
    "        RAWFRAME_DIR = RAWFRAME_DIR,\n",
    "        artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/\",\n",
    "        data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/YouCookII/\",\n",
    "        trn_split = 0.8,\n",
    "        mode = 'train',\n",
    "        split = 'training',\n",
    "        loggers = [\"csv\"],\n",
    "        seed = 0,\n",
    "        emission_model = {'bidirectional':True,'input_dim':512,'nheads':1,'lstm_lyrs':1,\\\n",
    "            'lstm_hdim':128,'attn_hdim':64,\\\n",
    "            'attn_dropout':0.0,\n",
    "            'tgt_dim': 17\n",
    "            },\n",
    "        cbs = [\"checkpoint\"],\n",
    "        trainer = {'log_every_n_steps': 1, \n",
    "        'max_epochs': 60},\n",
    "        checkpoint = {\"every_n_epochs\": 1,\n",
    "        \"monitor\": \"val_loss\"},\n",
    "        early_stop = {\"monitor\":\"val_loss\",\"mode\":\"min\",\"patience\":5},\n",
    "        lr = 1e-4,\n",
    "        lr_dim = 200,\n",
    "        num_warmup_steps = 4000,\n",
    "        batch_sz = 1\n",
    "        \n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/common/home/vk405/Projects/Crossmdl/nbs/csvlogs/clip_crf_trn_ncls/metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['val_loss'].dropna().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr-Adam</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loglikelihood</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13514</th>\n",
       "      <td>NaN</td>\n",
       "      <td>847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>668.03894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13515</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13516</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13517</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lr-Adam  step  train_loglikelihood  train_loss  epoch   val_loss\n",
       "13514      NaN   847                  NaN         NaN   52.0  668.03894\n",
       "13515   0.0001   848                  NaN         NaN    NaN        NaN\n",
       "13516   0.0001   848                  NaN         NaN    NaN        NaN\n",
       "13517   0.0001   848                  NaN         NaN    NaN        NaN"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[13514:13518,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['train_loss'].dropna().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = ClipCRF(cfg)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "def get_model(cfg):\n",
    "    hparams = cfg    \n",
    "    PATH = Path(cfg.artifacts_loc)/'ckpts'/cfg.version\n",
    "    ckpt = os.listdir(PATH)[-1]\n",
    "    net = ClipCRF(hparams)\n",
    "    print(f\"loading ckpt:{ckpt}\")\n",
    "    new_model = net.load_from_checkpoint(checkpoint_path=str(PATH/ckpt))\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ckpt:epoch=46-val_loss=547.01.ckpt\n"
     ]
    }
   ],
   "source": [
    "m = get_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.9500e-02, -1.0000e+03,  3.7342e-02, -4.1242e-02, -1.0000e+03,\n",
       "        -1.0000e+03, -1.0000e+03, -1.0000e+03, -1.0000e+03, -1.0000e+03,\n",
       "        -1.0000e+03, -1.0000e+03, -1.0000e+03, -1.0000e+03, -1.0000e+03,\n",
       "        -1.0000e+03, -1.0000e+03], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.crf.transitions[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing:['ukfCQQpZ0k4', 'NK2xHVWojgY', 'mixdagZ-fwI']\n",
      "cwsDQ7M5OTI\n",
      "uf65nfh6X2U\n",
      "segs are not matching:['cwsDQ7M5OTI', 'uf65nfh6X2U']\n"
     ]
    }
   ],
   "source": [
    "d = Dset(cfg.RAWFRAME_DIR,cfg.FEAT_DIR,cfg.split)\n",
    "trn_sz = int(len(d)*cfg.trn_split)\n",
    "val_sz = len(d)-trn_sz\n",
    "trndset,valdset = random_split(d,[trn_sz,val_sz])\n",
    "\n",
    "trnl = DataLoader(trndset,batch_size=1,shuffle=False,num_workers = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid,text,labels  = next(iter(trnl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 500, 512]), torch.Size([1, 9, 512]), torch.Size([1, 500]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid.shape,text.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-be86d3c016ba>:293: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:328.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    }
   ],
   "source": [
    "pred_labels = m(vid,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(labels.detach().cpu().numpy()[0] == pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [0,0,0,1,2,2,2,1,2]\n",
    "isinstance(labels,list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_segs(labels):\n",
    "    if isinstance(labels,list) and not isinstance(labels[0],list):\n",
    "        labels = [labels]\n",
    "    seglis = []\n",
    "    def get_segs(labellis):\n",
    "        segs = []\n",
    "        st,end = None,None\n",
    "        for i,ele in enumerate(labellis):\n",
    "            #print(f\"ele:{ele},st:{st},end:{end}\")\n",
    "            if ele == 0:\n",
    "                if st!=None and end!=None:\n",
    "                    segs.append((st,end))\n",
    "                st,end = None,None\n",
    "            elif ele == 1:\n",
    "                if st != None and end != None:\n",
    "                    segs.append((st,end))\n",
    "                #can start and end at the same token\n",
    "                st = i\n",
    "                end = i\n",
    "                \n",
    "            elif st!=None and ele == 2:\n",
    "                end = i\n",
    "                if i == (len(labellis)-1):segs.append((st,end))\n",
    "        return segs\n",
    "    for labellis in labels:\n",
    "        seg = get_segs(labellis)\n",
    "        seglis.append(seg)\n",
    "    return seglis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 6), (7, 8)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing:[]\n",
      "95WMX64RIBc\n",
      "segs are not matching:['95WMX64RIBc']\n"
     ]
    }
   ],
   "source": [
    "d_test = Dset(cfg.RAWFRAME_DIR,cfg.FEAT_DIR,'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "testl = DataLoader(d_test,batch_size=1,shuffle=False,num_workers = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid,text,labels  = next(iter(testl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = m(vid,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.248"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(labels.detach().cpu().numpy()[0] == pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model,dl):\n",
    "    frame_level_acc = []\n",
    "    labellis = []\n",
    "    predlis = []\n",
    "    model.eval()\n",
    "    for batch in tqdm(dl):\n",
    "        vid,text,labels = batch\n",
    "        pred_labels = model(vid,text)\n",
    "        ground_labels = labels.detach().cpu().numpy()[0]\n",
    "        predlis.append(pred_labels)\n",
    "        labellis.append(ground_labels)\n",
    "        acc = np.mean(ground_labels == pred_labels)\n",
    "        frame_level_acc.append(acc)\n",
    "    return np.mean(frame_level_acc),labellis,predlis\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 416/416 [01:38<00:00,  4.23it/s]\n"
     ]
    }
   ],
   "source": [
    "t_fl_acc,labellis,predlis = inference(m,testl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(predlis[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7398028846153846"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_fl_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsegs = labels_to_segs(labellis[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(147, 239), (254, 270), (282, 329), (333, 354), (361, 411)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsegs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(predlis[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsegs = [labels_to_segs(labellis[i].tolist())[0] for i in range(len(labellis))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "psegs = [get_segs(predlis[i][0]) for i in range(len(predlis))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segs(labellis):\n",
    "    segs = []\n",
    "    st,end = None,None\n",
    "    for i,ele in enumerate(labellis):\n",
    "        #print(f\"ele:{ele},st:{st},end:{end}\")\n",
    "        if ele == 0:\n",
    "            if st != None and end != None:\n",
    "                segs.append((st,end))\n",
    "            st,end = None,None\n",
    "            \n",
    "        elif ele == 2:\n",
    "            if st == None:\n",
    "                st,end = i,i\n",
    "            else:\n",
    "                end = i\n",
    "               \n",
    "            if i == (len(labellis)-1):segs.append((st,end))\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 3), (6, 6)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_segs([0,0,2,2,0,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(149, 240), (273, 412)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_segs(predlis[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(predlis[490][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "949"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labellis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid,txt,labels = batch\n",
    "\n",
    "preds = [[(149, 240), (273, 412)]]\n",
    "labels = [[(147, 239), (254, 270), (282, 329), (333, 354), (361, 411)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[(101, 115), (120, 132), (144, 151), (152, 160), (174, 205), (215, 261), (262, 292), (400, 419)]\n",
      "[]\n",
      "[(65, 101), (120, 141), (190, 207), (209, 230), (244, 272), (277, 278), (311, 321), (339, 344), (365, 368), (425, 447)]\n",
      "[]\n",
      "[(57, 64), (87, 111), (117, 134), (140, 171), (181, 215), (238, 265), (285, 286), (324, 326), (338, 376), (389, 437)]\n",
      "[]\n",
      "[(80, 113), (114, 135), (181, 203), (215, 316), (317, 384), (385, 418)]\n",
      "[]\n",
      "[(44, 69), (83, 127), (170, 175), (200, 206), (223, 269), (284, 290), (376, 407), (410, 430)]\n",
      "[]\n",
      "[(84, 119), (149, 163), (191, 195), (209, 237), (272, 316), (362, 430)]\n",
      "[]\n",
      "[(98, 247), (267, 413), (446, 494)]\n",
      "[]\n",
      "[(30, 86), (89, 97), (109, 164), (200, 268), (309, 334), (342, 358), (359, 368), (390, 398)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5555192307692308, 0.28378418504719577, 0.12492428434348384)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats(psegs,lsegs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds: list of lists corresponding to each video. [[(23,67),(89,102)]]\n",
    "import numpy as np\n",
    "def stats(preds,labels,sz=500):\n",
    "    accs = []\n",
    "    recalls = []\n",
    "    ious = []\n",
    "    for pred,label in zip(preds,labels):\n",
    "        # note: during inference we ensure that intervals are non-overlapping\n",
    "        #pred = sorted(pred,key=lambda x: x[0])\n",
    "        #label = sorted(label,key=lambda x: x[0])\n",
    "        pred_sz = len(pred)\n",
    "        lbl_sz = len(label)\n",
    "        pred_lis = np.zeros(sz)\n",
    "        label_lis = np.zeros(sz)\n",
    "        for ind in range(1,min(pred_sz+1,lbl_sz+1)):\n",
    "            p_st,p_end = pred[ind-1]\n",
    "            l_st,l_end = label[ind-1]\n",
    "            pred_lis[p_st:l_end+1] = ind\n",
    "            label_lis[l_st:l_end+1] = ind\n",
    "        for ind in range(pred_sz+1,len(label)+1):\n",
    "            #set ground-truth indx's\n",
    "            l_st,l_end = label[ind-1]\n",
    "            label_lis[l_st:l_end+1] = ind\n",
    "\n",
    "        acc = np.mean(pred_lis == label_lis)\n",
    "        accs.append(acc)\n",
    "        #calc recall\n",
    "        #calc iou\n",
    "        inter_cnt = 0\n",
    "        union_cnt = 0\n",
    "        recall_score = []\n",
    "        for p_seg,l_seg in zip(pred,label):\n",
    "            ps = set(range(p_seg[0],p_seg[-1]+1))\n",
    "            ls = set(range(l_seg[0],l_seg[-1]+1))\n",
    "            inter_cnt += len(ps.intersection(ls))\n",
    "            union_cnt += len(ps.union(ls))\n",
    "            #if ps.issubset(ls):\n",
    "            recall_score.append(len(ls.intersection(ps))/len(ls))\n",
    "            # correctly assign or fall into ground truth interval\n",
    "            #recall_score += 1\n",
    "        ms = len(label)-len(pred)\n",
    "        #if ms>0:\n",
    "            #for i in range(ms):\n",
    "                #recall_score.append(0.0)\n",
    "        if union_cnt:\n",
    "            ious.append(inter_cnt/union_cnt)\n",
    "            recalls.append(np.mean(recall_score))\n",
    "        else:\n",
    "            print(pred)\n",
    "            print(label)\n",
    "    return np.mean(accs),np.mean(recalls),np.mean(ious)\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt_emb = m.emission_model(vid.float(),txt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def noam_decay(step, num_warmup_steps, dim):\n",
    "  # Eq. (3) of the Transfomer paper. \n",
    "  return (dim ** (-0.5) * min(step ** (-0.5), step * num_warmup_steps**(-1.5)))\n",
    "\n",
    "plt.plot([step + 1 for step in range(10000)], [noam_decay(step + 1, 4000, 200) for step in range(10000)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cfg):\n",
    "    #pl.seed_everything(cfg.seed)\n",
    "    dir = cfg.artifacts_loc\n",
    "    version = str(cfg.version)\n",
    "    logger_list = get_loggers(dir, version,cfg.loggers)\n",
    "    cbs = []\n",
    "    if \"early_stop\" in cfg.cbs:\n",
    "        #? does'nt really work atm\n",
    "        params = cfg.early_stop\n",
    "        earlystopcb = EarlyStopping(**params, min_delta=0.00, verbose=False)\n",
    "        cbs.append(earlystopcb)\n",
    "    if \"checkpoint\" in cfg.cbs:\n",
    "        store_path = dir + \"ckpts/\" + str(cfg.version) + \"/\"\n",
    "        isExist = os.path.exists(store_path)\n",
    "        if not isExist:\n",
    "            os.makedirs(store_path)\n",
    "        fname = \"{epoch}-{val_loss:.2f}\"\n",
    "        params = cfg.checkpoint\n",
    "        checkptcb = ModelCheckpoint(**params, dirpath=store_path, filename=fname)\n",
    "        cbs.append(checkptcb)\n",
    "\n",
    "    #wandb.init(project=\"videoretrieval\", config=cfg)\n",
    "    if cfg.mode == 'train':\n",
    "        d = Dset(cfg.RAWFRAME_DIR,cfg.FEAT_DIR,cfg.split)\n",
    "        trn_sz = int(len(d)*cfg.trn_split)\n",
    "        val_sz = len(d)-trn_sz\n",
    "        trndset,valdset = random_split(d,[trn_sz,val_sz])\n",
    "        batch_sz = cfg.batch_sz\n",
    "        trnl = DataLoader(trndset,batch_size=batch_sz,shuffle=True,num_workers = 5)\n",
    "        vall = DataLoader(valdset,batch_size=batch_sz)\n",
    "        hparams = cfg  \n",
    "        lr_monitor = LearningRateMonitor(logging_interval='step')  \n",
    "        cbs.append(lr_monitor)\n",
    "        net = ClipCRF(hparams)\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger_list,callbacks=cbs,gpus=1,deterministic=True, **cfg.trainer\n",
    "        )\n",
    "        trainer.fit(net, trnl,vall)\n",
    "        return trainer\n",
    "        #trainer.tune(net,train_loader)\n",
    "            \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "FEAT_DIR = pathlib.Path('/common/users/vk405/CLIP_FEAT')\n",
    "RAWFRAME_DIR = pathlib.Path('/common/users/vk405/Youcook/')\n",
    "#early_stop\n",
    "#lstm_lyrs,lstm_hdim,bidirectional,input_dim,nheads,attn_hdim,attn_dropout,tgt_dim\n",
    "cfg = Namespace(\n",
    "    version = 'clip_crf_trn',\n",
    "    id = 0,\n",
    "    FEAT_DIR = FEAT_DIR,\n",
    "    RAWFRAME_DIR = RAWFRAME_DIR,\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/YouCookII/\",\n",
    "    trn_split = 0.8,\n",
    "    mode = 'train',\n",
    "    split = 'training',\n",
    "    loggers = [\"csv\"],\n",
    "    seed = 0,\n",
    "    emission_model = {'bidirectional':True,'input_dim':512,'nheads':1,'lstm_lyrs':1,\\\n",
    "        'lstm_hdim':128,'attn_hdim':64,\\\n",
    "        'attn_dropout':0.0,\n",
    "        'tgt_dim': 3\n",
    "        },\n",
    "    cbs = [\"checkpoint\"],\n",
    "    trainer = {'log_every_n_steps': 1, \n",
    "    'max_epochs': 150},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"train_loss\"},\n",
    "    early_stop = {\"monitor\":\"val_loss\",\"mode\":\"min\",\"patience\":5},\n",
    "    lr = 1e-4,\n",
    "    lr_dim = 200,\n",
    "    num_warmup_steps = 4000,\n",
    "    batch_sz = 1\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing:['ukfCQQpZ0k4', 'NK2xHVWojgY', 'mixdagZ-fwI']\n",
      "cwsDQ7M5OTI\n",
      "uf65nfh6X2U\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loggers/csv_logs.py:57: UserWarning: Experiment logs directory /common/home/vk405/Projects/Crossmdl/nbs/csvlogs/clip_crf exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name           | Type     | Params\n",
      "--------------------------------------------\n",
      "0 | emission_model | LstmAttn | 445 K \n",
      "1 | crf            | CRF      | 15    \n",
      "--------------------------------------------\n",
      "445 K     Trainable params\n",
      "0         Non-trainable params\n",
      "445 K     Total params\n",
      "1.781     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segs are not matching:['cwsDQ7M5OTI', 'uf65nfh6X2U']\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "<ipython-input-42-485a44048b33>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.reshape(torch.tensor(labels,dtype=torch.long),(seq_length,batch_size))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-a9b8ef3433c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-ba80abcb8d9e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         )\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrnl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#trainer.tune(net,train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    738\u001b[0m             )\n\u001b[1;32m    739\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         self._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    741\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \"\"\"\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_bar_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;31m# enable train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mdl_max_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataloader_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mdl_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# store batch level output per dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluation_step_and_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-485a44048b33>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0memissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloglike\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloglike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_t' is not defined"
     ]
    }
   ],
   "source": [
    "#t = run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Crossmdl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
