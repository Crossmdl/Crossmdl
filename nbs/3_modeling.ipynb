{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Sigmoid()\n",
    "# loss = nn.BCELoss()\n",
    "# input = torch.randn(3, requires_grad=True)\n",
    "# target = torch.empty(3).random_(2)\n",
    "# output = loss(m(input), target)\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class tdset(Dataset):\n",
    "#     def __init__(self,ids):\n",
    "#         self.ids = ids\n",
    "#         self.sz = np.random.randint(5,10)\n",
    "#     def __len__(self):\n",
    "#         return len(self.ids)\n",
    "#     def __getitem__(self,idx):\n",
    "#         return np.random.randn(self.sz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = tdset([0,9,8,6,8,4])\n",
    "# dset2 = tdset([0,9,8,6,8,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 50 percentile length is 25 frames.Thus we will have the sequence of video-frames = 25\n",
    "\n",
    "# class TOYMODEL(pl.LightningModule):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters(hparams)\n",
    "#         self.lin = nn.Linear(15,1)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         return self.lin(x)\n",
    "\n",
    "#     def training_step(self,batch,batch_idx):\n",
    "#         out = self.lin(x)\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def get_vids(base_dir,split):\n",
    "    trn_split = base_dir+split\n",
    "    trn_idlst = []\n",
    "    trn_vidlst = []\n",
    "\n",
    "    f = open(trn_split,'r')\n",
    "    for line in f:\n",
    "        id_,vid = line.split('/')\n",
    "        vid = vid.strip('\\n')\n",
    "        trn_idlst.append(id_)\n",
    "        trn_vidlst.append(vid)\n",
    "        #print(vid)\n",
    "        #break\n",
    "    f.close()\n",
    "    return trn_idlst,trn_vidlst\n",
    "\n",
    "    \n",
    "def get_features(data_dir,split='val',feat_dir='/common/users/vk405/feat_csv/'):\n",
    "    #feat_dir = data_dir\n",
    "    splits_dir = data_dir+'splits/'\n",
    "    if split == 'val':\n",
    "        feat_split_dir = feat_dir+'val_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'val_list.txt')  \n",
    "    elif split == 'train':\n",
    "        feat_split_dir = feat_dir+'train_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'train_list.txt') \n",
    "    elif split == 'test':\n",
    "        feat_split_dir = feat_dir+'test_frame_feat_csv/'  \n",
    "        vid_num,vid_name = get_vids(splits_dir,'test_list.txt')\n",
    "    else:\n",
    "        raise NotImplementedError(f'unknown split: {split}')     \n",
    "    feat_list = {}\n",
    "    vid_dtls = []\n",
    "    for num,name in zip(vid_num,vid_name):\n",
    "        feat_loc = os.path.join(feat_split_dir, f'{num}/{name}/0001/')\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if os.path.isdir(feat_loc):\n",
    "            feat_files = feat_loc + os.listdir(feat_loc)[0]\n",
    "            feat_list[name] = feat_files\n",
    "            #feat_list.append(feat_files)\n",
    "            vid_dtls.append((num,name))\n",
    "        else:\n",
    "            print(f\"video : {num}/{name} not found\")\n",
    "    assert len(feat_list) == len(vid_dtls),\"get-features is giving incorrect features\"\n",
    "    return feat_list,vid_dtls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_raw_labels(ids,annotns_file):\n",
    "\n",
    "    label_info = {}\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)\n",
    "        print(annotns.keys())\n",
    "        for _,vidname in ids:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if vidname in annotns['database']:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                duration = annotns['database'][vidname]['duration']\n",
    "                annot = annotns['database'][vidname]['annotations']\n",
    "                labels = []\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for segment_info in annot:\n",
    "                    interval = segment_info['segment']\n",
    "                    sent = segment_info['sentence']\n",
    "                    labels.append((interval,sent,duration))\n",
    "\n",
    "                label_info[vidname] = labels\n",
    "            else:\n",
    "                print(f\"label for {vidname} not present\")\n",
    "    return label_info\n",
    "\n",
    "def regress_labels(raw_labels):\n",
    "    regress_labels = {}\n",
    "    for key in raw_labels:\n",
    "        new_labels = []\n",
    "        for item in raw_labels[key]:\n",
    "            rng,sent,vidlen = item\n",
    "            mid = sum(rng)/2\n",
    "            duration = rng[-1]-rng[0]\n",
    "            mid_pred = (1/vidlen)*mid # location of mid-point w.r.t video length\n",
    "            duration_pred = (1/vidlen)*duration\n",
    "            new_labels.append(([mid_pred,duration_pred],sent))\n",
    "        regress_labels[key] = new_labels\n",
    "    return regress_labels\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LabelEncoder2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,max_len=499):\n",
    "        self.vidlens = []\n",
    "        self.truebounds = []\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def fit(self,raw_labels):\n",
    "        l = []\n",
    "        for key in raw_labels:\n",
    "            vid_len = raw_labels[key][0][-1]\n",
    "            sz = len(raw_labels[key])\n",
    "            for i in range(sz):l.append(vid_len)\n",
    "        self.vidlens = np.array(l)\n",
    "        return self\n",
    "        \n",
    "    def transform(self,raw_labels):\n",
    "        regress_labels = self._regress_labels(raw_labels)\n",
    "        return regress_labels\n",
    "\n",
    "    def decode(self,outputs):\n",
    "        return np.round(outputs*self.max_len)\n",
    "\n",
    "\n",
    "    def _regress_labels(self,raw_labels):\n",
    "        regress_labels = {}\n",
    "        bounds = []\n",
    "        for key in raw_labels:\n",
    "            new_labels = []\n",
    "            for item in raw_labels[key]:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                rng,sent,vidlen = item\n",
    "                new_rng = [rng[0]/self.max_len,rng[-1]/self.max_len]\n",
    "                bounds.append(rng)\n",
    "                new_labels.append((new_rng,sent))\n",
    "            regress_labels[key] = new_labels\n",
    "        self.truebounds = np.array(bounds)\n",
    "        return regress_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_labels(ids,annotns_file):\n",
    "\n",
    "    label_info = {}\n",
    "    with open(annotns_file) as json_file:\n",
    "        annotns = json.load(json_file)\n",
    "        #print(annotns.keys())\n",
    "        for _,vidname in ids:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if vidname in annotns:\n",
    "                #import pdb;pdb.set_trace()\n",
    "                duration = annotns[vidname]['duration']\n",
    "                annot = annotns[vidname]['annotations']\n",
    "                labels = []\n",
    "                #import pdb;pdb.set_trace()\n",
    "                for segment_info in annot:\n",
    "                    interval = segment_info['segment']\n",
    "                    st_end = [interval[0],interval[-1]]\n",
    "                    sent = segment_info['sentence']\n",
    "                    labels.append((st_end,sent,duration))\n",
    "\n",
    "                label_info[vidname] = labels\n",
    "            else:\n",
    "                print(f\"label for {vidname} not present\")\n",
    "    return label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "# Dataset/loader\n",
    "# This is newer version\n",
    "class YoucookDset2(Dataset):\n",
    "    def __init__(self,data_dir='/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\\\n",
    "        ,split='train',use_precomp_emb=True,seqlen=26,framecnt=499,id=0):\n",
    "        self.id = id\n",
    "        self.feat_locs = {}\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        self.use_precomp_emb = use_precomp_emb\n",
    "        self.text_emb = None\n",
    "        self.seqlen = seqlen\n",
    "        self.framecnt = framecnt\n",
    "        if self.split != 'test':\n",
    "            self.annotns_file = data_dir+'annotations/segment_youcookii_annotations_trainval.json'\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Split:{self.split},not yet correctly implemented\")\n",
    "        if self.use_precomp_emb:\n",
    "            self.txt_emb = joblib.load(os.path.join(self.data_dir,'emb.joblib'))\n",
    "\n",
    "        self.feat_locs,vids = get_features(self.data_dir,split=self.split)\n",
    "        assert len(vids) == len(self.feat_locs),\"features are wrong\"\n",
    "        #import pdb;pdb.set_trace()\n",
    "        label_info = get_labels(vids,self.annotns_file)\n",
    "        #self.labelencoder = LabelEncoder2()\n",
    "        self.final_labels = label_info\n",
    "        #self.labelencoder.fit_transform(label_info)\n",
    "        \n",
    "        #regress_labels(label_info)\n",
    "        #(vid_id,seg_id)\n",
    "        self.update_data()\n",
    "\n",
    "                \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def update_data(self,id=None):\n",
    "        self.data = []\n",
    "        #self.vid_len = []\n",
    "        if not id:\n",
    "            id = self.id\n",
    "        starting_pnt = np.arange(id,self.framecnt,self.seqlen)\n",
    "\n",
    "        for key in self.final_labels:\n",
    "            annot_len = len(self.final_labels[key])\n",
    "            if key in self.feat_locs:\n",
    "                file_loc = self.feat_locs[key]\n",
    "                #for stpnt in starting_pnt:\n",
    "                segments = list(zip(repeat(key,annot_len),repeat(file_loc,annot_len),\\\n",
    "                        range(annot_len)))\n",
    "                for seg in segments:\n",
    "                    for stpnt in starting_pnt:\n",
    "                        if stpnt+self.seqlen<=self.framecnt:\n",
    "                            datapnt = seg[:-1]+(stpnt,)+seg[-1:]\n",
    "                            self.data.append(datapnt)\n",
    "                    \n",
    "                #self.data.extend(segments)\n",
    "            else:\n",
    "                print(f\"video:{key} not found\")\n",
    "\n",
    "\n",
    "    def getclass_prob(self,lbl_rng,frame_rng):\n",
    "        lbl_ids = set(np.arange(lbl_rng[0],lbl_rng[-1]+1))\n",
    "        frame_ids = set(np.arange(frame_rng[0],frame_rng[-1]+1))\n",
    "        inter = lbl_ids.intersection(frame_ids)\n",
    "        if len(inter) == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return len(inter)/len(lbl_ids.union(frame_ids))\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if self.use_precomp_emb:\n",
    "            vidname,file_loc,stid,seg_ind = self.data[idx]\n",
    "            #import pdb;pdb.set_trace()\n",
    "            #self.txt_emb[vidname][seg_ind],\n",
    "            txt_info = self.final_labels[vidname][seg_ind]\n",
    "            label_value = self.getclass_prob(txt_info[0],(stid,stid+self.seqlen-1))\n",
    "            #import pdb;pdb.set_trace()\n",
    "            return pd.read_csv(file_loc).values.astype(np.float32)[stid:stid+self.seqlen,:],(self.txt_emb[vidname][seg_ind]).astype(np.float32),\\\n",
    "                label_value\n",
    "            #np.array(self.final_labels[vidname][seg_ind][0],dtype=np.float32)\n",
    "        else:\n",
    "            raise NotImplementedError(\"not yet correctly implemented\")\n",
    "\n",
    "        \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_dir = '/common/home/vk405/Projects/Crossmdl/Data/YouCookII/'\n",
    "\n",
    "\n",
    "\n",
    "# youcookdata = YoucookDset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youcookdata[19][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load dataset and dataloader once every epoch, makes stuff a bit slow but this is a simple approach.\n",
    "\n",
    "\n",
    "# youcookdata[19][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youcookdata.final_labels['sdB8qBlLS2E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youcookdata[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youcookdata[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model utils\n",
    "\n",
    "#!pip install transformers\n",
    "\n",
    "def init_parameters_xavier_uniform(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "def scaled_dot(query, key, mask_key=None):  \n",
    "    score = torch.matmul(query, key.transpose(-2, -1))\n",
    "    score /= math.sqrt(query.size(-1))\n",
    "    if mask_key is not None:\n",
    "        score = score.masked_fill(mask_key, -1e18)  # Represents negative infinity\n",
    "    return score      \n",
    "            \n",
    "def attend(query, key, value, mask_key=None, dropout=None):\n",
    "    # TODO: Implement\n",
    "    # Use scaled_dot, be sure to mask key\n",
    "    #smax = nn.Softmax(-1)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    score = scaled_dot(query,key,mask_key)  \n",
    "    attention = F.softmax(score,dim=-1)\n",
    "    if dropout is not None:#do = nn.Dropout(dropout)\n",
    "        attention = dropout(attention)\n",
    "    answer = torch.matmul(attention,value) \n",
    "    # Convexly combine value embeddings using attention, this should be just a matrix-matrix multiplication.\n",
    "    return answer, attention\n",
    "\n",
    "\n",
    "\n",
    "def split_heads(batch, num_heads):  \n",
    "    (batch_size, length, dim) = batch.size()  # These are the expected batch dimensions.\n",
    "    assert dim % num_heads == 0  # Assert that dimension is divisible by the number of heads.\n",
    "    dim_head = dim // num_heads\n",
    "\n",
    "    # No new memory allocation\n",
    "    splitted = batch.view(batch_size, -1, num_heads, dim_head).transpose(1, 2)  \n",
    "    return splitted  # (batch_size, num_heads, length, dim_head), note that now the last two dimensions are compatible with our attention functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_heads(batch):  \n",
    "    (batch_size, num_heads, length, dim_head) = batch.size()  # These are the expected batch dimensions.\n",
    "\n",
    "    # New memory allocation (reshape), can't avoid.\n",
    "    merged = batch.transpose(1, 2).reshape(batch_size, -1, num_heads * dim_head)\n",
    "    return merged  # (batch_size, length, dim)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "\n",
    "        self.linear_query = nn.Linear(dim, dim)\n",
    "        self.linear_key = nn.Linear(dim, dim)\n",
    "        self.linear_value = nn.Linear(dim, dim)\n",
    "        self.linear_final = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, query, key, value, mask_key=None, layer_cache=None,\n",
    "              memory_attention=False):\n",
    "        \"\"\"\n",
    "        INPUT\n",
    "          query: (batch_size, length_query, dim)\n",
    "          key: (batch_size, length_key, dim)\n",
    "          value: (batch_size, length_key, dim_value)\n",
    "          mask_key: (*, 1, length_key) if queries share the same mask, else\n",
    "                    (*, length_query, length_key)\n",
    "          layer_cache: if not None, stepwise decoding (cache of key/value)\n",
    "          memory_attention: doing memory attention in stepwise decoding?\n",
    "        OUTPUT\n",
    "          answer: (batch_size, length_query, dim_value)\n",
    "          attention: (batch_size, num_heads, length_query, length_key) else\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.linear_query(query)\n",
    "        query = split_heads(query, self.num_heads)  # (batch_size, num_heads, -1, dim_head)\n",
    "\n",
    "        def process_key_value(key, value):  # Only called when necessary.\n",
    "            key = self.linear_key(key)\n",
    "            key = split_heads(key, self.num_heads)\n",
    "            value = self.linear_value(value)\n",
    "            value = split_heads(value, self.num_heads)\n",
    "            return key, value\n",
    "\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if layer_cache is None:\n",
    "            key, value = process_key_value(key, value)\n",
    "        else:\n",
    "            assert query.size(2) == 1  # Stepwise decoding\n",
    "            \n",
    "            if memory_attention:\n",
    "                if layer_cache['memory_key'] is None:  # One-time calculation\n",
    "                    key, value = process_key_value(key, value)\n",
    "                    # (batch_size, num_heads, length_memory, dim)\n",
    "                    layer_cache['memory_key'] = key\n",
    "                    layer_cache['memory_value'] = value\n",
    "\n",
    "                key = layer_cache['memory_key']\n",
    "                value = layer_cache['memory_value']\n",
    "\n",
    "            else:  # Self-attention during decoding\n",
    "                key, value = process_key_value(key, value)\n",
    "                assert key.size(2) == 1 and value.size(2) == 1\n",
    "                \n",
    "                # Append to previous.\n",
    "                if layer_cache['self_key'] is not None:\n",
    "                    key = torch.cat((layer_cache['self_key'], key), dim=2)\n",
    "                    value = torch.cat((layer_cache['self_value'], value), dim=2)\n",
    "                    \n",
    "                 # (batch_size, num_heads, length_decoded, dim)\n",
    "                layer_cache['self_key'] = key  # Recache.\n",
    "                layer_cache['self_value'] = value\n",
    "        # Because we've splitted embeddings into heads, we must also split the mask. \n",
    "        # And because each query uses the same mask for all heads (we don't use different masking for different heads), \n",
    "        # we can specify length 1 for the head dimension.\n",
    "        if mask_key is not None:  \n",
    "            mask_key = mask_key.unsqueeze(1)  # (batch_size, 1, -1, length_key)\n",
    "\n",
    "        answer, attention = attend(query, key, value, mask_key, self.dropout)\n",
    "\n",
    "        answer = merge_heads(answer)  # (batch_size, length_key, dim)\n",
    "        answer = self.linear_final(answer)\n",
    "\n",
    "        return answer, attention\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_hidden, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, dim_hidden)\n",
    "        self.w2 = nn.Linear(dim_hidden, dim)\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.drop1 = nn.Dropout(drop_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(drop_rate)\n",
    "    def forward(self, x):\n",
    "        inter = self.drop1(self.relu(self.w1(self.layer_norm(x))))\n",
    "        output = self.drop2(self.w2(inter))\n",
    "        return output + x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SinusoidalPositioner(nn.Module):\n",
    "    def __init__(self, dim, drop_rate=0.1, length_max=5000):\n",
    "        super().__init__()\n",
    "        frequency = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.) / dim))  # Using different frequency for each dim\n",
    "        positions = torch.arange(0, length_max).unsqueeze(1)\n",
    "        wave = torch.zeros(length_max, dim)\n",
    "        wave[:, 0::2] = torch.sin(frequency * positions)\n",
    "        wave[:, 1::2] = torch.cos(frequency * positions)\n",
    "        self.register_buffer('wave', wave.unsqueeze(0))  # (1, length_max, dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.dim = dim\n",
    "        self.length_max = length_max\n",
    "    def forward(self, x, step=-1):\n",
    "        assert x.size(-2) <= self.length_max\n",
    "\n",
    "        if step < 0:  # Take the corresponding leftmost embeddings.\n",
    "            position_encoding = self.wave[:, :x.size(-2), :]\n",
    "        else:  # Take the embedding at the step.\n",
    "            position_encoding = self.wave[:, step, :]\n",
    "\n",
    "        x = x * math.sqrt(self.dim)\n",
    "        return self.dropout(x + position_encoding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, dim, num_heads, dim_hidden, drop_rate):\n",
    "    super().__init__()\n",
    "    self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "    self.self_attention = MultiHeadAttention(dim, num_heads, drop_rate)\n",
    "    self.drop = nn.Dropout(drop_rate)\n",
    "    self.feedforward = PositionwiseFeedForward(dim, dim_hidden, drop_rate)\n",
    "\n",
    "  def forward(self, source, mask_source=None):\n",
    "    # TODO: Implement\n",
    "    #print(source.shape)\n",
    "    normed = self.layer_norm(source)  \n",
    "    # Apply layer norm on source\n",
    "\n",
    "    attended, attention = self.self_attention(normed,normed,normed,mask_source)\n",
    "    #None, None  # Apply self-attention on normed (be sure to use mask_source).\n",
    "    attended = self.drop(attended) + source  \n",
    "    # Re-write attended by applying dropout and adding a residual connection to source.\n",
    "    return self.feedforward(attended), attention\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self,dim,num_heads,dim_hidden,drop_rate):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.context_attention = MultiHeadAttention(dim, num_heads, drop_rate)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "        self.feedforward = PositionwiseFeedForward(dim, dim_hidden, drop_rate)\n",
    "        \n",
    "    def forward(self,target,memory,layer_cache=None):\n",
    "        \n",
    "        cross_attn_target = self.layer_norm(target)\n",
    "        attended, attention = self.context_attention(cross_attn_target,memory,memory,layer_cache=layer_cache,memory_attention=True)\n",
    "        \n",
    "        attended = target + self.drop(attended)\n",
    "        \n",
    "        return self.feedforward(attended),attention\n",
    "\n",
    "\n",
    "\n",
    "layer_cache = {'memory_key': None, 'memory_value': None, 'self_key': None, 'self_value': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CrossattnModel(pl.LightningModule):\n",
    "    def __init__(self,hparams,dset=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        #self.hparams = hparams\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #self.net= Model(hparams)\n",
    "        #self.hparams  = hparams\n",
    "        self.positioner = SinusoidalPositioner(self.hparams.edim, drop_rate=0., length_max=1000)\n",
    "        self.attn = CrossAttentionLayer(self.hparams.edim,self.hparams.nheads,\\\n",
    "                           self.hparams.attnhdim,self.hparams.dropoutp)\n",
    "        self.wrdcnn =  nn.Conv1d(self.hparams.wrdim, self.hparams.edim, 1, stride=1)\n",
    "        self.vidcnn =  nn.Conv1d(self.hparams.vidim, self.hparams.edim, 1, stride=1)\n",
    "        self.hid_layer = nn.Linear(self.hparams.edim,self.hparams.hdim)\n",
    "        self.out_layer = nn.Linear(self.hparams.hdim,1)\n",
    "        #self.init_parameters_xavier_uniform()\n",
    "        self.dset = dset\n",
    "\n",
    "    def forward(self,x):\n",
    "        #keep this for inference\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "        \n",
    "    def net(self,x):\n",
    "        vid_x,wrd_x = x\n",
    "        #import pdb;pdb.set_trace()\n",
    "        wrd_x = wrd_x.unsqueeze(1).transpose(1,2)\n",
    "        vid_x = vid_x.transpose(1,2)\n",
    "        #print(f\"inside model, wrd_x:{wrd_x.shape},vi\")\n",
    "        tgt = self.wrdcnn(wrd_x.float()).transpose(1,2)\n",
    "        src = self.vidcnn(vid_x.float()).transpose(1,2)\n",
    "        src_posencode = self.positioner(src)\n",
    "        #for i in range(self.hparams.lyrs):\n",
    "        attended,attn_score = self.attn(tgt,src_posencode)\n",
    "            #tgt = \n",
    "        out = F.sigmoid(self.out_layer(F.relu(self.hid_layer(F.relu(attended)))))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        #for tranining\n",
    "        vid_feat,wrd_feat,labels = batch\n",
    "        x_hat = self.net((vid_feat.float(),wrd_feat.float()))\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #loss = nn.BCELoss()\n",
    "        loss = F.binary_cross_entropy(x_hat.squeeze().float(), labels.squeeze().float())\n",
    "        #print(f\"inside train step, loss:{loss}\")\n",
    "        self.log(\"train_loss\",loss,on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        #for validation\n",
    "        vid_feat,wrd_feat,labels = batch\n",
    "        x_hat = self.net((vid_feat.float(),wrd_feat.float()))\n",
    "        #import pdb;pdb.set_trace()\n",
    "        loss = F.binary_cross_entropy(x_hat.squeeze().float(), labels.squeeze().float())\n",
    "        #print(f\"inside train step, loss:{loss}\")\n",
    "        self.log(\"val_loss\",loss,on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr if 'lr' in self.hparams else 1e-3\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "    def init_parameters_xavier_uniform(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ind = self.current_epoch % self.hparams.seqlen\n",
    "        self.dset.id = ind\n",
    "        self.dset.update_data()\n",
    "        print(f\"train dataloader called with ind:{ind}\")\n",
    "        return DataLoader(self.dset,self.hparams.batch_size,shuffle=True,num_workers=10)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seqlen=25,framecnt=500,id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = '/common/home/vk405/Projects/Crossmdl/nbs/lightning_logs/'\n",
    "\n",
    "# from pytorch_lightning.loggers import CSVLogger\n",
    "# version = '3'\n",
    "# csvlogger = CSVLogger(log_dir,version)\n",
    "# #trainer = pl.Trainer(logger= csvlogger,\\\n",
    "#     #s=1gpu,max_epochs=15)\n",
    "# #reload_dataloaders_every_epoch=True\n",
    "# #youcoodvld_dl = DataLoader(youcookvld2,batch_size=64,shuffle=False,num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #trainer.fit(model,youcookdl)\n",
    "# #model = CrossattnModel(hparams)\n",
    "\n",
    "# youcookdata = YoucookDset2()\n",
    "\n",
    "\n",
    "# import torch.utils.data as data_utils\n",
    "# #indices = torch.arange(1000)\n",
    "# #youcookdatasubset = data_utils.Subset(youcookdata, indices)\n",
    "# youcookdl = DataLoader(youcookdata,batch_size=128,shuffle=True,num_workers=20)\n",
    "\n",
    "\n",
    "\n",
    "# model = CrossattnModel(hparams,youcookdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.fit(model,youcookdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # similar, but with a fixed 10 batches no matter the size of the dataset\n",
    "# #trainer.fit(model,youcookdl)\n",
    "# #sanity check\n",
    "# version = '5'\n",
    "# log_dir = '/common/home/vk405/Projects/Crossmdl/nbs/lightning_logs/'\n",
    "# csvlogger = CSVLogger(log_dir,version)\n",
    "# trainer = pl.Trainer(logger=csvlogger,gpus=1,accelerator=\"gpu\",auto_scale_batch_size=\"binsearch\")\n",
    "# trainer.tune(model)\n",
    "\n",
    "# #trainer.fit(model,youcookdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4096 works\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "wandb_logger = lambda dir, version: WandbLogger(\n",
    "    name=\"wandb\", save_dir=dir, version=version\n",
    ")\n",
    "csvlogger = lambda dir, version: CSVLogger(dir, name=\"csvlogs\", version=version)\n",
    "tblogger = lambda dir, version: TensorBoardLogger(dir, name=\"tblogs\", version=version)\n",
    "\n",
    "def get_loggers(dir,version,lis=[\"csv\"]):\n",
    "    lgrs = []\n",
    "    if \"wandb\" in lis:\n",
    "        lgrs.append(wandb_logger(dir, version))\n",
    "    if \"csv\" in lis:\n",
    "        lgrs.append(csvlogger(dir, version))\n",
    "    if \"tb\" in lis:\n",
    "        lgrs.append(tblogger(dir, version))\n",
    "    return lgrs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cfg):\n",
    "    pl.seed_everything(cfg.seed)\n",
    "    dir = cfg.artifacts_loc\n",
    "    version = str(cfg.version)\n",
    "    logger_list = get_loggers(dir, version,cfg.loggers)\n",
    "    cbs = []\n",
    "    if \"early_stop\" in cfg.cbs:\n",
    "        #? does'nt really work atm\n",
    "        params = cfg.model.cbs.early_stop\n",
    "        earlystopcb = EarlyStopping(**params, min_delta=0.00, verbose=False)\n",
    "        cbs.append(earlystopcb)\n",
    "    if \"checkpoint\" in cfg.cbs:\n",
    "        store_path = dir + \"ckpts/\" + str(cfg.version) + \"/\"\n",
    "        isExist = os.path.exists(store_path)\n",
    "        if not isExist:\n",
    "            os.makedirs(store_path)\n",
    "        fname = \"{epoch}-{train_loss:.2f}\"\n",
    "        params = cfg.checkpoint\n",
    "        checkptcb = ModelCheckpoint(**params, dirpath=store_path, filename=fname)\n",
    "        cbs.append(checkptcb)\n",
    "\n",
    "    wandb.init(project=\"videoretrieval\", config=cfg)\n",
    "    if cfg.mode == 'train':\n",
    "        youcookdata = YoucookDset2(data_dir=cfg.data_dir,split=cfg.mode,\\\n",
    "               use_precomp_emb=cfg.use_precomp_emb,\\\n",
    "                   seqlen=cfg.seqlen,framecnt=cfg.framecnt,id=cfg.id)\n",
    "        train_loader = DataLoader(youcookdata,\\\n",
    "            batch_size=cfg.batch_size,shuffle=True,num_workers=4,pin_memory=True)\n",
    "        hparams = cfg    \n",
    "        net = CrossattnModel(hparams,youcookdata)\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger_list,callbacks=cbs, gpus=1,deterministic=True, **cfg.trainer\n",
    "        )\n",
    "        trainer.fit(net, train_loader)\n",
    "        return trainer\n",
    "        #trainer.tune(net,train_loader)\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "cfg = Namespace(\n",
    "    version = 'trail_again',\n",
    "    id = 0,\n",
    "    artifacts_loc = \"/common/home/vk405/Projects/Crossmdl/nbs/\",\n",
    "    data_dir = \"/common/home/vk405/Projects/Crossmdl/Data/YouCookII/\",\n",
    "    mode = 'train',\n",
    "    loggers = [\"csv\",\"wandb\"],\n",
    "    seed = 0,\n",
    "    cbs = [\"checkpoint\"],\n",
    "    trainer = {'log_every_n_steps': 1,\n",
    "    'max_epochs': 10},\n",
    "    checkpoint = {\"every_n_epochs\": 1,\n",
    "    \"monitor\": \"train_loss\"},\n",
    "\n",
    "    use_precomp_emb = True,\n",
    "    edim = 100,\n",
    "    attnhdim = 50,\n",
    "    nheads = 10,\n",
    "    wrdim = 768,\n",
    "    vidim = 512,\n",
    "    hdim = 30,\n",
    "    dropoutp=0.0,\n",
    "    seqlen=26,\n",
    "    framecnt=499,\n",
    "    batch_size=512\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvin136\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/vin136/videoretrieval/runs/35hscjy8\" target=\"_blank\">brisk-snow-14</a></strong> to <a href=\"https://wandb.ai/vin136/videoretrieval\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:122: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/loggers/csv_logs.py:57: UserWarning: Experiment logs directory /common/home/vk405/Projects/Crossmdl/nbs/csvlogs/trail_again exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name       | Type                 | Params\n",
      "----------------------------------------------------\n",
      "0 | positioner | SinusoidalPositioner | 0     \n",
      "1 | attn       | CrossAttentionLayer  | 51.0 K\n",
      "2 | wrdcnn     | Conv1d               | 76.9 K\n",
      "3 | vidcnn     | Conv1d               | 51.3 K\n",
      "4 | hid_layer  | Linear               | 3.0 K \n",
      "5 | out_layer  | Linear               | 31    \n",
      "----------------------------------------------------\n",
      "182 K     Trainable params\n",
      "0         Non-trainable params\n",
      "182 K     Total params\n",
      "0.729     Total estimated model params size (MB)\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /common/home/vk405/Projects/Crossmdl/nbs/ckpts/trail_again exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 3/384 [00:30<1:05:15, 10.28s/it, loss=0.527, v_num=gain]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/.local/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|â–         | 7/384 [01:01<55:03,  8.76s/it, loss=0.338, v_num=gain]  "
     ]
    }
   ],
   "source": [
    "traned_model = run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "youcookdata = YoucookDset2(data_dir=cfg.data_dir,split=cfg.mode,\\\n",
    "               use_precomp_emb=cfg.use_precomp_emb,\\\n",
    "                   seqlen=cfg.seqlen,framecnt=cfg.framecnt,id=cfg.id)\n",
    "net = CrossattnModel(cfg,youcookdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/common/home/vk405/Projects/Crossmdl/nbs/ckpts/trail_again/epoch=0-train_loss=0.18.ckpt'\n",
    "\n",
    "#net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "new_model = net.load_from_checkpoint(checkpoint_path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "youcookdata = YoucookDset2(data_dir=cfg.data_dir,split=cfg.mode,\\\n",
    "               use_precomp_emb=cfg.use_precomp_emb,\\\n",
    "                   seqlen=cfg.seqlen,framecnt=cfg.framecnt,id=cfg.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made a blunder-> dataset is not balanced.\n",
    "best_fits = []\n",
    "labels = []\n",
    "max_len = 10\n",
    "for a in youcookdata:\n",
    "    vid_em,txt_emb,label = a\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #print(label)\n",
    "    if label>=0.6:\n",
    "        best_fits.append((vid_em,txt_emb))\n",
    "        labels.append(label)\n",
    "    if len(labels)>max_len:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(youcookdata,\\\n",
    "            batch_size=cfg.batch_size,shuffle=True,num_workers=4,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 26, 512])\n",
      "torch.Size([512, 768])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26, 512])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampl_inpt[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/.local/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "for i in range(len(labels)):\n",
    "    sampl_inpt = (torch.tensor(best_fits[i][0]).unsqueeze(axis=0),torch.tensor(best_fits[i][1]).unsqueeze(0))\n",
    "    pred_sampl = net(sampl_inpt)\n",
    "    out = pred_sampl.detach().item()\n",
    "    preds_list.append(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9124129414558411,\n",
       " 0.8083134889602661,\n",
       " 0.9570930004119873,\n",
       " 0.9850350618362427,\n",
       " 0.36355796456336975,\n",
       " 0.9555460810661316,\n",
       " 0.6632380485534668,\n",
       " 0.7027997374534607,\n",
       " 0.9942954182624817,\n",
       " 0.9886470437049866,\n",
       " 0.9781237244606018]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made a blunder-> dataset is not balanced.\n",
    "best_fits = []\n",
    "labels = []\n",
    "max_len = 10\n",
    "for a in youcookdata:\n",
    "    vid_em,txt_emb,label = a\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #print(label)\n",
    "    if label<=0.1:\n",
    "        best_fits.append((vid_em,txt_emb))\n",
    "        labels.append(label)\n",
    "    if len(labels)>max_len:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/vk405/.local/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "for i in range(len(labels)):\n",
    "    sampl_inpt = (torch.tensor(best_fits[i][0]).unsqueeze(axis=0),torch.tensor(best_fits[i][1]).unsqueeze(0))\n",
    "    pred_sampl = net(sampl_inpt)\n",
    "    out = pred_sampl.detach().item()\n",
    "    preds_list.append(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7997788190841675,\n",
       " 0.9861284494400024,\n",
       " 0.9484415650367737,\n",
       " 0.9937611222267151,\n",
       " 0.9359134435653687,\n",
       " 0.911560595035553,\n",
       " 0.9371639490127563,\n",
       " 0.9650371074676514,\n",
       " 0.9784882068634033,\n",
       " 0.9068688750267029,\n",
       " 0.9708224534988403]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9124129414558411"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sampl.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made a blunder-> dataset is not balanced.\n",
    "best_fits = []\n",
    "labels = []\n",
    "max_len = 100\n",
    "preds_list = []\n",
    "for a in youcookdata:\n",
    "    vid_em,txt_emb,label = a\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #print(label)\n",
    "    sampl_inpt = (torch.tensor(vid_em).unsqueeze(axis=0),torch.tensor(txt_emb).unsqueeze(0))\n",
    "    pred_sampl = net(sampl_inpt)\n",
    "    out = pred_sampl.detach().item()\n",
    "    preds_list.append(out)\n",
    "    labels.append(label)\n",
    "    if len(labels)>max_len:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2000/196403 [02:11<3:33:20, 15.19it/s]\n"
     ]
    }
   ],
   "source": [
    "#get a label distribution\n",
    "\n",
    "from tqdm import tqdm\n",
    "# I made a blunder-> dataset is not balanced.\n",
    "#best_fits = []\n",
    "labels = []\n",
    "max_len = 2000\n",
    "for a in tqdm(youcookdata):\n",
    "    vid_em,txt_emb,label = a\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #print(label)\n",
    "    #best_fits.append((vid_em,txt_emb))\n",
    "    labels.append(label)\n",
    "    if len(labels)>max_len:\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'percentile_ind':np.linspace(0,100,25),\n",
    " 'percentile':[np.percentile(labels,p) for p in np.linspace(0,100,25)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentile_ind</th>\n",
       "      <th>percentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>45.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>58.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>62.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>70.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>79.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>83.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>87.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>91.666667</td>\n",
       "      <td>0.087995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>95.833333</td>\n",
       "      <td>0.353480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    percentile_ind  percentile\n",
       "0         0.000000    0.000000\n",
       "1         4.166667    0.000000\n",
       "2         8.333333    0.000000\n",
       "3        12.500000    0.000000\n",
       "4        16.666667    0.000000\n",
       "5        20.833333    0.000000\n",
       "6        25.000000    0.000000\n",
       "7        29.166667    0.000000\n",
       "8        33.333333    0.000000\n",
       "9        37.500000    0.000000\n",
       "10       41.666667    0.000000\n",
       "11       45.833333    0.000000\n",
       "12       50.000000    0.000000\n",
       "13       54.166667    0.000000\n",
       "14       58.333333    0.000000\n",
       "15       62.500000    0.000000\n",
       "16       66.666667    0.000000\n",
       "17       70.833333    0.000000\n",
       "18       75.000000    0.000000\n",
       "19       79.166667    0.000000\n",
       "20       83.333333    0.000000\n",
       "21       87.500000    0.000000\n",
       "22       91.666667    0.087995\n",
       "23       95.833333    0.353480\n",
       "24      100.000000    0.962963"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distribution is highly skewed\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19230769230769232,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.423728813559322,\n",
       " 0.4482758620689655,\n",
       " 0.09090909090909091,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7307692307692307,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.26666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.23076923076923078,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14583333333333334,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.38461538461538464,\n",
       " 0.3170731707317073,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.021739130434782608,\n",
       " 0.7407407407407407,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6923076923076923,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8666666666666667,\n",
       " 0.07692307692307693,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06578947368421052,\n",
       " 0.4727272727272727,\n",
       " 0.42105263157894735,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.031746031746031744,\n",
       " 0.6666666666666666,\n",
       " 0.2037037037037037,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.24,\n",
       " 0.631578947368421,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6923076923076923,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10526315789473684,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.38636363636363635,\n",
       " 0.4186046511627907,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11627906976744186,\n",
       " 0.5483870967741935,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2571428571428571,\n",
       " 0.2571428571428571,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06818181818181818,\n",
       " 0.6206896551724138,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04081632653061224,\n",
       " 0.8214285714285714,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.696969696969697,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6923076923076923,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03636363636363636,\n",
       " 0.8387096774193549,\n",
       " 0.05555555555555555,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4857142857142857,\n",
       " 0.20930232558139536,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19230769230769232,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08653846153846154,\n",
       " 0.2988505747126437,\n",
       " 0.2988505747126437,\n",
       " 0.2988505747126437,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07692307692307693,\n",
       " 0.4482758620689655,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8461538461538461,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.22727272727272727,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.22727272727272727,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03333333333333333,\n",
       " 0.14814814814814814,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.34615384615384615,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5769230769230769,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3076923076923077,\n",
       " 0.6190476190476191,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.38461538461538464,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.13333333333333333,\n",
       " 0.13333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5384615384615384,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5384615384615384,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.4864864864864865,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02127659574468085,\n",
       " 0.7777777777777778,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.45714285714285713,\n",
       " 0.21428571428571427,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6923076923076923,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9230769230769231,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2641509433962264,\n",
       " 0.6341463414634146,\n",
       " 0.015151515151515152,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.058823529411764705,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3076923076923077,\n",
       " 0.6190476190476191,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7647058823529411,\n",
       " 0.15384615384615385,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2692307692307692,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mytrainer = run(cfg)\n",
    "#'epoch=0-train_loss=0.18.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.seed_everything(cfg.seed)\n",
    "# dir = cfg.artifacts_loc\n",
    "# version = str(cfg.version)\n",
    "# logger_list = get_loggers(dir, version,cfg.loggers)\n",
    "\n",
    "# youcookdata = YoucookDset2(data_dir=cfg.data_dir,split=cfg.mode,\\\n",
    "#                use_precomp_emb=cfg.use_precomp_emb,\\\n",
    "#                    seqlen=cfg.seqlen,framecnt=cfg.framecnt,id=cfg.id)\n",
    "\n",
    "# train_loader = DataLoader(youcookdata,\\\n",
    "# batch_size=cfg.batch_size,shuffle=True,num_workers=4,pin_memory=True)\n",
    "# hparams = cfg    \n",
    "# net = CrossattnModel(hparams,youcookdata)\n",
    "# trainer = pl.Trainer(\n",
    "# logger=logger_list, auto_lr_find=True,callbacks=[], gpus=1,deterministic=True, **cfg.trainer\n",
    "# )\n",
    "# trainer.fit(net, train_loader)\n",
    "# #trainer.tune(net,train_loader)\n",
    "\n",
    "# # Run learning rate finder\n",
    "# lr_finder = trainer.tuner.lr_find(net,train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Results can be found in\n",
    "# lr_finder.results\n",
    "\n",
    "# # Plot with\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_finder.results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
