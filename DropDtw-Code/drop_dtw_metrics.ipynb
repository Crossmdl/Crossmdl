{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d55899f3-041f-45d4-a56f-8dcb491a540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: joblib in /usr/lib/anaconda3/lib/python3.7/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66ad2e13-3420-4aca-8910-811ce1f1dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468514e4-c631-4082-beda-25d4b43aae5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 512)\n"
     ]
    }
   ],
   "source": [
    "filename=\"txt_pOWe4zB-E-4.joblib\"\n",
    "txt=joblib.load(filename, mmap_mode=None)\n",
    "print(txt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b8edf5-b6bf-4388-a3db-fdab0e703576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 512)\n"
     ]
    }
   ],
   "source": [
    "vid_filename=\"vid_pOWe4zB-E-4.joblib\"\n",
    "vid=joblib.load(vid_filename, mmap_mode=None)\n",
    "print(vid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e307c0f7-e7d0-4b25-96d7-1d05611fc0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 512)\n"
     ]
    }
   ],
   "source": [
    "filename=\"txt_pq2PcaRnzc4.joblib\"\n",
    "txt2=joblib.load(filename, mmap_mode=None)\n",
    "print(txt2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74767258-35c7-40e7-819e-88f2166416f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 512])\n"
     ]
    }
   ],
   "source": [
    "vid_filename=\"vid_pq2PcaRnzc4.joblib\"\n",
    "vid2=joblib.load(vid_filename, mmap_mode=None)\n",
    "vid2=torch.tensor(vid2)\n",
    "print(vid2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16fbfc15-a43e-4cae-bf43-85f011301e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import compute_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9be1e52-c10c-42e0-bb69-e627c9e0607f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lmdb in /common/home/ps1169/.local/lib/python3.7/site-packages (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cb8130e-f1c6-4ebe-ad89-79eb45dc8455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_steps\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a0ad48ce2101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d={'num_steps':[9,14],'frame_features':[vid,vid2],'step_features':[txt,txt2]}\n",
    "dataset=pd.DataFrame(d)\n",
    "for sample in dataset:\n",
    "    print(sample)\n",
    "    print(sample['num_steps'])\n",
    "    print(sample['frame_features'].shape)\n",
    "    print(sample['step_features'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "321f2257-8f4e-4a53-966e-3a9e790e9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import framewise_eval,compute_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74770654-9215-4cf3-b07c-482fbe482693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, dataset,name,distance,dp_algo,drop_cost,keep_percentile,use_unlabeled):\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.distance = distance\n",
    "        self.dp_algo = dp_algo\n",
    "        self.drop_cost = drop_cost\n",
    "        self.keep_percentile = keep_percentile\n",
    "        self.use_unlabeled=use_unlabeled\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a370da5-3706-4211-a90c-7355210bb6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=Config('YouCook2','','cos','DropDTW','logit',0.3,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "471bf3cd-89e8-446b-a4c2-7a3634df782e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cos'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "695ef5dc-7bc1-4a36-8864-7cc35b149e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [02:16<00:00,  8.71it/s]\n",
      "100%|██████████| 1192/1192 [00:34<00:00, 34.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38.847147651006644,\n",
       " 9.65700773103205,\n",
       " 44.0263422818792,\n",
       " 12.285798828504483,\n",
       " 0.2713851498046027)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=None\n",
    "gamma=1\n",
    "compute_all_metrics(dataset,model,gamma,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ac312-77e7-4d89-aa71-ddee9c578ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "return accuracy_std * 100, iou_std * 100, accuracy_dtw * 100, iou_dtw * 100, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48cd09c7-b4b4-4f83-9aea-eabf682a1155",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'drop_cost1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-443137498a27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_cost1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'drop_cost1'"
     ]
    }
   ],
   "source": [
    "config.drop_cost1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05cc925c-9d6c-4ad2-a1c4-0fc5fca34a27",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-3dab9a9635ca>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-3dab9a9635ca>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    name: name of the example\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "name: name of the example\n",
    "cls: class (or activity) of the video\n",
    "cls_name: name of the class (or activity)\n",
    "num_frames: Number of frames N\n",
    "frame_features: features from all N frames in the video, has size [N, d]\n",
    "num_steps: Number of steps K\n",
    "step_ids: ids of steps that happen in this video, has len K\n",
    "step_features: Feature matrix of size [K, d], where d\n",
    "step_starts: start of each step (in frames)\n",
    "step_ends: end of each step (in frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20e8a3ee-534b-4ba2-98c0-e9fbc59aa0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('youcookii_annotations_trainval.json')\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "data=data['database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8204092a-7159-4c4b-afe3-57fe2c880105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(id,data,mypath):\n",
    "    final_dict={}\n",
    "    step_ids=[]\n",
    "    step_starts_sec=[]\n",
    "    step_ends_sec=[]\n",
    "    duration=data['duration']\n",
    "    txt_filename=mypath+'text/txt_'+str(id)+'.joblib'\n",
    "    video_filename=mypath+'video/vid_'+str(id)+'.joblib'\n",
    "    text_features=joblib.load(txt_filename, mmap_mode=None)\n",
    "    text_features=torch.tensor(text_features)\n",
    "    video_features=joblib.load(video_filename, mmap_mode=None)\n",
    "    video_features=torch.tensor(video_features)\n",
    "    \n",
    "    frames=len(video_features)\n",
    "    \n",
    "    final_dict['name']=id\n",
    "    final_dict['cls']=data['recipe_type']\n",
    "    final_dict['cls_name']=data['recipe_type']\n",
    "    final_dict['num_frames']=torch.tensor(frames)\n",
    "    final_dict['frame_features']=video_features\n",
    "    final_dict['num_steps']=torch.tensor(len(text_features))\n",
    "    final_dict['step_features']=text_features\n",
    "    \n",
    "    for val in data['annotations']:\n",
    "        step_ids.append(val['id'])\n",
    "        step_starts_sec.append(val['segment'][0])\n",
    "        step_ends_sec.append(val['segment'][1])\n",
    "        \n",
    "    final_dict['step_ids']=torch.tensor(step_ids)\n",
    "    final_dict['step_starts_sec']=torch.tensor(step_starts_sec)\n",
    "    final_dict['step_ends_sec']=torch.tensor(step_ends_sec)\n",
    "    final_dict['step_ends'] = np.array(\n",
    "    [int((s*frames)//duration) for s in step_ends_sec]\n",
    "    )\n",
    "    final_dict['step_starts'] = np.array(\n",
    "    [int((s*frames)//duration) for s in step_starts_sec]\n",
    "    )\n",
    "    \n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9a3f1a4-2b63-4168-aee9-ed3d8df7bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "path='./CLIP_FEAT/training/'\n",
    "dataset.append(getData('pOWe4zB-E-4',data['pOWe4zB-E-4'],path))\n",
    "# dataset.append(getData('pq2PcaRnzc4',data['pq2PcaRnzc4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cda4ad0-1c67-45b6-81db-19dd4954b6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'pOWe4zB-E-4',\n",
       "  'cls': '425',\n",
       "  'cls_name': '425',\n",
       "  'num_frames': tensor(500),\n",
       "  'frame_features': tensor([[-0.1498, -0.2634, -0.5659,  ...,  0.7729, -0.3650, -0.2932],\n",
       "          [-0.2961, -0.0937,  0.2305,  ...,  1.1836,  0.0320, -0.0127],\n",
       "          [-0.8257, -0.3501,  0.0958,  ...,  1.0938, -0.2103,  0.1567],\n",
       "          ...,\n",
       "          [-0.4021, -0.0199, -0.1875,  ...,  1.2236, -0.1443,  0.3391],\n",
       "          [-0.4875,  0.0098, -0.2311,  ...,  1.1357, -0.1349,  0.3179],\n",
       "          [-0.1498, -0.2634, -0.5659,  ...,  0.7729, -0.3650, -0.2932]]),\n",
       "  'num_steps': tensor(9),\n",
       "  'step_features': tensor([[-0.1165,  0.0089,  0.0501,  ...,  0.8604, -0.2446,  0.0275],\n",
       "          [-0.2649,  0.0190,  0.0769,  ...,  0.5029, -0.0727, -0.5811],\n",
       "          [-0.3186, -0.3428,  0.0249,  ...,  0.4702, -0.2244,  0.1191],\n",
       "          ...,\n",
       "          [-0.0118, -0.3308,  0.2417,  ...,  0.3335, -0.3528, -0.0211],\n",
       "          [-0.1559, -0.0925,  0.2240,  ...,  0.6509, -0.5757, -0.1139],\n",
       "          [-0.1427, -0.4126,  0.3643,  ...,  0.4512, -0.5518,  0.0528]]),\n",
       "  'step_ids': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       "  'step_starts_sec': tensor([  5,  78, 130, 155, 190, 218, 228, 244, 256]),\n",
       "  'step_ends_sec': tensor([ 78, 114, 150, 162, 205, 228, 235, 256, 263]),\n",
       "  'step_ends': array([141, 206, 272, 294, 372, 413, 426, 464, 477]),\n",
       "  'step_starts': array([  9, 141, 235, 281, 344, 395, 413, 442, 464])}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "917a8048-f6ea-434c-9619-c9eaf45562c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a34023f7-f6e4-4f7d-ad5d-b6272ada8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath='./CLIP_FEAT/training/video'\n",
    "# onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "# onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea2ba98b-4548-4657-a40a-04132ad21260",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "path='./CLIP_FEAT/training/'\n",
    "for f in listdir(mypath):\n",
    "    id=f[4:15]\n",
    "    if(id!='hJzPVZf6JVo'):\n",
    "        dataset.append(getData(id,data[id],path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f14a660-9a55-41ec-b154-d20de16264a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e59b9d8-bd8b-4eef-8d85-892963378034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a9e52c5b-b31d-43f8-b1d1-3a9e93b7f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95dad241-aa4e-4963-bea5-16cbb17a7628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc652cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    def __init__(self, dataset,name,distance,dp_algo,drop_cost,keep_percentile,use_unlabeled):\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.distance = distance\n",
    "        self.dp_algo = dp_algo\n",
    "        self.drop_cost = drop_cost\n",
    "        self.keep_percentile = keep_percentile\n",
    "        self.use_unlabeled=use_unlabeled\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea01f7b9-278a-4f5b-8c91-556e0260535b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EmbeddingsMapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-35d12c98f144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = EmbeddingsMapping(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearnable_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_cost\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'learn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtext_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     batchnorm=args.batchnorm)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EmbeddingsMapping' is not defined"
     ]
    }
   ],
   "source": [
    "# data = dataset\n",
    "# model = EmbeddingsMapping(\n",
    "#     d=512, learnable_drop=(args.drop_cost == 'learn'), video_layers=args.video_layers,\n",
    "#     text_layers=args.text_layers, normalization_dataset=data.train_dataset,\n",
    "#     batchnorm=args.batchnorm)\n",
    "\n",
    "# # load drop costs from a pre-trained model\n",
    "# if args.pretrained_drop:\n",
    "#     # assumes that the model with the same name has been already trained\n",
    "#     # this retraines the model, but uses drop_mapping intialization from the previous training\n",
    "#     from glob import glob\n",
    "#     weights_path = glob(os.path.join(WEIGHTS_PATH, args.name, f\"weights-epoch=*.ckpt\"))[0]\n",
    "#     state_dict = {k[6:]: v for k, v in torch.load(weights_path, map_location=device)['state_dict'].items()\n",
    "#                   if k.startswith('model.drop_mapping')}\n",
    "#     model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# train_module = TrainModule(model, data)\n",
    "\n",
    "# checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "#     monitor='Metrics/Recall',\n",
    "#     dirpath=os.path.join(PROJECT_PATH, 'weights', args.name),\n",
    "#     filename='weights-{epoch:02d}',\n",
    "#     save_top_k=1,\n",
    "#     mode='max',\n",
    "# )\n",
    "# vis_callback = VisualizationCallback()\n",
    "# logger = pl.loggers.TensorBoardLogger('tb_logs', args.name)\n",
    "\n",
    "# trainer = pl.Trainer(gpus=1, callbacks=[checkpoint_callback, vis_callback],\n",
    "#                      max_epochs=args.epochs, logger=logger)\n",
    "\n",
    "# trainer.fit(train_module, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dafd7f6f-1e3e-4f43-a406-9fb408ceac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7121f70-a049-45a1-b323-193a57e45dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle', 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04040a76-4260-40e5-8832-383dd7a4b4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24d86b28-cbd1-4424-abe6-05fda2b9ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3d feautres\n",
    "def getDatas3d(id,data,mypath):\n",
    "    final_dict={}\n",
    "    step_ids=[]\n",
    "    step_starts_sec=[]\n",
    "    step_ends_sec=[]\n",
    "    duration=data['duration']\n",
    "    txt_filename=mypath+'/text_embeddings/'+str(id)+'.pt'\n",
    "    video_filename=mypath+'s3d_features/'+str(id)+'.npy'\n",
    "    text_feats=torch.load(txt_filename)\n",
    "    text_features=text_feats[\"text_embedding\"].to(dtype=float)\n",
    "    video_features=np.load(video_filename)\n",
    "    video_features=torch.tensor(video_features)\n",
    "    \n",
    "    frames=len(video_features)\n",
    "    \n",
    "    final_dict['name']=id\n",
    "    final_dict['cls']=data['recipe_type']\n",
    "    final_dict['cls_name']=data['recipe_type']\n",
    "    final_dict['num_frames']=torch.tensor(frames)\n",
    "    final_dict['frame_features']=video_features\n",
    "    final_dict['num_steps']=torch.tensor(len(text_features))\n",
    "    final_dict['step_features']=text_features\n",
    "    \n",
    "    for val in data['annotations']:\n",
    "        step_ids.append(val['id'])\n",
    "        step_starts_sec.append(val['segment'][0])\n",
    "        step_ends_sec.append(val['segment'][1])\n",
    "        \n",
    "    final_dict['step_ids']=torch.tensor(step_ids)\n",
    "    final_dict['step_starts_sec']=torch.tensor(step_starts_sec)\n",
    "    final_dict['step_ends_sec']=torch.tensor(step_ends_sec)\n",
    "    final_dict['step_ends'] = np.array(\n",
    "    [int((s*frames)//duration) for s in step_ends_sec]\n",
    "    )\n",
    "    final_dict['step_starts'] = np.array(\n",
    "    [int((s*frames)//duration) for s in step_starts_sec]\n",
    "    )\n",
    "    \n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d30f9545-0591-4e5a-afbe-e5d70de20c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dataset=[]\n",
    "mypath='./s3d_features/'\n",
    "path='./'\n",
    "i=0\n",
    "for f in listdir(mypath):\n",
    "    id=f[:11]\n",
    "#     print(id)\n",
    "      \n",
    "    if id!='PqyqB_FkWrQ' and id in data:\n",
    "        dataset.append(getDatas3d(id,data[id],path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7298f36-b39e-477d-a31a-27264c94f8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1961d8c9-df02-4f8c-a6e4-baf3b5cbe667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('ZGFoNfn4tBg' in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7c551a2-0c9b-44c8-a966-42d87277eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('youcookii_annotations_trainval.json')\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "data=data['database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21f7ba32-36b4-421f-8c47-2ab8cae589a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1672/1672 [01:15<00:00, 22.13it/s]\n",
      "100%|██████████| 1672/1672 [00:24<00:00, 68.37it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53.038015380880985,\n",
       " 31.71463833389377,\n",
       " 62.95751898223876,\n",
       " 41.87704176602314,\n",
       " 11.00881943369952)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=None\n",
    "gamma=1\n",
    "compute_all_metrics(dataset,model,gamma,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5486f662-115e-4fd8-a50d-bc1a216a15df",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-23938a1adf2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3d_dataset.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('s3d_dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "53dc9d03-3cbd-49b4-86ac-6e22642093e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0050, -0.0490,  0.0059,  ...,  0.0014, -0.0144,  0.0187],\n",
      "        [-0.0289, -0.0180, -0.0198,  ...,  0.0232, -0.0047,  0.0262],\n",
      "        [ 0.0048, -0.0178, -0.0122,  ...,  0.0180, -0.0120,  0.0086],\n",
      "        ...,\n",
      "        [-0.0095,  0.0159,  0.0062,  ...,  0.0113, -0.0084,  0.0288],\n",
      "        [-0.0016,  0.0165,  0.0009,  ...,  0.0036, -0.0058,  0.0216],\n",
      "        [-0.0110,  0.0063, -0.0051,  ...,  0.0051, -0.0092,  0.0267]],\n",
      "       dtype=torch.float64)\n",
      "<built-in method type of Tensor object at 0x7f212b256c80>\n",
      "tensor([[ 8.8086, 18.8481,  5.6956,  ...,  5.0874,  2.4268, -1.2983],\n",
      "        [11.6736, -0.3964, -1.8675,  ..., 11.3856, 10.9289, -1.2079],\n",
      "        [-0.0474, 10.2388,  4.3133,  ..., -6.5701,  0.8452,  7.0208],\n",
      "        [ 1.8079,  3.9229, -6.9373,  ..., -1.0768,  1.4951,  4.4176]],\n",
      "       dtype=torch.float64, grad_fn=<CopyBackwards>)\n",
      "tensor([[ 8.8086, 18.8481,  5.6956,  ...,  5.0874,  2.4268, -1.2983],\n",
      "        [11.6736, -0.3964, -1.8675,  ..., 11.3856, 10.9289, -1.2079],\n",
      "        [-0.0474, 10.2388,  4.3133,  ..., -6.5701,  0.8452,  7.0208],\n",
      "        [ 1.8079,  3.9229, -6.9373,  ..., -1.0768,  1.4951,  4.4176]],\n",
      "       dtype=torch.float64, grad_fn=<CopyBackwards>)\n",
      "<built-in method type of Tensor object at 0x7f212b256d80>\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset:\n",
    "    print(sample['frame_features'])\n",
    "    print(sample['frame_features'].type)\n",
    "    print(sample['step_features'])\n",
    "    print(sample['step_features'])\n",
    "    print(sample['step_features'].type)\n",
    "    sim = (sample['step_features'] @ sample['frame_features'].T)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf7622-f93d-4139-8b0c-d49ecaebd123",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_std * 100, iou_std * 100, accuracy_dtw * 100, iou_dtw * 100, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940a88e-629d-44ff-8e84-1b21f8ae8650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
